date_published,title,authors,summary,url,category
2023-11-03 09:01:37+00:00,Fast ellipsoidal conformal and quasi-conformal parameterization of genus-0 closed surfaces,['Gary P. T. Choi'],"Surface parameterization plays a fundamental role in many science and
engineering problems. In particular, as genus-0 closed surfaces are
topologically equivalent to a sphere, many spherical parameterization methods
have been developed over the past few decades. However, in practice, mapping a
genus-0 closed surface onto a sphere may result in a large distortion due to
their geometric difference. In this work, we propose a new framework for
computing ellipsoidal conformal and quasi-conformal parameterizations of
genus-0 closed surfaces, in which the target parameter domain is an ellipsoid
instead of a sphere. By combining simple conformal transformations with
different types of quasi-conformal mappings, we can easily achieve a large
variety of ellipsoidal parameterizations with their bijectivity guaranteed by
quasi-conformal theory. Numerical experiments are presented to demonstrate the
effectiveness of the proposed framework.",http://arxiv.org/pdf/2311.01788v1,cs.CG
2023-11-02 17:59:30+00:00,Conformal Policy Learning for Sensorimotor Control Under Distribution Shifts,"['Huang Huang', 'Satvik Sharma', 'Antonio Loquercio', 'Anastasios Angelopoulos', 'Ken Goldberg', 'Jitendra Malik']","This paper focuses on the problem of detecting and reacting to changes in the
distribution of a sensorimotor controller's observables. The key idea is the
design of switching policies that can take conformal quantiles as input, which
we define as conformal policy learning, that allows robots to detect
distribution shifts with formal statistical guarantees. We show how to design
such policies by using conformal quantiles to switch between base policies with
different characteristics, e.g. safety or speed, or directly augmenting a
policy observation with a quantile and training it with reinforcement learning.
Theoretically, we show that such policies achieve the formal convergence
guarantees in finite time. In addition, we thoroughly evaluate their advantages
and limitations on two compelling use cases: simulated autonomous driving and
active perception with a physical quadruped. Empirical results demonstrate that
our approach outperforms five baselines. It is also the simplest of the
baseline strategies besides one ablation. Being easy to use, flexible, and with
formal guarantees, our work demonstrates how conformal prediction can be an
effective tool for sensorimotor learning under uncertainty.",http://arxiv.org/pdf/2311.01457v1,cs.RO
2023-11-01 18:37:07+00:00,Conformalized Deep Splines for Optimal and Efficient Prediction Sets,"['Nathaniel Diamant', 'Ehsan Hajiramezanali', 'Tommaso Biancalani', 'Gabriele Scalia']","Uncertainty estimation is critical in high-stakes machine learning
applications. One effective way to estimate uncertainty is conformal
prediction, which can provide predictive inference with statistical coverage
guarantees. We present a new conformal regression method, Spline Prediction
Intervals via Conformal Estimation (SPICE), that estimates the conditional
density using neural-network-parameterized splines. We prove universal
approximation and optimality results for SPICE, which are empirically validated
by our experiments. SPICE is compatible with two different efficient-to-compute
conformal scores, one oracle-optimal for marginal coverage (SPICE-ND) and the
other asymptotically optimal for conditional coverage (SPICE-HPD). Results on
benchmark datasets demonstrate SPICE-ND models achieve the smallest average
prediction set sizes, including average size reductions of nearly 50% for some
datasets compared to the next best baseline. SPICE-HPD models achieve the best
conditional coverage compared to baselines. The SPICE implementation is made
available.",http://arxiv.org/pdf/2311.00774v1,cs.LG
2023-10-30 18:28:50+00:00,GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models,"['Seongwon Kim', 'Parisa Mollaei', 'Akshay Antony', 'Rishikesh Magar', 'Amir Barati Farimani']","With the rise of Transformers and Large Language Models (LLMs) in Chemistry
and Biology, new avenues for the design and understanding of therapeutics have
opened up to the scientific community. Protein sequences can be modeled as
language and can take advantage of recent advances in LLMs, specifically with
the abundance of our access to the protein sequence datasets. In this paper, we
developed the GPCR-BERT model for understanding the sequential design of G
Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of
FDA-approved pharmaceuticals. However, there is a lack of comprehensive
understanding regarding the relationship between amino acid sequence, ligand
selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By
utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with
prediction tasks of variations in the motifs, we were able to shed light on
several relationships between residues in the binding pocket and some of the
conserved motifs. To achieve this, we took advantage of attention weights, and
hidden states of the model that are interpreted to extract the extent of
contributions of amino acids in dictating the type of masked ones. The
fine-tuned models demonstrated high accuracy in predicting hidden residues
within the motifs. In addition, the analysis of embedding was performed over 3D
structures to elucidate the higher-order interactions within the conformations
of the receptors.",http://arxiv.org/pdf/2310.19915v1,cs.LG
2023-10-30 07:41:42+00:00,D4Explainer: In-Distribution GNN Explanations via Discrete Denoising Diffusion,"['Jialin Chen', 'Shirley Wu', 'Abhijit Gupta', 'Rex Ying']","The widespread deployment of Graph Neural Networks (GNNs) sparks significant
interest in their explainability, which plays a vital role in model auditing
and ensuring trustworthy graph learning. The objective of GNN explainability is
to discern the underlying graph structures that have the most significant
impact on model predictions. Ensuring that explanations generated are reliable
necessitates consideration of the in-distribution property, particularly due to
the vulnerability of GNNs to out-of-distribution data. Unfortunately,
prevailing explainability methods tend to constrain the generated explanations
to the structure of the original graph, thereby downplaying the significance of
the in-distribution property and resulting in explanations that lack
reliability. To address these challenges, we propose D4Explainer, a novel
approach that provides in-distribution GNN explanations for both counterfactual
and model-level explanation scenarios. The proposed D4Explainer incorporates
generative graph distribution learning into the optimization objective, which
accomplishes two goals: 1) generate a collection of diverse counterfactual
graphs that conform to the in-distribution property for a given instance, and
2) identify the most discriminative graph patterns that contribute to a
specific class prediction, thus serving as model-level explanations. It is
worth mentioning that D4Explainer is the first unified framework that combines
both counterfactual and model-level explanations. Empirical evaluations
conducted on synthetic and real-world datasets provide compelling evidence of
the state-of-the-art performance achieved by D4Explainer in terms of
explanation accuracy, faithfulness, diversity, and robustness.",http://arxiv.org/pdf/2310.19321v1,cs.LG
2023-10-29 17:47:45+00:00,Extending the Cooperative Dual-Task Space in Conformal Geometric Algebra,"['Tobias Löw', 'Sylvain Calinon']","In this work, we are presenting an extension of the cooperative dual-task
space (CDTS) in conformal geometric algebra. The CDTS was first defined using
dual quaternion algebra and is a well established framework for the simplified
definition of tasks using two manipulators. By integrating conformal geometric
algebra, we aim to further enhance the geometric expressiveness and thus
simplify the modeling of various tasks. We show this formulation by first
presenting the CDTS and then its extension that is based around a cooperative
pointpair. This extension keeps all the benefits of the original formulation
that is based on dual quaternions, but adds more tools for geometric modeling
of the dual-arm tasks. We also present how this CGA-CDTS can be seamlessly
integrated with an optimal control framework in geometric algebra that was
derived in previous work. In the experiments, we demonstrate how to model
different objectives and constraints using the CGA-CDTS. Using a setup of two
Franka Emika robots we then show the effectiveness of our approach using model
predictive control in real world experiments.",http://arxiv.org/pdf/2310.19093v1,cs.RO
2023-10-29 10:31:59+00:00,TRIAGE: Characterizing and auditing training data for improved regression,"['Nabeel Seedat', 'Jonathan Crabbé', 'Zhaozhi Qian', 'Mihaela van der Schaar']","Data quality is crucial for robust machine learning algorithms, with the
recent interest in data-centric AI emphasizing the importance of training data
characterization. However, current data characterization methods are largely
focused on classification settings, with regression settings largely
understudied. To address this, we introduce TRIAGE, a novel data
characterization framework tailored to regression tasks and compatible with a
broad class of regressors. TRIAGE utilizes conformal predictive distributions
to provide a model-agnostic scoring method, the TRIAGE score. We operationalize
the score to analyze individual samples' training dynamics and characterize
samples as under-, over-, or well-estimated by the model. We show that TRIAGE's
characterization is consistent and highlight its utility to improve performance
via data sculpting/filtering, in multiple regression settings. Additionally,
beyond sample level, we show TRIAGE enables new approaches to dataset selection
and feature acquisition. Overall, TRIAGE highlights the value unlocked by data
characterization in real-world regression applications",http://arxiv.org/pdf/2310.18970v1,cs.LG
2023-10-27 12:48:30+00:00,Transductive conformal inference with adaptive scores,"['Ulysse Gazin', 'Gilles Blanchard', 'Etienne Roquain']","Conformal inference is a fundamental and versatile tool that provides
distribution-free guarantees for many machine learning tasks. We consider the
transductive setting, where decisions are made on a test sample of $m$ new
points, giving rise to $m$ conformal $p$-values. {While classical results only
concern their marginal distribution, we show that their joint distribution
follows a P\'olya urn model, and establish a concentration inequality for their
empirical distribution function.} The results hold for arbitrary exchangeable
scores, including {\it adaptive} ones that can use the covariates of the
test+calibration samples at training stage for increased accuracy. We
demonstrate the usefulness of these theoretical results through uniform,
in-probability guarantees for two machine learning tasks of current interest:
interval prediction for transductive transfer learning and novelty detection
based on two-class classification.",http://arxiv.org/pdf/2310.18108v1,stat.ME
2023-10-26 17:59:32+00:00,High-Dimensional Prediction for Sequential Decision Making,"['Georgy Noarov', 'Ramya Ramalingam', 'Aaron Roth', 'Stephan Xie']","We study the problem of making predictions of an adversarially chosen
high-dimensional state that are unbiased subject to an arbitrary collection of
conditioning events, with the goal of tailoring these events to downstream
decision makers. We give efficient algorithms for solving this problem, as well
as a number of applications that stem from choosing an appropriate set of
conditioning events.
  For example, we can efficiently make predictions targeted at polynomially
many decision makers, giving each of them optimal swap regret if they
best-respond to our predictions. We generalize this to online combinatorial
optimization, where the decision makers have a very large action space, to give
the first algorithms offering polynomially many decision makers no regret on
polynomially many subsequences that may depend on their actions and the
context. We apply these results to get efficient no-subsequence-regret
algorithms in extensive-form games (EFGs), yielding a new family of regret
guarantees for EFGs that generalizes some existing EFG regret notions, e.g.
regret to informed causal deviations, and is generally incomparable to other
known such notions.
  Next, we develop a novel transparent alternative to conformal prediction for
building valid online adversarial multiclass prediction sets. We produce class
scores that downstream algorithms can use for producing valid-coverage
prediction sets, as if these scores were the true conditional class
probabilities. We show this implies strong conditional validity guarantees
including set-size-conditional and multigroup-fair coverage for polynomially
many downstream prediction sets. Moreover, our class scores can be guaranteed
to have improved $L_2$ loss, cross-entropy loss, and generally any Bregman
loss, compared to any collection of benchmark models, yielding a
high-dimensional real-valued version of omniprediction.",http://arxiv.org/pdf/2310.17651v2,cs.LG
2023-10-26 15:53:18+00:00,CBD: A Certified Backdoor Detector Based on Local Dominant Probability,"['Zhen Xiang', 'Zidi Xiong', 'Bo Li']","Backdoor attack is a common threat to deep neural networks. During testing,
samples embedded with a backdoor trigger will be misclassified as an
adversarial target by a backdoored model, while samples without the backdoor
trigger will be correctly classified. In this paper, we present the first
certified backdoor detector (CBD), which is based on a novel, adjustable
conformal prediction scheme based on our proposed statistic local dominant
probability. For any classifier under inspection, CBD provides 1) a detection
inference, 2) the condition under which the attacks are guaranteed to be
detectable for the same classification domain, and 3) a probabilistic upper
bound for the false positive rate. Our theoretical results show that attacks
with triggers that are more resilient to test-time noise and have smaller
perturbation magnitudes are more likely to be detected with guarantees.
Moreover, we conduct extensive experiments on four benchmark datasets
considering various backdoor types, such as BadNet, CB, and Blend. CBD achieves
comparable or even higher detection accuracy than state-of-the-art detectors,
and it in addition provides detection certification. Notably, for backdoor
attacks with random perturbation triggers bounded by $\ell_2\leq0.75$ which
achieves more than 90\% attack success rate, CBD achieves 100\% (98\%), 100\%
(84\%), 98\% (98\%), and 72\% (40\%) empirical (certified) detection true
positive rates on the four benchmark datasets GTSRB, SVHN, CIFAR-10, and
TinyImageNet, respectively, with low false positive rates.",http://arxiv.org/pdf/2310.17498v1,cs.LG
2023-10-25 14:40:33+00:00,Agreeing to Stop: Reliable Latency-Adaptive Decision Making via Ensembles of Spiking Neural Networks,"['Jiechen Chen', 'Sangwoo Park', 'Osvaldo Simeone']","Spiking neural networks (SNNs) are recurrent models that can leverage
sparsity in input time series to efficiently carry out tasks such as
classification. Additional efficiency gains can be obtained if decisions are
taken as early as possible as a function of the complexity of the input time
series. The decision on when to stop inference and produce a decision must rely
on an estimate of the current accuracy of the decision. Prior work demonstrated
the use of conformal prediction (CP) as a principled way to quantify
uncertainty and support adaptive-latency decisions in SNNs. In this paper, we
propose to enhance the uncertainty quantification capabilities of SNNs by
implementing ensemble models for the purpose of improving the reliability of
stopping decisions. Intuitively, an ensemble of multiple models can decide when
to stop more reliably by selecting times at which most models agree that the
current accuracy level is sufficient. The proposed method relies on different
forms of information pooling from ensemble models, and offers theoretical
reliability guarantees. We specifically show that variational inference-based
ensembles with p-variable pooling significantly reduce the average latency of
state-of-the-art methods, while maintaining reliability guarantees.",http://arxiv.org/pdf/2310.16675v1,cs.NE
2023-10-24 08:59:40+00:00,Guaranteed Coverage Prediction Intervals with Gaussian Process Regression,['Harris Papadopoulos'],"Gaussian Process Regression (GPR) is a popular regression method, which
unlike most Machine Learning techniques, provides estimates of uncertainty for
its predictions. These uncertainty estimates however, are based on the
assumption that the model is well-specified, an assumption that is violated in
most practical applications, since the required knowledge is rarely available.
As a result, the produced uncertainty estimates can become very misleading; for
example the prediction intervals (PIs) produced for the 95\% confidence level
may cover much less than 95\% of the true labels. To address this issue, this
paper introduces an extension of GPR based on a Machine Learning framework
called, Conformal Prediction (CP). This extension guarantees the production of
PIs with the required coverage even when the model is completely misspecified.
The proposed approach combines the advantages of GPR with the valid coverage
guarantee of CP, while the performed experimental results demonstrate its
superiority over existing methods.",http://arxiv.org/pdf/2310.15641v1,cs.LG
2023-10-21 05:54:26+00:00,Pre-Training on Large-Scale Generated Docking Conformations with HelixDock to Unlock the Potential of Protein-ligand Structure Prediction Models,"['Lihang Liu', 'Donglong He', 'Xianbin Ye', 'Shanzhuo Zhang', 'Xiaonan Zhang', 'Jingbo Zhou', 'Jun Li', 'Hua Chai', 'Fan Wang', 'Jingzhou He', 'Liang Zheng', 'Yonghui Li', 'Xiaomin Fang']","Molecular docking, a pivotal computational tool for drug discovery, predicts
the binding interactions between small molecules (ligands) and target proteins
(receptors). Conventional physics-based docking tools, though widely used, face
limitations in precision due to restricted conformational sampling and
imprecise scoring functions. Recent endeavors have employed deep learning
techniques to enhance docking accuracy, but their generalization remains a
concern due to limited training data. Leveraging the success of extensive and
diverse data in other domains, we introduce HelixDock, a novel approach for
site-specific molecular docking. Hundreds of millions of binding poses are
generated by traditional docking tools, encompassing diverse protein targets
and small molecules. Our deep learning-based docking model, a SE(3)-equivariant
network, is pre-trained with this large-scale dataset and then fine-tuned with
a small number of precise receptor-ligand complex structures. Comparative
analyses against physics-based and deep learning-based baseline methods
highlight HelixDock's superiority, especially on challenging test sets. Our
study elucidates the scaling laws of the pre-trained molecular docking models,
showcasing consistent improvements with increased model parameters and
pre-train data quantities. Harnessing the power of extensive and diverse
generated data holds promise for advancing AI-driven drug discovery.",http://arxiv.org/pdf/2310.13913v1,cs.LG
2023-10-20 15:41:50+00:00,Towards equilibrium molecular conformation generation with GFlowNets,"['Alexandra Volokhova', 'Michał Koziarski', 'Alex Hernández-García', 'Cheng-Hao Liu', 'Santiago Miret', 'Pablo Lemos', 'Luca Thiede', 'Zichao Yan', 'Alán Aspuru-Guzik', 'Yoshua Bengio']","Sampling diverse, thermodynamically feasible molecular conformations plays a
crucial role in predicting properties of a molecule. In this paper we propose
to use GFlowNet for sampling conformations of small molecules from the
Boltzmann distribution, as determined by the molecule's energy. The proposed
approach can be used in combination with energy estimation methods of different
fidelity and discovers a diverse set of low-energy conformations for highly
flexible drug-like molecules. We demonstrate that GFlowNet can reproduce
molecular potential energy surfaces by sampling proportionally to the Boltzmann
distribution.",http://arxiv.org/pdf/2310.14782v1,cs.LG
2023-10-19 17:57:57+00:00,PAC Prediction Sets Under Label Shift,"['Wenwen Si', 'Sangdon Park', 'Insup Lee', 'Edgar Dobriban', 'Osbert Bastani']","Prediction sets capture uncertainty by predicting sets of labels rather than
individual labels, enabling downstream decisions to conservatively account for
all plausible outcomes. Conformal inference algorithms construct prediction
sets guaranteed to contain the true label with high probability. These
guarantees fail to hold in the face of distribution shift, which is precisely
when reliable uncertainty quantification can be most useful. We propose a novel
algorithm for constructing prediction sets with PAC guarantees in the label
shift setting. This method estimates the predicted probabilities of the classes
in a target domain, as well as the confusion matrix, then propagates
uncertainty in these estimates through a Gaussian elimination algorithm to
compute confidence intervals for importance weights. Finally, it uses these
intervals to construct prediction sets. We evaluate our approach on five
datasets: the CIFAR-10, ChestX-Ray and Entity-13 image datasets, the tabular
CDC Heart dataset, and the AGNews text dataset. Our algorithm satisfies the PAC
guarantee while producing smaller, more informative, prediction sets compared
to several baselines.",http://arxiv.org/pdf/2310.12964v1,stat.ML
2023-10-18 15:17:10+00:00,Conformal Drug Property Prediction with Density Estimation under Covariate Shift,"['Siddhartha Laghuvarapu', 'Zhen Lin', 'Jimeng Sun']","In drug discovery, it is vital to confirm the predictions of pharmaceutical
properties from computational models using costly wet-lab experiments. Hence,
obtaining reliable uncertainty estimates is crucial for prioritizing drug
molecules for subsequent experimental validation. Conformal Prediction (CP) is
a promising tool for creating such prediction sets for molecular properties
with a coverage guarantee. However, the exchangeability assumption of CP is
often challenged with covariate shift in drug discovery tasks: Most datasets
contain limited labeled data, which may not be representative of the vast
chemical space from which molecules are drawn. To address this limitation, we
propose a method called CoDrug that employs an energy-based model leveraging
both training data and unlabelled data, and Kernel Density Estimation (KDE) to
assess the densities of a molecule set. The estimated densities are then used
to weigh the molecule samples while building prediction sets and rectifying for
distribution shift. In extensive experiments involving realistic distribution
drifts in various small-molecule drug discovery tasks, we demonstrate the
ability of CoDrug to provide valid prediction sets and its utility in
addressing the distribution shift arising from de novo drug design models. On
average, using CoDrug can reduce the coverage gap by over 35% when compared to
conformal prediction sets not adjusted for covariate shift.",http://arxiv.org/pdf/2310.12033v1,cs.LG
2023-10-17 10:24:25+00:00,On the Temperature of Bayesian Graph Neural Networks for Conformal Prediction,"['Seohyeon Cha', 'Honggu Kang', 'Joonhyuk Kang']","Accurate uncertainty quantification in graph neural networks (GNNs) is
essential, especially in high-stakes domains where GNNs are frequently
employed. Conformal prediction (CP) offers a promising framework for
quantifying uncertainty by providing $\textit{valid}$ prediction sets for any
black-box model. CP ensures formal probabilistic guarantees that a prediction
set contains a true label with a desired probability. However, the size of
prediction sets, known as $\textit{inefficiency}$, is influenced by the
underlying model and data generating process. On the other hand, Bayesian
learning also provides a credible region based on the estimated posterior
distribution, but this region is $\textit{well-calibrated}$ only when the model
is correctly specified. Building on a recent work that introduced a scaling
parameter for constructing valid credible regions from posterior estimate, our
study explores the advantages of incorporating a temperature parameter into
Bayesian GNNs within CP framework. We empirically demonstrate the existence of
temperatures that result in more efficient prediction sets. Furthermore, we
conduct an analysis to identify the factors contributing to inefficiency and
offer valuable insights into the relationship between CP performance and model
calibration.",http://arxiv.org/pdf/2310.11479v2,cs.LG
2023-10-16 11:35:41+00:00,Forking Uncertainties: Reliable Prediction and Model Predictive Control with Sequence Models via Conformal Risk Control,"['Matteo Zecchin', 'Sangwoo Park', 'Osvaldo Simeone']","In many real-world problems, predictions are leveraged to monitor and control
cyber-physical systems, demanding guarantees on the satisfaction of reliability
and safety requirements. However, predictions are inherently uncertain, and
managing prediction uncertainty presents significant challenges in environments
characterized by complex dynamics and forking trajectories. In this work, we
assume access to a pre-designed probabilistic implicit or explicit sequence
model, which may have been obtained using model-based or model-free methods. We
introduce probabilistic time series-conformal risk prediction (PTS-CRC), a
novel post-hoc calibration procedure that operates on the predictions produced
by any pre-designed probabilistic forecaster to yield reliable error bars. In
contrast to existing art, PTS-CRC produces predictive sets based on an ensemble
of multiple prototype trajectories sampled from the sequence model, supporting
the efficient representation of forking uncertainties. Furthermore, unlike the
state of the art, PTS-CRC can satisfy reliability definitions beyond coverage.
This property is leveraged to devise a novel model predictive control (MPC)
framework that addresses open-loop and closed-loop control problems under
general average constraints on the quality or safety of the control policy. We
experimentally validate the performance of PTS-CRC prediction and control by
studying a number of use cases in the context of wireless networking. Across
all the considered tasks, PTS-CRC predictors are shown to provide more
informative predictive sets, as well as safe control policies with larger
returns.",http://arxiv.org/pdf/2310.10299v1,cs.IT
2023-10-16 01:58:27+00:00,Conformal Contextual Robust Optimization,"['Yash Patel', 'Sahana Rayan', 'Ambuj Tewari']","Data-driven approaches to predict-then-optimize decision-making problems seek
to mitigate the risk of uncertainty region misspecification in safety-critical
settings. Current approaches, however, suffer from considering overly
conservative uncertainty regions, often resulting in suboptimal decisionmaking.
To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for
leveraging highly informative, nonconvex conformal prediction regions over
high-dimensional spaces based on conditional generative models, which have the
desired distribution-free coverage guarantees. Despite guaranteeing robustness,
such black-box optimization procedures alone inspire little confidence owing to
the lack of explanation of why a particular decision was found to be optimal.
We, therefore, augment CPO to additionally provide semantically meaningful
visual summaries of the uncertainty regions to give qualitative intuition for
the optimal decision. We highlight the CPO framework by demonstrating results
on a suite of simulation-based inference benchmark tasks and a vehicle routing
task based on probabilistic weather prediction.",http://arxiv.org/pdf/2310.10003v1,stat.ME
2023-10-15 19:24:52+00:00,Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data,"['Shiladitya Dutta', 'Hongbo Wei', 'Lars van der Laan', 'Ahmed M. Alaa']","Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., ""an image of a <category>"", and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.",http://arxiv.org/pdf/2310.09926v1,cs.AI
2023-10-12 10:56:25+00:00,Conformal inference for regression on Riemannian Manifolds,"['Alejandro Cholaquidis', 'Fabrice Gamboa', 'Leonardo Moreno']","Regression on manifolds, and, more broadly, statistics on manifolds, has
garnered significant importance in recent years due to the vast number of
applications for this type of data. Circular data is a classic example, but so
is data in the space of covariance matrices, data on the Grassmannian manifold
obtained as a result of principal component analysis, among many others. In
this work we investigate prediction sets for regression scenarios when the
response variable, denoted by $Y$, resides in a manifold, and the covariable,
denoted by X, lies in Euclidean space. This extends the concepts delineated in
[Lei and Wasserman, 2014] to this novel context. Aligning with traditional
principles in conformal inference, these prediction sets are distribution-free,
indicating that no specific assumptions are imposed on the joint distribution
of $(X, Y)$, and they maintain a non-parametric character. We prove the
asymptotic almost sure convergence of the empirical version of these regions on
the manifold to their population counterparts. The efficiency of this method is
shown through a comprehensive simulation study and an analysis involving
real-world data.",http://arxiv.org/pdf/2310.08209v1,stat.ML
2023-10-11 19:52:11+00:00,Conformal prediction with local weights: randomization enables local guarantees,"['Rohan Hore', 'Rina Foygel Barber']","In this work, we consider the problem of building distribution-free
prediction intervals with finite-sample conditional coverage guarantees.
Conformal prediction (CP) is an increasingly popular framework for building
prediction intervals with distribution-free guarantees, but these guarantees
only ensure marginal coverage: the probability of coverage is averaged over a
random draw of both the training and test data, meaning that there might be
substantial undercoverage within certain subpopulations. Instead, ideally, we
would want to have local coverage guarantees that hold for each possible value
of the test point's features. While the impossibility of achieving pointwise
local coverage is well established in the literature, many variants of
conformal prediction algorithm show favorable local coverage properties
empirically. Relaxing the definition of local coverage can allow for a
theoretical understanding of this empirical phenomenon. We aim to bridge this
gap between theoretical validation and empirical performance by proving
achievable and interpretable guarantees for a relaxed notion of local coverage.
Building on the localized CP method of Guan (2023) and the weighted CP
framework of Tibshirani et al. (2019), we propose a new method,
randomly-localized conformal prediction (RLCP), which returns prediction
intervals that are not only marginally valid but also achieve a relaxed local
coverage guarantee and guarantees under covariate shift. Through a series of
simulations and real data experiments, we validate these coverage guarantees of
RLCP while comparing it with the other local conformal prediction methods.",http://arxiv.org/pdf/2310.07850v1,stat.ME
2023-10-10 08:54:14+00:00,Conformal Prediction for Deep Classifier via Label Ranking,"['Jianguo Huang', 'Huajun Xi', 'Linjun Zhang', 'Huaxiu Yao', 'Yue Qiu', 'Hongxin Wei']","Conformal prediction is a statistical framework that generates prediction
sets containing ground-truth labels with a desired coverage guarantee. The
predicted probabilities produced by machine learning models are generally
miscalibrated, leading to large prediction sets in conformal prediction. In
this paper, we empirically and theoretically show that disregarding the
probabilities' value will mitigate the undesirable effect of miscalibrated
probability values. Then, we propose a novel algorithm named $\textit{Sorted
Adaptive prediction sets}$ (SAPS), which discards all the probability values
except for the maximum softmax probability. The key idea behind SAPS is to
minimize the dependence of the non-conformity score on the probability values
while retaining the uncertainty information. In this manner, SAPS can produce
sets of small size and communicate instance-wise uncertainty. Theoretically, we
provide a finite-sample coverage guarantee of SAPS and show that the expected
value of set size from SAPS is always smaller than APS. Extensive experiments
validate that SAPS not only lessens the prediction sets but also broadly
enhances the conditional coverage rate and adaptation of prediction sets.",http://arxiv.org/pdf/2310.06430v1,cs.LG
2023-10-09 20:02:11+00:00,On Time Domain Conformer Models for Monaural Speech Separation in Noisy Reverberant Acoustic Environments,"['William Ravenscroft', 'Stefan Goetze', 'Thomas Hain']","Speech separation remains an important topic for multi-speaker technology
researchers. Convolution augmented transformers (conformers) have performed
well for many speech processing tasks but have been under-researched for speech
separation. Most recent state-of-the-art (SOTA) separation models have been
time-domain audio separation networks (TasNets). A number of successful models
have made use of dual-path (DP) networks which sequentially process local and
global information. Time domain conformers (TD-Conformers) are an analogue of
the DP approach in that they also process local and global context sequentially
but have a different time complexity function. It is shown that for realistic
shorter signal lengths, conformers are more efficient when controlling for
feature dimension. Subsampling layers are proposed to further improve
computational efficiency. The best TD-Conformer achieves 14.6 dB and 21.2 dB
SISDR improvement on the WHAMR and WSJ0-2Mix benchmarks, respectively.",http://arxiv.org/pdf/2310.06125v1,cs.SD
2023-10-09 17:59:30+00:00,Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions,"['Jordan Lekeufack', 'Anastasios N. Angelopoulos', 'Andrea Bajcsy', 'Michael I. Jordan', 'Jitendra Malik']","We introduce Conformal Decision Theory, a framework for producing safe
autonomous decisions despite imperfect machine learning predictions. Examples
of such decisions are ubiquitous, from robot planning algorithms that rely on
pedestrian predictions, to calibrating autonomous manufacturing to exhibit high
throughput and low error, to the choice of trusting a nominal policy versus
switching to a safe backup policy at run-time. The decisions produced by our
algorithms are safe in the sense that they come with provable statistical
guarantees of having low risk without any assumptions on the world model
whatsoever; the observations need not be I.I.D. and can even be adversarial.
The theory extends results from conformal prediction to calibrate decisions
directly, without requiring the construction of prediction sets. Experiments
demonstrate the utility of our approach in robot motion planning around humans,
automated stock trading, and robot manufacturing.",http://arxiv.org/pdf/2310.05921v2,stat.ML
2023-10-08 13:01:36+00:00,A time-varying finance-led model for U.S. business cycles,['Marcio Santetti'],"This paper empirically assesses predictions of Goodwin's model of cyclical
growth regarding demand and distributive regimes when integrating the real and
financial sectors. In addition, it evaluates how financial and employment
shocks affect the labor market and monetary policy variables over six different
U.S. business-cycle peaks. It identifies a parsimonious Time-Varying Vector
Autoregressive model with Stochastic Volatility (TVP-VAR-SV) with the labor
share of income, the employment rate, residential investment, and the interest
rate spread as endogenous variables. Using Bayesian inference methods, key
results suggest (i) a combination of profit-led demand and profit-squeeze
distribution; (ii) weakening of these regimes during the Great Moderation; and
(iii) significant connections between the standard Goodwinian variables and
residential investment as well as term spreads. Findings presented here broadly
conform to the transition to increasingly deregulated financial and labor
markets initiated in the 1980s.",http://arxiv.org/pdf/2310.05153v1,econ.GN
2023-10-05 13:57:24+00:00,Distribution-free risk assessment of regression-based machine learning algorithms,"['Sukrita Singh', 'Neeraj Sarna', 'Yuanyuan Li', 'Yang Li', 'Agni Orfanoudaki', 'Michael Berger']","Machine learning algorithms have grown in sophistication over the years and
are increasingly deployed for real-life applications. However, when using
machine learning techniques in practical settings, particularly in high-risk
applications such as medicine and engineering, obtaining the failure
probability of the predictive model is critical. We refer to this problem as
the risk-assessment task. We focus on regression algorithms and the
risk-assessment task of computing the probability of the true label lying
inside an interval defined around the model's prediction. We solve the
risk-assessment problem using the conformal prediction approach, which provides
prediction intervals that are guaranteed to contain the true label with a given
probability. Using this coverage property, we prove that our approximated
failure probability is conservative in the sense that it is not lower than the
true failure probability of the ML algorithm. We conduct extensive experiments
to empirically study the accuracy of the proposed method for problems with and
without covariate shift. Our analysis focuses on different modeling regimes,
dataset sizes, and conformal prediction methodologies.",http://arxiv.org/pdf/2310.03545v1,cs.LG
2023-10-05 01:26:13+00:00,Sparse Deep Learning for Time Series Data: Theory and Applications,"['Mingxuan Zhang', 'Yan Sun', 'Faming Liang']","Sparse deep learning has become a popular technique for improving the
performance of deep neural networks in areas such as uncertainty
quantification, variable selection, and large-scale network compression.
However, most existing research has focused on problems where the observations
are independent and identically distributed (i.i.d.), and there has been little
work on the problems where the observations are dependent, such as time series
data and sequential data in natural language processing. This paper aims to
address this gap by studying the theory for sparse deep learning with dependent
data. We show that sparse recurrent neural networks (RNNs) can be consistently
estimated, and their predictions are asymptotically normally distributed under
appropriate assumptions, enabling the prediction uncertainty to be correctly
quantified. Our numerical results show that sparse deep learning outperforms
state-of-the-art methods, such as conformal predictions, in prediction
uncertainty quantification for time series data. Furthermore, our results
indicate that the proposed method can consistently identify the autoregressive
order for time series data and outperform existing methods in large-scale model
compression. Our proposed method has important practical implications in fields
such as finance, healthcare, and energy, where both accurate point estimates
and prediction uncertainty quantification are of concern.",http://arxiv.org/pdf/2310.03243v1,stat.ML
2023-10-04 14:51:07+00:00,Conformal Predictions for Longitudinal Data,"['Devesh Batra', 'Salvatore Mercuri', 'Raad Khraishi']","We introduce Longitudinal Predictive Conformal Inference (LPCI), a novel
distribution-free conformal prediction algorithm for longitudinal data. Current
conformal prediction approaches for time series data predominantly focus on the
univariate setting, and thus lack cross-sectional coverage when applied
individually to each time series in a longitudinal dataset. The current
state-of-the-art for longitudinal data relies on creating infinitely-wide
prediction intervals to guarantee both cross-sectional and asymptotic
longitudinal coverage. The proposed LPCI method addresses this by ensuring that
both longitudinal and cross-sectional coverages are guaranteed without
resorting to infinitely wide intervals. In our approach, we model the residual
data as a quantile fixed-effects regression problem, constructing prediction
intervals with a trained quantile regressor. Our extensive experiments
demonstrate that LPCI achieves valid cross-sectional coverage and outperforms
existing benchmarks in terms of longitudinal coverage rates. Theoretically, we
establish LPCI's asymptotic coverage guarantees for both dimensions, with
finite-width intervals. The robust performance of LPCI in generating reliable
prediction intervals for longitudinal data underscores its potential for broad
applications, including in medicine, finance, and supply chain management.",http://arxiv.org/pdf/2310.02863v1,stat.ML
2023-10-02 15:00:19+00:00,Non-Exchangeable Conformal Risk Control,"['António Farinhas', 'Chrysoula Zerva', 'Dennis Ulmer', 'André F. T. Martins']","Split conformal prediction has recently sparked great interest due to its
ability to provide formally guaranteed uncertainty sets or intervals for
predictions made by black-box neural models, ensuring a predefined probability
of containing the actual ground truth. While the original formulation assumes
data exchangeability, some extensions handle non-exchangeable data, which is
often the case in many real-world scenarios. In parallel, some progress has
been made in conformal methods that provide statistical guarantees for a
broader range of objectives, such as bounding the best F1-score or minimizing
the false negative rate in expectation. In this paper, we leverage and extend
these two lines of work by proposing non-exchangeable conformal risk control,
which allows controlling the expected value of any monotone loss function when
the data is not exchangeable. Our framework is flexible, makes very few
assumptions, and allows weighting the data based on its statistical similarity
with the test examples; a careful choice of weights may result on tighter
bounds, making our framework useful in the presence of change points, time
series, or other forms of distribution drift. Experiments with both synthetic
and real world data show the usefulness of our method.",http://arxiv.org/pdf/2310.01262v1,cs.LG
2023-09-29 20:06:46+00:00,Learning Over Molecular Conformer Ensembles: Datasets and Benchmarks,"['Yanqiao Zhu', 'Jeehyun Hwang', 'Keir Adams', 'Zhen Liu', 'Bozhao Nan', 'Brock Stenfors', 'Yuanqi Du', 'Jatin Chauhan', 'Olaf Wiest', 'Olexandr Isayev', 'Connor W. Coley', 'Yizhou Sun', 'Wei Wang']","Molecular Representation Learning (MRL) has proven impactful in numerous
biochemical applications such as drug discovery and enzyme design. While Graph
Neural Networks (GNNs) are effective at learning molecular representations from
a 2D molecular graph or a single 3D structure, existing works often overlook
the flexible nature of molecules, which continuously interconvert across
conformations via chemical bond rotations and minor vibrational perturbations.
To better account for molecular flexibility, some recent works formulate MRL as
an ensemble learning problem, focusing on explicitly learning from a set of
conformer structures. However, most of these studies have limited datasets,
tasks, and models. In this work, we introduce the first MoleculAR Conformer
Ensemble Learning (MARCEL) benchmark to thoroughly evaluate the potential of
learning on conformer ensembles and suggest promising research directions.
MARCEL includes four datasets covering diverse molecule- and reaction-level
properties of chemically diverse molecules including organocatalysts and
transition-metal catalysts, extending beyond the scope of common GNN benchmarks
that are confined to drug-like molecules. In addition, we conduct a
comprehensive empirical study, which benchmarks representative 1D, 2D, and 3D
molecular representation learning models, along with two strategies that
explicitly incorporate conformer ensembles into 3D MRL models. Our findings
reveal that direct learning from an accessible conformer space can improve
performance on a variety of tasks and models.",http://arxiv.org/pdf/2310.00115v1,cs.LG
2023-09-27 17:16:32+00:00,Node-Aligned Graph-to-Graph Generation for Retrosynthesis Prediction,"['Lin Yao', 'Zhen Wang', 'Wentao Guo', 'Shang Xiang', 'Wentan Liu', 'Guolin Ke']","Single-step retrosynthesis is a crucial task in organic chemistry and drug
design, requiring the identification of required reactants to synthesize a
specific compound. with the advent of computer-aided synthesis planning, there
is growing interest in using machine-learning techniques to facilitate the
process. Existing template-free machine learning-based models typically utilize
transformer structures and represent molecules as ID sequences. However, these
methods often face challenges in fully leveraging the extensive topological
information of the molecule and aligning atoms between the production and
reactants, leading to results that are not as competitive as those of
semi-template models. Our proposed method, Node-Aligned Graph-to-Graph (NAG2G),
also serves as a transformer-based template-free model but utilizes 2D
molecular graphs and 3D conformation information. Furthermore, our approach
simplifies the incorporation of production-reactant atom mapping alignment by
leveraging node alignment to determine a specific order for node generation and
generating molecular graphs in an auto-regressive manner node-by-node. This
method ensures that the node generation order coincides with the node order in
the input graph, overcoming the difficulty of determining a specific node
generation order in an auto-regressive manner. Our extensive benchmarking
results demonstrate that the proposed NAG2G can outperform the previous
state-of-the-art baselines in various metrics.",http://arxiv.org/pdf/2309.15798v1,cs.LG
2023-09-27 12:50:51+00:00,Leveraging Topology for Domain Adaptive Road Segmentation in Satellite and Aerial Imagery,"['Javed Iqbal', 'Aliza Masood', 'Waqas Sultani', 'Mohsen Ali']","Getting precise aspects of road through segmentation from remote sensing
imagery is useful for many real-world applications such as autonomous vehicles,
urban development and planning, and achieving sustainable development goals.
Roads are only a small part of the image, and their appearance, type, width,
elevation, directions, etc. exhibit large variations across geographical areas.
Furthermore, due to differences in urbanization styles, planning, and the
natural environments; regions along the roads vary significantly. Due to these
variations among the train and test domains, the road segmentation algorithms
fail to generalize to new geographical locations. Unlike the generic domain
alignment scenarios, road segmentation has no scene structure, and generic
domain adaptation methods are unable to enforce topological properties like
continuity, connectivity, smoothness, etc., thus resulting in degraded domain
alignment. In this work, we propose a topology-aware unsupervised domain
adaptation approach for road segmentation in remote sensing imagery.
Specifically, we predict road skeleton, an auxiliary task to impose the
topological constraints. To enforce consistent predictions of road and
skeleton, especially in the unlabeled target domain, the conformity loss is
defined across the skeleton prediction head and the road-segmentation head.
Furthermore, for self-training, we filter out the noisy pseudo-labels by using
a connectivity-based pseudo-labels refinement strategy, on both road and
skeleton segmentation heads, thus avoiding holes and discontinuities. Extensive
experiments on the benchmark datasets show the effectiveness of the proposed
approach compared to existing state-of-the-art methods. Specifically, for
SpaceNet to DeepGlobe adaptation, the proposed approach outperforms the
competing methods by a minimum margin of 6.6%, 6.7%, and 9.8% in IoU, F1-score,
and APLS, respectively.",http://arxiv.org/pdf/2309.15625v1,cs.CV
2023-09-23 18:40:53+00:00,"Valid and efficient imprecise-probabilistic inference with partial priors, III. Marginalization",['Ryan Martin'],"As Basu (1977) writes, ""Eliminating nuisance parameters from a model is
universally recognized as a major problem of statistics,"" but after more than
50 years since Basu wrote these words, the two mainstream schools of thought in
statistics have yet to solve the problem. Fortunately, the two mainstream
frameworks aren't the only options. This series of papers rigorously develops a
new and very general inferential model (IM) framework for
imprecise-probabilistic statistical inference that is provably valid and
efficient, while simultaneously accommodating incomplete or partial prior
information about the relevant unknowns when it's available. The present paper,
Part III in the series, tackles the marginal inference problem. Part II showed
that, for parametric models, the likelihood function naturally plays a central
role and, here, when nuisance parameters are present, the same principles
suggest that the profile likelihood is the key player. When the likelihood
factors nicely, so that the interest and nuisance parameters are perfectly
separated, the valid and efficient profile-based marginal IM solution is
immediate. But even when the likelihood doesn't factor nicely, the same
profile-based solution remains valid and leads to efficiency gains. This is
demonstrated in several examples, including the famous Behrens--Fisher and
gamma mean problems, where I claim the proposed IM solution is the best
solution available. Remarkably, the same profiling-based construction offers
validity guarantees in the prediction and non-parametric inference problems.
Finally, I show how a broader view of this new IM construction can handle
non-parametric inference on risk minimizers and makes a connection between
non-parametric IMs and conformal prediction.",http://arxiv.org/pdf/2309.13454v1,stat.ME
2023-09-21 22:12:24+00:00,Confidence Calibration for Systems with Cascaded Predictive Modules,"['Yunye Gong', 'Yi Yao', 'Xiao Lin', 'Ajay Divakaran', 'Melinda Gervasio']","Existing conformal prediction algorithms estimate prediction intervals at
target confidence levels to characterize the performance of a regression model
on new test samples. However, considering an autonomous system consisting of
multiple modules, prediction intervals constructed for individual modules fall
short of accommodating uncertainty propagation over different modules and thus
cannot provide reliable predictions on system behavior. We address this
limitation and present novel solutions based on conformal prediction to provide
prediction intervals calibrated for a predictive system consisting of cascaded
modules (e.g., an upstream feature extraction module and a downstream
regression module). Our key idea is to leverage module-level validation data to
characterize the system-level error distribution without direct access to
end-to-end validation data. We provide theoretical justification and empirical
experimental results to demonstrate the effectiveness of proposed solutions. In
comparison to prediction intervals calibrated for individual modules, our
solutions generate improved intervals with more accurate performance guarantees
for system predictions, which are demonstrated on both synthetic systems and
real-world systems performing overlap prediction for indoor navigation using
the Matterport3D dataset.",http://arxiv.org/pdf/2309.12510v1,cs.LG
2023-09-20 02:40:59+00:00,Conformalized Multimodal Uncertainty Regression and Reasoning,"['Domenico Parente', 'Nastaran Darabi', 'Alex C. Stutts', 'Theja Tulabandhula', 'Amit Ranjan Trivedi']","This paper introduces a lightweight uncertainty estimator capable of
predicting multimodal (disjoint) uncertainty bounds by integrating conformal
prediction with a deep-learning regressor. We specifically discuss its
application for visual odometry (VO), where environmental features such as
flying domain symmetries and sensor measurements under ambiguities and
occlusion can result in multimodal uncertainties. Our simulation results show
that uncertainty estimates in our framework adapt sample-wise against
challenging operating conditions such as pronounced noise, limited training
data, and limited parametric size of the prediction model. We also develop a
reasoning framework that leverages these robust uncertainty estimates and
incorporates optical flow-based reasoning to improve prediction prediction
accuracy. Thus, by appropriately accounting for predictive uncertainties of
data-driven learning and closing their estimation loop via rule-based
reasoning, our methodology consistently surpasses conventional deep learning
approaches on all these challenging scenarios--pronounced noise, limited
training data, and limited model size-reducing the prediction error by 2-3x.",http://arxiv.org/pdf/2309.11018v1,cs.LG
2023-09-20 00:37:35+00:00,PAGER: A Framework for Failure Analysis of Deep Regression Models,"['Jayaraman J. Thiagarajan', 'Vivek Narayanaswamy', 'Puja Trivedi', 'Rushil Anirudh']","Safe deployment of AI models requires proactive detection of potential
prediction failures to prevent costly errors. While failure detection in
classification problems has received significant attention, characterizing
failure modes in regression tasks is more complicated and less explored.
Existing approaches rely on epistemic uncertainties or feature inconsistency
with the training distribution to characterize model risk. However, we show
that uncertainties are necessary but insufficient to accurately characterize
failure, owing to the various sources of error. In this paper, we propose PAGER
(Principled Analysis of Generalization Errors in Regressors), a framework to
systematically detect and characterize failures in deep regression models.
Built upon the recently proposed idea of anchoring in deep models, PAGER
unifies both epistemic uncertainties and novel, complementary non-conformity
scores to organize samples into different risk regimes, thereby providing a
comprehensive analysis of model errors. Additionally, we introduce novel
metrics for evaluating failure detectors in regression tasks. We demonstrate
the effectiveness of PAGER on synthetic and real-world benchmarks. Our results
highlight the capability of PAGER to identify regions of accurate
generalization and detect failure cases in out-of-distribution and
out-of-support scenarios.",http://arxiv.org/pdf/2309.10977v1,cs.LG
2023-09-18 19:05:25+00:00,Conformal Temporal Logic Planning using Large Language Models: Knowing When to Do What and When to Ask for Help,"['Jun Wang', 'Jiaming Tong', 'Kaiyuan Tan', 'Yevgeniy Vorobeychik', 'Yiannis Kantaros']","This paper addresses a new motion planning problem for mobile robots tasked
with accomplishing multiple high-level sub-tasks, expressed using natural
language (NL), in a temporal and logical order. To formally define such
missions, we leverage LTL defined over NL-based atomic predicates modeling the
considered NL-based sub-tasks. This is contrast to related planning approaches
that define LTL tasks over atomic predicates capturing desired low-level system
configurations. Our goal is to design robot plans that satisfy LTL tasks
defined over NL-based atomic propositions. A novel technical challenge arising
in this setup lies in reasoning about correctness of a robot plan with respect
to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a
hierarchical conformal natural language planner, that relies on a novel
integration of existing tools that include (i) automata theory to determine the
NL-specified sub-task the robot should accomplish next to make mission
progress; (ii) Large Language Models to design robot plans satisfying these
sub-tasks; and (iii) conformal prediction to reason probabilistically about
correctness of the designed plans and mission satisfaction and to determine if
external assistance is required. We provide extensive comparative experiments
on mobile manipulation tasks. The project website is ltl-llm.github.io.",http://arxiv.org/pdf/2309.10092v1,cs.RO
2023-09-18 09:02:44+00:00,Mutual Information-calibrated Conformal Feature Fusion for Uncertainty-Aware Multimodal 3D Object Detection at the Edge,"['Alex C. Stutts', 'Danilo Erricolo', 'Sathya Ravi', 'Theja Tulabandhula', 'Amit Ranjan Trivedi']","In the expanding landscape of AI-enabled robotics, robust quantification of
predictive uncertainties is of great importance. Three-dimensional (3D) object
detection, a critical robotics operation, has seen significant advancements;
however, the majority of current works focus only on accuracy and ignore
uncertainty quantification. Addressing this gap, our novel study integrates the
principles of conformal inference (CI) with information theoretic measures to
perform lightweight, Monte Carlo-free uncertainty estimation within a
multimodal framework. Through a multivariate Gaussian product of the latent
variables in a Variational Autoencoder (VAE), features from RGB camera and
LiDAR sensor data are fused to improve the prediction accuracy. Normalized
mutual information (NMI) is leveraged as a modulator for calibrating
uncertainty bounds derived from CI based on a weighted loss function. Our
simulation results show an inverse correlation between inherent predictive
uncertainty and NMI throughout the model's training. The framework demonstrates
comparable or better performance in KITTI 3D object detection benchmarks to
similar methods that are not uncertainty-aware, making it suitable for
real-time edge robotics.",http://arxiv.org/pdf/2309.09593v1,cs.CV
2023-09-16 12:21:57+00:00,Data-driven Reachability using Christoffel Functions and Conformal Prediction,"['Abdelmouaiz Tebjou', 'Goran Frehse', 'Faïcel Chamroukhi']","An important mathematical tool in the analysis of dynamical systems is the
approximation of the reach set, i.e., the set of states reachable after a given
time from a given initial state. This set is difficult to compute for complex
systems even if the system dynamics are known and given by a system of ordinary
differential equations with known coefficients. In practice, parameters are
often unknown and mathematical models difficult to obtain. Data-based
approaches are promised to avoid these difficulties by estimating the reach set
based on a sample of states. If a model is available, this training set can be
obtained through numerical simulation. In the absence of a model, real-life
observations can be used instead. A recently proposed approach for data-based
reach set approximation uses Christoffel functions to approximate the reach
set. Under certain assumptions, the approximation is guaranteed to converge to
the true solution. In this paper, we improve upon these results by notably
improving the sample efficiency and relaxing some of the assumptions by
exploiting statistical guarantees from conformal prediction with training and
calibration sets. In addition, we exploit an incremental way to compute the
Christoffel function to avoid the calibration set while maintaining the
statistical convergence guarantees. Furthermore, our approach is robust to
outliers in the training and calibration set.",http://arxiv.org/pdf/2309.08976v1,cs.LG
2023-09-15 11:10:46+00:00,Heteroskedastic conformal regression,"['Nicolas Dewolf', 'Bernard De Baets', 'Willem Waegeman']","Conformal prediction, and split conformal prediction as a specific
implementation, offer a distribution-free approach to estimating prediction
intervals with statistical guarantees. Recent work has shown that split
conformal prediction can produce state-of-the-art prediction intervals when
focusing on marginal coverage, i.e., on a calibration dataset the method
produces on average prediction intervals that contain the ground truth with a
predefined coverage level. However, such intervals are often not adaptive,
which can be problematic for regression problems with heteroskedastic noise.
This paper tries to shed new light on how adaptive prediction intervals can be
constructed using methods such as normalized and Mondrian conformal prediction.
We present theoretical and experimental results in which these methods are
investigated in a systematic way.",http://arxiv.org/pdf/2309.08313v1,stat.ML
2023-09-14 05:34:59+00:00,Uncertainty Intervals for Prediction Errors in Time Series Forecasting,"['Hui Xu', 'Song Mei', 'Stephen Bates', 'Jonathan Taylor', 'Robert Tibshirani']","Inference for prediction errors is critical in time series forecasting
pipelines. However, providing statistically meaningful uncertainty intervals
for prediction errors remains relatively under-explored. Practitioners often
resort to forward cross-validation (FCV) for obtaining point estimators and
constructing confidence intervals based on the Central Limit Theorem (CLT). The
naive version assumes independence, a condition that is usually invalid due to
time correlation. These approaches lack statistical interpretations and
theoretical justifications even under stationarity.
  This paper systematically investigates uncertainty intervals for prediction
errors in time series forecasting. We first distinguish two key inferential
targets: the stochastic test error over near future data points, and the
expected test error as the expectation of the former. The stochastic test error
is often more relevant in applications needing to quantify uncertainty over
individual time series instances. To construct prediction intervals for the
stochastic test error, we propose the quantile-based forward cross-validation
(QFCV) method. Under an ergodicity assumption, QFCV intervals have
asymptotically valid coverage and are shorter than marginal empirical
quantiles. In addition, we also illustrate why naive CLT-based FCV intervals
fail to provide valid uncertainty intervals, even with certain corrections. For
non-stationary time series, we further provide rolling intervals by combining
QFCV with adaptive conformal prediction to give time-average coverage
guarantees. Overall, we advocate the use of QFCV procedures and demonstrate
their coverage and efficiency through simulations and real data examples.",http://arxiv.org/pdf/2309.07435v1,stat.ME
2023-09-13 22:04:50+00:00,Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining,"['Xianghao Zhan', 'Qinmei Xu', 'Yuanning Zheng', 'Guangming Lu', 'Olivier Gevaert']","Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.",http://arxiv.org/pdf/2309.07332v1,cs.LG
2023-09-10 17:35:43+00:00,Adaptive conformal classification with noisy labels,"['Matteo Sesia', 'Y. X. Rachel Wang', 'Xin Tong']","This paper develops novel conformal prediction methods for classification
tasks that can automatically adapt to random label contamination in the
calibration sample, enabling more informative prediction sets with stronger
coverage guarantees compared to state-of-the-art approaches. This is made
possible by a precise theoretical characterization of the effective coverage
inflation (or deflation) suffered by standard conformal inferences in the
presence of label contamination, which is then made actionable through new
calibration algorithms. Our solution is flexible and can leverage different
modeling assumptions about the label contamination process, while requiring no
knowledge about the data distribution or the inner workings of the
machine-learning classifier. The advantages of the proposed methods are
demonstrated through extensive simulations and an application to object
classification with the CIFAR-10H image data set.",http://arxiv.org/pdf/2309.05092v1,stat.ME
2023-09-09 11:14:04+00:00,RR-CP: Reliable-Region-Based Conformal Prediction for Trustworthy Medical Image Classification,"['Yizhe Zhang', 'Shuo Wang', 'Yejia Zhang', 'Danny Z. Chen']","Conformal prediction (CP) generates a set of predictions for a given test
sample such that the prediction set almost always contains the true label
(e.g., 99.5\% of the time). CP provides comprehensive predictions on possible
labels of a given test sample, and the size of the set indicates how certain
the predictions are (e.g., a set larger than one is `uncertain'). Such distinct
properties of CP enable effective collaborations between human experts and
medical AI models, allowing efficient intervention and quality check in
clinical decision-making. In this paper, we propose a new method called
Reliable-Region-Based Conformal Prediction (RR-CP), which aims to impose a
stronger statistical guarantee so that the user-specified error rate (e.g.,
0.5\%) can be achieved in the test time, and under this constraint, the size of
the prediction set is optimized (to be small). We consider a small prediction
set size an important measure only when the user-specified error rate is
achieved. Experiments on five public datasets show that our RR-CP performs
well: with a reasonably small-sized prediction set, it achieves the
user-specified error rate (e.g., 0.5\%) significantly more frequently than
exiting CP methods.",http://arxiv.org/pdf/2309.04760v1,cs.LG
2023-09-08 01:36:58+00:00,3D Denoisers are Good 2D Teachers: Molecular Pretraining via Denoising and Cross-Modal Distillation,"['Sungjun Cho', 'Dae-Woong Jeong', 'Sung Moon Ko', 'Jinwoo Kim', 'Sehui Han', 'Seunghoon Hong', 'Honglak Lee', 'Moontae Lee']","Pretraining molecular representations from large unlabeled data is essential
for molecular property prediction due to the high cost of obtaining
ground-truth labels. While there exist various 2D graph-based molecular
pretraining approaches, these methods struggle to show statistically
significant gains in predictive performance. Recent work have thus instead
proposed 3D conformer-based pretraining under the task of denoising, which led
to promising results. During downstream finetuning, however, models trained
with 3D conformers require accurate atom-coordinates of previously unseen
molecules, which are computationally expensive to acquire at scale. In light of
this limitation, we propose D&D, a self-supervised molecular representation
learning framework that pretrains a 2D graph encoder by distilling
representations from a 3D denoiser. With denoising followed by cross-modal
knowledge distillation, our approach enjoys use of knowledge obtained from
denoising as well as painless application to downstream tasks with no access to
accurate conformers. Experiments on real-world molecular property prediction
datasets show that the graph encoder trained via D&D can infer 3D information
based on the 2D graph and shows superior performance and label-efficiency
against other baselines.",http://arxiv.org/pdf/2309.04062v1,cs.LG
2023-09-07 15:50:48+00:00,Conformal Autoregressive Generation: Beam Search with Coverage Guarantees,"['Nicolas Deutschmann', 'Marvin Alberts', 'María Rodríguez Martínez']","We introduce two new extensions to the beam search algorithm based on
conformal predictions (CP) to produce sets of sequences with theoretical
coverage guarantees. The first method is very simple and proposes
dynamically-sized subsets of beam search results but, unlike typical CP
procedures, has an upper bound on the achievable guarantee depending on a
post-hoc calibration measure. Our second algorithm introduces the conformal set
prediction procedure as part of the decoding process, producing a variable beam
width which adapts to the current uncertainty. While more complex, this
procedure can achieve coverage guarantees selected a priori. We provide
marginal coverage bounds for each method, and evaluate them empirically on a
selection of tasks drawing from natural language processing and chemistry.",http://arxiv.org/pdf/2309.03797v1,cs.LG
2023-09-05 11:34:21+00:00,Bring the Noise: Introducing Noise Robustness to Pretrained Automatic Speech Recognition,"['Patrick Eickhoff', 'Matthias Möller', 'Theresa Pekarek Rosin', 'Johannes Twiefel', 'Stefan Wermter']","In recent research, in the domain of speech processing, large End-to-End
(E2E) systems for Automatic Speech Recognition (ASR) have reported
state-of-the-art performance on various benchmarks. These systems intrinsically
learn how to handle and remove noise conditions from speech. Previous research
has shown, that it is possible to extract the denoising capabilities of these
models into a preprocessor network, which can be used as a frontend for
downstream ASR models. However, the proposed methods were limited to specific
fully convolutional architectures. In this work, we propose a novel method to
extract the denoising capabilities, that can be applied to any encoder-decoder
architecture. We propose the Cleancoder preprocessor architecture that extracts
hidden activations from the Conformer ASR model and feeds them to a decoder to
predict denoised spectrograms. We train our pre-processor on the Noisy Speech
Database (NSD) to reconstruct denoised spectrograms from noisy inputs. Then, we
evaluate our model as a frontend to a pretrained Conformer ASR model as well as
a frontend to train smaller Conformer ASR models from scratch. We show that the
Cleancoder is able to filter noise from speech and that it improves the total
Word Error Rate (WER) of the downstream model in noisy conditions for both
applications.",http://arxiv.org/pdf/2309.02145v1,cs.CL
2023-09-04 19:39:21+00:00,CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence,"['Alberto Carlevaro', 'Sara Narteni', 'Fabrizio Dabbene', 'Marco Muselli', 'Maurizio Mongelli']","Everyday life is increasingly influenced by artificial intelligence, and
there is no question that machine learning algorithms must be designed to be
reliable and trustworthy for everyone. Specifically, computer scientists
consider an artificial intelligence system safe and trustworthy if it fulfills
five pillars: explainability, robustness, transparency, fairness, and privacy.
In addition to these five, we propose a sixth fundamental aspect: conformity,
that is, the probabilistic assurance that the system will behave as the machine
learner expects. In this paper, we propose a methodology to link conformal
prediction with explainable machine learning by defining CONFIDERAI, a new
score function for rule-based models that leverages both rules predictive
ability and points geometrical position within rules boundaries. We also
address the problem of defining regions in the feature space where conformal
guarantees are satisfied by exploiting techniques to control the number of
non-conformal samples in conformal regions based on support vector data
description (SVDD). The overall methodology is tested with promising results on
benchmark and real datasets, such as DNS tunneling detection or cardiovascular
disease prediction.",http://arxiv.org/pdf/2309.01778v2,cs.LG
2023-08-30 17:13:30+00:00,An Uncertainty-Aware Pseudo-Label Selection Framework using Regularized Conformal Prediction,['Matin Moezzi'],"Consistency regularization-based methods are prevalent in semi-supervised
learning (SSL) algorithms due to their exceptional performance. However, they
mainly depend on domain-specific data augmentations, which are not usable in
domains where data augmentations are less practicable. On the other hand,
Pseudo-labeling (PL) is a general and domain-agnostic SSL approach that, unlike
consistency regularization-based methods, does not rely on the domain. PL
underperforms due to the erroneous high-confidence predictions from poorly
calibrated models. This paper proposes an uncertainty-aware pseudo-label
selection framework that employs uncertainty sets yielded by the conformal
regularization algorithm to fix the poor calibration neural networks, reducing
noisy training data. The codes of this work are available at:
https://github.com/matinmoezzi/ups conformal classification",http://arxiv.org/pdf/2309.15963v1,cs.LG
2023-08-29 10:02:10+00:00,Small Area Estimation with Random Forests and the LASSO,"['Victoire Michal', 'Jon Wakefield', 'Alexandra M. Schmidt', 'Alicia Cavanaugh', 'Brian Robinson', 'Jill Baumgartner']","We consider random forests and LASSO methods for model-based small area
estimation when the number of areas with sampled data is a small fraction of
the total areas for which estimates are required. Abundant auxiliary
information is available for the sampled areas, from the survey, and for all
areas, from an exterior source, and the goal is to use auxiliary variables to
predict the outcome of interest. We compare areal-level random forests and
LASSO approaches to a frequentist forward variable selection approach and a
Bayesian shrinkage method. Further, to measure the uncertainty of estimates
obtained from random forests and the LASSO, we propose a modification of the
split conformal procedure that relaxes the assumption of identically
distributed data. This work is motivated by Ghanaian data available from the
sixth Living Standard Survey (GLSS) and the 2010 Population and Housing Census.
We estimate the areal mean household log consumption using both datasets. The
outcome variable is measured only in the GLSS for 3\% of all the areas (136 out
of 5019) and more than 170 potential covariates are available from both
datasets. Among the four modelling methods considered, the Bayesian shrinkage
performed the best in terms of bias, MSE and prediction interval coverages and
scores, as assessed through a cross-validation study. We find substantial
between-area variation, the log consumption areal point estimates showing a
1.3-fold variation across the GAMA region. The western areas are the poorest
while the Accra Metropolitan Area district gathers the richest areas.",http://arxiv.org/pdf/2308.15180v1,stat.ME
2023-08-29 08:02:41+00:00,Group-Conditional Conformal Prediction via Quantile Regression Calibration for Crop and Weed Classification,"['Paul Melki', 'Lionel Bombrun', 'Boubacar Diallo', 'Jérôme Dias', 'Jean-Pierre da Costa']","As deep learning predictive models become an integral part of a large
spectrum of precision agricultural systems, a barrier to the adoption of such
automated solutions is the lack of user trust in these highly complex, opaque
and uncertain models. Indeed, deep neural networks are not equipped with any
explicit guarantees that can be used to certify the system's performance,
especially in highly varying uncontrolled environments such as the ones
typically faced in computer vision for agriculture.Fortunately, certain methods
developed in other communities can prove to be important for agricultural
applications. This article presents the conformal prediction framework that
provides valid statistical guarantees on the predictive performance of any
black box prediction machine, with almost no assumptions, applied to the
problem of deep visual classification of weeds and crops in real-world
conditions. The framework is exposed with a focus on its practical aspects and
special attention accorded to the Adaptive Prediction Sets (APS) approach that
delivers marginal guarantees on the model's coverage. Marginal results are then
shown to be insufficient to guarantee performance on all groups of individuals
in the population as characterized by their environmental and pedo-climatic
auxiliary data gathered during image acquisition.To tackle this shortcoming,
group-conditional conformal approaches are presented: the ''classical'' method
that consists of iteratively applying the APS procedure on all groups, and a
proposed elegant reformulation and implementation of the procedure using
quantile regression on group membership indicators. Empirical results showing
the validity of the proposed approach are presented and compared to the
marginal APS then discussed.",http://arxiv.org/pdf/2308.15094v1,cs.CV
2023-08-28 20:32:22+00:00,Conformal Meta-learners for Predictive Inference of Individual Treatment Effects,"['Ahmed Alaa', 'Zaid Ahmad', 'Mark van der Laan']","We investigate the problem of machine learning-based (ML) predictive
inference on individual treatment effects (ITEs). Previous work has focused
primarily on developing ML-based meta-learners that can provide point estimates
of the conditional average treatment effect (CATE); these are model-agnostic
approaches for combining intermediate nuisance estimates to produce estimates
of CATE. In this paper, we develop conformal meta-learners, a general framework
for issuing predictive intervals for ITEs by applying the standard conformal
prediction (CP) procedure on top of CATE meta-learners. We focus on a broad
class of meta-learners based on two-stage pseudo-outcome regression and develop
a stochastic ordering framework to study their validity. We show that inference
with conformal meta-learners is marginally valid if their (pseudo outcome)
conformity scores stochastically dominate oracle conformity scores evaluated on
the unobserved ITEs. Additionally, we prove that commonly used CATE
meta-learners, such as the doubly-robust learner, satisfy a model- and
distribution-free stochastic (or convex) dominance condition, making their
conformal inferences valid for practically-relevant levels of target coverage.
Whereas existing procedures conduct inference on nuisance parameters (i.e.,
potential outcomes) via weighted CP, conformal meta-learners enable direct
inference on the target parameter (ITE). Numerical experiments show that
conformal meta-learners provide valid intervals with competitive efficiency
while retaining the favorable point estimation properties of CATE
meta-learners.",http://arxiv.org/pdf/2308.14895v1,cs.LG
2023-08-23 17:01:53+00:00,How Safe Am I Given What I See? Calibrated Prediction of Safety Chances for Image-Controlled Autonomy,"['Zhenjiang Mao', 'Carson Sobolewski', 'Ivan Ruchkin']","End-to-end learning has emerged as a major paradigm for developing autonomous
systems. Unfortunately, with its performance and convenience comes an even
greater challenge of safety assurance. A key factor of this challenge is the
absence of the notion of a low-dimensional and interpretable dynamical state,
around which traditional assurance methods revolve. Focusing on the online
safety prediction problem, this paper proposes a configurable family of
learning pipelines based on generative world models, which do not require
low-dimensional states. To implement these pipelines, we overcome the
challenges of learning safety-informed latent representations and missing
safety labels under prediction-induced distribution shift. These pipelines come
with statistical calibration guarantees on their safety chance predictions
based on conformal prediction. We perform an extensive evaluation of the
proposed learning pipelines on two case studies of image-controlled systems: a
racing car and a cartpole.",http://arxiv.org/pdf/2308.12252v1,cs.LG
2023-08-23 07:50:43+00:00,Approximating Score-based Explanation Techniques Using Conformal Regression,"['Amr Alkhatib', 'Henrik Boström', 'Sofiane Ennadir', 'Ulf Johansson']","Score-based explainable machine-learning techniques are often used to
understand the logic behind black-box models. However, such explanation
techniques are often computationally expensive, which limits their application
in time-critical contexts. Therefore, we propose and investigate the use of
computationally less costly regression models for approximating the output of
score-based explanation techniques, such as SHAP. Moreover, validity guarantees
for the approximated values are provided by the employed inductive conformal
prediction framework. We propose several non-conformity measures designed to
take the difficulty of approximating the explanations into account while
keeping the computational cost low. We present results from a large-scale
empirical investigation, in which the approximate explanations generated by our
proposed models are evaluated with respect to efficiency (interval size). The
results indicate that the proposed method can significantly improve execution
time compared to the fast version of SHAP, TreeSHAP. The results also suggest
that the proposed method can produce tight intervals, while providing validity
guarantees. Moreover, the proposed approach allows for comparing explanations
of different approximation methods and selecting a method based on how
informative (tight) are the predicted intervals.",http://arxiv.org/pdf/2308.11975v1,cs.LG
2023-08-18 16:07:01+00:00,Robust Uncertainty Quantification using Conformalised Monte Carlo Prediction,"['Daniel Bethell', 'Simos Gerasimou', 'Radu Calinescu']","Deploying deep learning models in safety-critical applications remains a very
challenging task, mandating the provision of assurances for the dependable
operation of these models. Uncertainty quantification (UQ) methods estimate the
model's confidence per prediction, informing decision-making by considering the
effect of randomness and model misspecification. Despite the advances of
state-of-the-art UQ methods, they are computationally expensive or produce
conservative prediction sets/intervals. We introduce MC-CP, a novel hybrid UQ
method that combines a new adaptive Monte Carlo (MC) dropout method with
conformal prediction (CP). MC-CP adaptively modulates the traditional MC
dropout at runtime to save memory and computation resources, enabling
predictions to be consumed by CP, yielding robust prediction sets/intervals.
Throughout comprehensive experiments, we show that MC-CP delivers significant
improvements over advanced UQ methods, like MC dropout, RAPS and CQR, both in
classification and regression benchmarks. MC-CP can be easily added to existing
models, making its deployment simple.",http://arxiv.org/pdf/2308.09647v1,cs.LG
2023-08-17 05:42:27+00:00,Tipping Point Forecasting in Non-Stationary Dynamics on Function Spaces,"['Miguel Liu-Schiaffini', 'Clare E. Singer', 'Nikola Kovachki', 'Tapio Schneider', 'Kamyar Azizzadenesheli', 'Anima Anandkumar']","Tipping points are abrupt, drastic, and often irreversible changes in the
evolution of non-stationary and chaotic dynamical systems. For instance,
increased greenhouse gas concentrations are predicted to lead to drastic
decreases in low cloud cover, referred to as a climatological tipping point. In
this paper, we learn the evolution of such non-stationary dynamical systems
using a novel recurrent neural operator (RNO), which learns mappings between
function spaces. After training RNO on only the pre-tipping dynamics, we employ
it to detect future tipping points using an uncertainty-based approach. In
particular, we propose a conformal prediction framework to forecast tipping
points by monitoring deviations from physics constraints (such as conserved
quantities and partial differential equations), enabling forecasting of these
abrupt changes along with a rigorous measure of uncertainty. We illustrate our
proposed methodology on non-stationary ordinary and partial differential
equations, such as the Lorenz-63 and Kuramoto-Sivashinsky equations. We also
apply our methods to forecast a climate tipping point in stratocumulus cloud
cover. In our experiments, we demonstrate that even partial or approximate
physics constraints can be used to accurately forecast future tipping points.",http://arxiv.org/pdf/2308.08794v1,cs.LG
2023-08-14 14:39:13+00:00,Conformal Predictions Enhanced Expert-guided Meshing with Graph Neural Networks,"['Amin Heyrani Nobari', 'Justin Rey', 'Suhas Kodali', 'Matthew Jones', 'Faez Ahmed']","Computational Fluid Dynamics (CFD) is widely used in different engineering
fields, but accurate simulations are dependent upon proper meshing of the
simulation domain. While highly refined meshes may ensure precision, they come
with high computational costs. Similarly, adaptive remeshing techniques require
multiple simulations and come at a great computational cost. This means that
the meshing process is reliant upon expert knowledge and years of experience.
Automating mesh generation can save significant time and effort and lead to a
faster and more efficient design process. This paper presents a machine
learning-based scheme that utilizes Graph Neural Networks (GNN) and expert
guidance to automatically generate CFD meshes for aircraft models. In this
work, we introduce a new 3D segmentation algorithm that outperforms two
state-of-the-art models, PointNet++ and PointMLP, for surface classification.
We also present a novel approach to project predictions from 3D mesh
segmentation models to CAD surfaces using the conformal predictions method,
which provides marginal statistical guarantees and robust uncertainty
quantification and handling. We demonstrate that the addition of conformal
predictions effectively enables the model to avoid under-refinement, hence
failure, in CFD meshing even for weak and less accurate models. Finally, we
demonstrate the efficacy of our approach through a real-world case study that
demonstrates that our automatically generated mesh is comparable in quality to
expert-generated meshes and enables the solver to converge and produce accurate
results. Furthermore, we compare our approach to the alternative of adaptive
remeshing in the same case study and find that our method is 5 times faster in
the overall process of simulation. The code and data for this project are made
publicly available at https://github.com/ahnobari/AutoSurf.",http://arxiv.org/pdf/2308.07358v1,cs.GR
2023-08-10 01:24:25+00:00,Local-Global Information Interaction Debiasing for Dynamic Scene Graph Generation,"['Xinyu Lyu', 'Jingwei Liu', 'Yuyu Guo', 'Lianli Gao']","The task of dynamic scene graph generation (DynSGG) aims to generate scene
graphs for given videos, which involves modeling the spatial-temporal
information in the video. However, due to the long-tailed distribution of
samples in the dataset, previous DynSGG models fail to predict the tail
predicates. We argue that this phenomenon is due to previous methods that only
pay attention to the local spatial-temporal information and neglect the
consistency of multiple frames. To solve this problem, we propose a novel
DynSGG model based on multi-task learning, DynSGG-MTL, which introduces the
local interaction information and global human-action interaction information.
The interaction between objects and frame features makes the model more fully
understand the visual context of the single image. Long-temporal human actions
supervise the model to generate multiple scene graphs that conform to the
global constraints and avoid the model being unable to learn the tail
predicates. Extensive experiments on Action Genome dataset demonstrate the
efficacy of our proposed framework, which not only improves the dynamic scene
graph generation but also alleviates the long-tail problem.",http://arxiv.org/pdf/2308.05274v2,cs.CV
2023-08-08 13:03:36+00:00,Federated Inference with Reliable Uncertainty Quantification over Wireless Channels via Conformal Prediction,"['Meiyi Zhu', 'Matteo Zecchin', 'Sangwoo Park', 'Caili Guo', 'Chunyan Feng', 'Osvaldo Simeone']","Consider a setting in which devices and a server share a pre-trained model.
The server wishes to make an inference on a new input given the model. Devices
have access to data, previously not used for training, and can communicate to
the server over a common wireless channel. If the devices have no access to the
new input, can communication from devices to the server enhance the quality of
the inference decision at the server? Recent work has introduced federated
conformal prediction (CP), which leverages devices-to-server communication to
improve the reliability of the server's decision. With federated CP, devices
communicate to the server information about the loss accrued by the shared
pre-trained model on the local data, and the server leverages this information
to calibrate a decision interval, or set, so that it is guaranteed to contain
the correct answer with a pre-defined target reliability level. Previous work
assumed noise-free communication, whereby devices can communicate a single real
number to the server. In this paper, we study for the first time federated CP
in a wireless setting. We introduce a novel protocol, termed wireless federated
conformal prediction (WFCP), which builds on type-based multiple access (TBMA)
and on a novel quantile correction strategy. WFCP is proved to provide formal
reliability guarantees in terms of coverage of the predicted set produced by
the server. Using numerical results, we demonstrate the significant advantages
of WFCP against digital implementations of existing federated CP schemes,
especially in regimes with limited communication resources and/or large number
of devices.",http://arxiv.org/pdf/2308.04237v1,cs.IT
2023-08-02 01:44:30+00:00,VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference,"['Yonghe Zhao', 'Qiang Huang', 'Siwei Wu', 'Yun Peng', 'Huiyan Sun']","Causal inference plays a vital role in diverse domains like epidemiology,
healthcare, and economics. De-confounding and counterfactual prediction in
observational data has emerged as a prominent concern in causal inference
research. While existing models tackle observed confounders, the presence of
unobserved confounders remains a significant challenge, distorting causal
inference and impacting counterfactual outcome accuracy. To address this, we
propose a novel variational learning model of unobserved confounders for
counterfactual inference (VLUCI), which generates the posterior distribution of
unobserved confounders. VLUCI relaxes the unconfoundedness assumption often
overlooked by most causal inference methods. By disentangling observed and
unobserved confounders, VLUCI constructs a doubly variational inference model
to approximate the distribution of unobserved confounders, which are used for
inferring more accurate counterfactual outcomes. Extensive experiments on
synthetic and semi-synthetic datasets demonstrate VLUCI's superior performance
in inferring unobserved confounders. It is compatible with state-of-the-art
counterfactual inference models, significantly improving inference accuracy at
both group and individual levels. Additionally, VLUCI provides confidence
intervals for counterfactual outcomes, aiding decision-making in risk-sensitive
domains. We further clarify the considerations when applying VLUCI to cases
where unobserved confounders don't strictly conform to our model assumptions
using the public IHDP dataset as an example, highlighting the practical
advantages of VLUCI.",http://arxiv.org/pdf/2308.00904v2,cs.LG
2023-07-31 17:59:16+00:00,Conformal PID Control for Time Series Prediction,"['Anastasios N. Angelopoulos', 'Emmanuel J. Candes', 'Ryan J. Tibshirani']","We study the problem of uncertainty quantification for time series
prediction, with the goal of providing easy-to-use algorithms with formal
guarantees. The algorithms we present build upon ideas from conformal
prediction and control theory, are able to prospectively model conformal scores
in an online setting, and adapt to the presence of systematic errors due to
seasonality, trends, and general distribution shifts. Our theory both
simplifies and strengthens existing analyses in online conformal prediction.
Experiments on 4-week-ahead forecasting of statewide COVID-19 death counts in
the U.S. show an improvement in coverage over the ensemble forecaster used in
official CDC communications. We also run experiments on predicting electricity
demand, market returns, and temperature using autoregressive, Theta, Prophet,
and Transformer models. We provide an extendable codebase for testing our
methods and for the integration of new algorithms, data sets, and forecasting
rules.",http://arxiv.org/pdf/2307.16895v1,cs.LG
2023-07-31 01:32:06+00:00,Probabilistically robust conformal prediction,"['Subhankar Ghosh', 'Yuanjie Shi', 'Taha Belkhouja', 'Yan Yan', 'Jana Doppa', 'Brian Jones']","Conformal prediction (CP) is a framework to quantify uncertainty of machine
learning classifiers including deep neural networks. Given a testing example
and a trained classifier, CP produces a prediction set of candidate labels with
a user-specified coverage (i.e., true class label is contained with high
probability). Almost all the existing work on CP assumes clean testing data and
there is not much known about the robustness of CP algorithms w.r.t
natural/adversarial perturbations to testing examples. This paper studies the
problem of probabilistically robust conformal prediction (PRCP) which ensures
robustness to most perturbations around clean input examples. PRCP generalizes
the standard CP (cannot handle perturbations) and adversarially robust CP
(ensures robustness w.r.t worst-case perturbations) to achieve better
trade-offs between nominal performance and robustness. We propose a novel
adaptive PRCP (aPRCP) algorithm to achieve probabilistically robust coverage.
The key idea behind aPRCP is to determine two parallel thresholds, one for data
samples and another one for the perturbations on data (aka
""quantile-of-quantile"" design). We provide theoretical analysis to show that
aPRCP algorithm achieves robust coverage. Our experiments on CIFAR-10,
CIFAR-100, and ImageNet datasets using deep neural networks demonstrate that
aPRCP achieves better trade-offs than state-of-the-art CP and adversarially
robust CP algorithms.",http://arxiv.org/pdf/2307.16360v1,cs.LG
2023-07-24 20:45:39+00:00,Conformal prediction for frequency-severity modeling,"['Helton Graziadei', 'Paulo C. Marques F.', 'Eduardo F. L. de Melo', 'Rodrigo S. Targino']","We present a nonparametric model-agnostic framework for building prediction
intervals of insurance claims, with finite sample statistical guarantees,
extending the technique of split conformal prediction to the domain of
two-stage frequency-severity modeling. The effectiveness of the framework is
showcased with simulated and real datasets. When the underlying severity model
is a random forest, we extend the two-stage split conformal prediction
procedure, showing how the out-of-bag mechanism can be leveraged to eliminate
the need for a calibration set and to enable the production of prediction
intervals with adaptive width.",http://arxiv.org/pdf/2307.13124v2,stat.ME
2023-07-24 01:58:48+00:00,Model-free generalized fiducial inference,['Jonathan P Williams'],"Motivated by the need for the development of safe and reliable methods for
uncertainty quantification in machine learning, I propose and develop ideas for
a model-free statistical framework for imprecise probabilistic prediction
inference. This framework facilitates uncertainty quantification in the form of
prediction sets that offer finite sample control of type 1 errors, a property
shared with conformal prediction sets, but this new approach also offers more
versatile tools for imprecise probabilistic reasoning. Furthermore, I propose
and consider the theoretical and empirical properties of a precise
probabilistic approximation to the model-free imprecise framework.
Approximating a belief/plausibility measure pair by an [optimal in some sense]
probability measure in the credal set is a critical resolution needed for the
broader adoption of imprecise probabilistic approaches to inference in
statistical and machine learning communities. It is largely undetermined in the
statistical and machine learning literatures, more generally, how to properly
quantify uncertainty in that there is no generally accepted standard of
accountability of stated uncertainties. The research I present in this
manuscript is aimed at motivating a framework for statistical inference with
reliability and accountability as the guiding principles.",http://arxiv.org/pdf/2307.12472v1,stat.ML
2023-07-22 10:03:32+00:00,Conformal Group Recommender System,"['Venkateswara Rao Kagita', 'Anshuman Singh', 'Vikas Kumar', 'Pavan Kalyan Reddy Neerudu', 'Arun K Pujari', 'Rohit Kumar Bondugula']","Group recommender systems (GRS) are critical in discovering relevant items
from a near-infinite inventory based on group preferences rather than
individual preferences, like recommending a movie, restaurant, or tourist
destination to a group of individuals. The traditional models of group
recommendation are designed to act like a black box with a strict focus on
improving recommendation accuracy, and most often, they place the onus on the
users to interpret recommendations. In recent years, the focus of Recommender
Systems (RS) research has shifted away from merely improving recommendation
accuracy towards value additions such as confidence and explanation. In this
work, we propose a conformal prediction framework that provides a measure of
confidence with prediction in conjunction with a group recommender system to
augment the system-generated plain recommendations. In the context of group
recommender systems, we propose various nonconformity measures that play a
vital role in the efficiency of the conformal framework. We also show that
defined nonconformity satisfies the exchangeability property. Experimental
results demonstrate the effectiveness of the proposed approach over several
benchmark datasets. Furthermore, our proposed approach also satisfies validity
and efficiency properties.",http://arxiv.org/pdf/2307.12034v1,cs.IR
2023-07-20 07:23:15+00:00,Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities,"['Samie Mostafavi', 'Gourav Prateek Sharma', 'James Gross']","With the emergence of new application areas, such as cyber-physical systems
and human-in-the-loop applications, there is a need to guarantee a certain
level of end-to-end network latency with extremely high reliability, e.g.,
99.999%. While mechanisms specified under IEEE 802.1as time-sensitive
networking (TSN) can be used to achieve these requirements for switched
Ethernet networks, implementing TSN mechanisms in wireless networks is
challenging due to their stochastic nature. To conform the wireless link to a
reliability level of 99.999%, the behavior of extremely rare outliers in the
latency probability distribution, or the tail of the distribution, must be
analyzed and controlled. This work proposes predicting the tail of the latency
distribution using state-of-the-art data-driven approaches, such as mixture
density networks (MDN) and extreme value mixture models, to estimate the
likelihood of rare latencies conditioned on the network parameters, which can
be used to make more informed decisions in wireless transmission. Actual
latency measurements of IEEE 802.11g (WiFi), commercial private and a
software-defined 5G network are used to benchmark the proposed approaches and
evaluate their sensitivities concerning the tail probabilities.",http://arxiv.org/pdf/2307.10648v1,cs.NI
2023-07-20 03:54:47+00:00,Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning,"['Yutian Pang', 'Jueming Hu', 'Christopher S. Lieber', 'Nancy J. Cooke', 'Yongming Liu']","Air traffic control (ATC) is a safety-critical service system that demands
constant attention from ground air traffic controllers (ATCos) to maintain
daily aviation operations. The workload of the ATCos can have negative effects
on operational safety and airspace usage. To avoid overloading and ensure an
acceptable workload level for the ATCos, it is important to predict the ATCos'
workload accurately for mitigation actions. In this paper, we first perform a
review of research on ATCo workload, mostly from the air traffic perspective.
Then, we briefly introduce the setup of the human-in-the-loop (HITL)
simulations with retired ATCos, where the air traffic data and workload labels
are obtained. The simulations are conducted under three Phoenix approach
scenarios while the human ATCos are requested to self-evaluate their workload
ratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next,
we propose a graph-based deep-learning framework with conformal prediction to
identify the ATCo workload levels. The number of aircraft under the
controller's control varies both spatially and temporally, resulting in
dynamically evolving graphs. The experiment results suggest that (a) besides
the traffic density feature, the traffic conflict feature contributes to the
workload prediction capabilities (i.e., minimum horizontal/vertical separation
distance); (b) directly learning from the spatiotemporal graph layout of
airspace with graph neural network can achieve higher prediction accuracy,
compare to hand-crafted traffic complexity features; (c) conformal prediction
is a valuable tool to further boost model prediction accuracy, resulting a
range of predicted workload labels. The code used is available at
\href{https://github.com/ymlasu/para-atm-collection/blob/master/air-traffic-prediction/ATC-Workload-Prediction/}{$\mathsf{Link}$}.",http://arxiv.org/pdf/2307.10559v2,cs.LG
2023-07-19 07:58:21+00:00,GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence,"['Xia Huang', 'Kai Fong Ernest Chong']","Web image datasets curated online inherently contain ambiguous
in-distribution (ID) instances and out-of-distribution (OOD) instances, which
we collectively call non-conforming (NC) instances. In many recent approaches
for mitigating the negative effects of NC instances, the core implicit
assumption is that the NC instances can be found via entropy maximization. For
""entropy"" to be well-defined, we are interpreting the output prediction vector
of an instance as the parameter vector of a multinomial random variable, with
respect to some trained model with a softmax output layer. Hence, entropy
maximization is based on the idealized assumption that NC instances have
predictions that are ""almost"" uniformly distributed. However, in real-world web
image datasets, there are numerous NC instances whose predictions are far from
being uniformly distributed. To tackle the limitation of entropy maximization,
we propose $(\alpha, \beta)$-generalized KL divergence,
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$, which can be used to identify
significantly more NC instances. Theoretical properties of
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ are proven, and we also show
empirically that a simple use of $\mathcal{D}_{\text{KL}}^{\alpha,
\beta}(p\|q)$ outperforms all baselines on the NC instance identification task.
Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a
new iterative training framework, GenKL, that identifies and relabels NC
instances. When evaluated on three web image datasets, Clothing1M,
Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art
classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$
(top-1/top-5), respectively.",http://arxiv.org/pdf/2307.09810v1,cs.CV
2023-07-18 14:40:48+00:00,Conformal prediction under ambiguous ground truth,"['David Stutz', 'Abhijit Guha Roy', 'Tatiana Matejovicova', 'Patricia Strachan', 'Ali Taylan Cemgil', 'Arnaud Doucet']","Conformal Prediction (CP) allows to perform rigorous uncertainty
quantification by constructing a prediction set $C(X)$ satisfying $\mathbb{P}(Y
\in C(X))\geq 1-\alpha$ for a user-chosen $\alpha \in [0,1]$ by relying on
calibration data $(X_1,Y_1),...,(X_n,Y_n)$ from $\mathbb{P}=\mathbb{P}^{X}
\otimes \mathbb{P}^{Y|X}$. It is typically implicitly assumed that
$\mathbb{P}^{Y|X}$ is the ""true"" posterior label distribution. However, in many
real-world scenarios, the labels $Y_1,...,Y_n$ are obtained by aggregating
expert opinions using a voting procedure, resulting in a one-hot distribution
$\mathbb{P}_{vote}^{Y|X}$. For such ``voted'' labels, CP guarantees are thus
w.r.t. $\mathbb{P}_{vote}=\mathbb{P}^X \otimes \mathbb{P}_{vote}^{Y|X}$ rather
than the true distribution $\mathbb{P}$. In cases with unambiguous ground truth
labels, the distinction between $\mathbb{P}_{vote}$ and $\mathbb{P}$ is
irrelevant. However, when experts do not agree because of ambiguous labels,
approximating $\mathbb{P}^{Y|X}$ with a one-hot distribution
$\mathbb{P}_{vote}^{Y|X}$ ignores this uncertainty. In this paper, we propose
to leverage expert opinions to approximate $\mathbb{P}^{Y|X}$ using a
non-degenerate distribution $\mathbb{P}_{agg}^{Y|X}$. We develop Monte Carlo CP
procedures which provide guarantees w.r.t. $\mathbb{P}_{agg}=\mathbb{P}^X
\otimes \mathbb{P}_{agg}^{Y|X}$ by sampling multiple synthetic pseudo-labels
from $\mathbb{P}_{agg}^{Y|X}$ for each calibration example $X_1,...,X_n$. In a
case study of skin condition classification with significant disagreement among
expert annotators, we show that applying CP w.r.t. $\mathbb{P}_{vote}$
under-covers expert annotations: calibrated for $72\%$ coverage, it falls short
by on average $10\%$; our Monte Carlo CP closes this gap both empirically and
theoretically.",http://arxiv.org/pdf/2307.09302v2,cs.LG
2023-07-18 14:33:09+00:00,Model-free selective inference under covariate shift via weighted conformal p-values,"['Ying Jin', 'Emmanuel J. Candès']","This paper introduces novel weighted conformal p-values and methods for
model-free selective inference. The problem is as follows: given test units
with covariates $X$ and missing responses $Y$, how do we select units for which
the responses $Y$ are larger than user-specified values while controlling the
proportion of false positives? Can we achieve this without any modeling
assumptions on the data and without any restriction on the model for predicting
the responses? Last, methods should be applicable when there is a covariate
shift between training and test data, which commonly occurs in practice.
  We answer these questions by first leveraging any prediction model to produce
a class of well-calibrated weighted conformal p-values, which control the
type-I error in detecting a large response. These p-values cannot be passed on
to classical multiple testing procedures since they may not obey a well-known
positive dependence property. Hence, we introduce weighted conformalized
selection (WCS), a new procedure which controls false discovery rate (FDR) in
finite samples. Besides prediction-assisted candidate selection, WCS (1) allows
to infer multiple individual treatment effects, and (2) extends to outlier
detection with inlier distributions shifts. We demonstrate performance via
simulations and applications to causal inference, drug discovery, and outlier
detection datasets.",http://arxiv.org/pdf/2307.09291v2,stat.ME
2023-07-13 05:31:40+00:00,Prescriptive Process Monitoring Under Resource Constraints: A Reinforcement Learning Approach,"['Mahmoud Shoush', 'Marlon Dumas']","Prescriptive process monitoring methods seek to optimize the performance of
business processes by triggering interventions at runtime, thereby increasing
the probability of positive case outcomes. These interventions are triggered
according to an intervention policy. Reinforcement learning has been put
forward as an approach to learning intervention policies through trial and
error. Existing approaches in this space assume that the number of resources
available to perform interventions in a process is unlimited, an unrealistic
assumption in practice. This paper argues that, in the presence of resource
constraints, a key dilemma in the field of prescriptive process monitoring is
to trigger interventions based not only on predictions of their necessity,
timeliness, or effect but also on the uncertainty of these predictions and the
level of resource utilization. Indeed, committing scarce resources to an
intervention when the necessity or effects of this intervention are highly
uncertain may intuitively lead to suboptimal intervention effects. Accordingly,
the paper proposes a reinforcement learning approach for prescriptive process
monitoring that leverages conformal prediction techniques to consider the
uncertainty of the predictions upon which an intervention decision is based. An
evaluation using real-life datasets demonstrates that explicitly modeling
uncertainty using conformal predictions helps reinforcement learning agents
converge towards policies with higher net intervention gain",http://arxiv.org/pdf/2307.06564v1,cs.AI
2023-07-11 08:36:12+00:00,Conformalization of Sparse Generalized Linear Models,"['Etash Kumar Guha', 'Eugene Ndiaye', 'Xiaoming Huo']","Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n,
y_n)\}$, the conformal prediction method estimates a confidence set for
$y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely
assuming that the joint distribution of the data is permutation invariant.
Although attractive, computing such a set is computationally infeasible in most
regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can
take an infinite number of possible candidate values, and generating conformal
sets requires retraining a predictive model for each candidate. In this paper,
we focus on a sparse linear model with only a subset of variables for
prediction and use numerical continuation techniques to approximate the
solution path efficiently. The critical property we exploit is that the set of
selected variables is invariant under a small perturbation of the input data.
Therefore, it is sufficient to enumerate and refit the model only at the change
points of the set of active features and smoothly interpolate the rest of the
solution via a Predictor-Corrector mechanism. We show how our path-following
algorithm accurately approximates conformal prediction sets and illustrate its
performance using synthetic and real data examples.",http://arxiv.org/pdf/2307.05109v1,cs.LG
2023-07-11 03:13:25+00:00,Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction,"['Suk Yee Yong', 'Cheng Soon Ong']","Precise measurements of the black hole mass are essential to gain insight on
the black hole and host galaxy co-evolution. A direct measure of the black hole
mass is often restricted to nearest galaxies and instead, an indirect method
using the single-epoch virial black hole mass estimation is used for objects at
high redshifts. However, this method is subjected to biases and uncertainties
as it is reliant on the scaling relation from a small sample of local active
galactic nuclei. In this study, we propose the application of conformalised
quantile regression (CQR) to quantify the uncertainties of the black hole
predictions in a machine learning setting. We compare CQR with various
prediction interval techniques and demonstrated that CQR can provide a more
useful prediction interval indicator. In contrast to baseline approaches for
prediction interval estimation, we show that the CQR method provides prediction
intervals that adjust to the black hole mass and its related properties. That
is it yields a tighter constraint on the prediction interval (hence more
certain) for a larger black hole mass, and accordingly, bright and broad
spectral line width source. Using a combination of neural network model and CQR
framework, the recovered virial black hole mass predictions and uncertainties
are comparable to those measured from the Sloan Digital Sky Survey. The code is
publicly available at https://github.com/yongsukyee/uncertain_blackholemass.",http://arxiv.org/pdf/2307.04993v1,cs.LG
2023-07-07 02:42:06+00:00,TRAC: Trustworthy Retrieval Augmented Chatbot,"['Shuo Li', 'Sangdon Park', 'Insup Lee', 'Osbert Bastani']","Although conversational AIs have demonstrated fantastic performance, they
often generate incorrect information, or hallucinations. Retrieval augmented
generation has emerged as a promising solution to reduce these hallucinations.
However, these techniques still cannot guarantee correctness. Focusing on
question answering, we propose a framework that can provide statistical
guarantees for the retrieval augmented question answering system by combining
conformal prediction and global testing. In addition, we use Bayesian
optimization to choose hyperparameters of the global test to maximize the
performance of the system. Our empirical results on the Natural Questions
dataset demonstrate that our method can provide the desired coverage guarantee
while minimizing the average prediction set size.",http://arxiv.org/pdf/2307.04642v1,cs.CL
2023-07-06 13:35:06+00:00,How word semantics and phonology affect handwriting of Alzheimer's patients: a machine learning based analysis,"['Nicole Dalia Cilia', 'Claudio De Stefano', 'Francesco Fontanella', 'Sabato Marco Siniscalchi']","Using kinematic properties of handwriting to support the diagnosis of
neurodegenerative disease is a real challenge: non-invasive detection
techniques combined with machine learning approaches promise big steps forward
in this research field. In literature, the tasks proposed focused on different
cognitive skills to elicitate handwriting movements. In particular, the meaning
and phonology of words to copy can compromise writing fluency. In this paper,
we investigated how word semantics and phonology affect the handwriting of
people affected by Alzheimer's disease. To this aim, we used the data from six
handwriting tasks, each requiring copying a word belonging to one of the
following categories: regular (have a predictable phoneme-grapheme
correspondence, e.g., cat), non-regular (have atypical phoneme-grapheme
correspondence, e.g., laugh), and non-word (non-meaningful pronounceable letter
strings that conform to phoneme-grapheme conversion rules). We analyzed the
data using a machine learning approach by implementing four well-known and
widely-used classifiers and feature selection. The experimental results showed
that the feature selection allowed us to derive a different set of highly
distinctive features for each word type. Furthermore, non-regular words needed,
on average, more features but achieved excellent classification performance:
the best result was obtained on a non-regular, reaching an accuracy close to
90%.",http://arxiv.org/pdf/2307.04762v1,cs.CL
2023-07-04 21:25:12+00:00,Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners,"['Allen Z. Ren', 'Anushri Dixit', 'Alexandra Bodrova', 'Sumeet Singh', 'Stephen Tu', 'Noah Brown', 'Peng Xu', 'Leila Takayama', 'Fei Xia', 'Jake Varley', 'Zhenjia Xu', 'Dorsa Sadigh', 'Andy Zeng', 'Anirudha Majumdar']","Large language models (LLMs) exhibit a wide range of promising capabilities
-- from step-by-step planning to commonsense reasoning -- that may provide
utility for robots, but remain prone to confidently hallucinated predictions.
In this work, we present KnowNo, which is a framework for measuring and
aligning the uncertainty of LLM-based planners such that they know when they
don't know and ask for help when needed. KnowNo builds on the theory of
conformal prediction to provide statistical guarantees on task completion while
minimizing human help in complex multi-step planning settings. Experiments
across a variety of simulated and real robot setups that involve tasks with
different modes of ambiguity (e.g., from spatial to numeric uncertainties, from
human preferences to Winograd schemas) show that KnowNo performs favorably over
modern baselines (which may involve ensembles or extensive prompt tuning) in
terms of improving efficiency and autonomy, while providing formal assurances.
KnowNo can be used with LLMs out of the box without model-finetuning, and
suggests a promising lightweight approach to modeling uncertainty that can
complement and scale with the growing capabilities of foundation models.
Website: https://robot-help.github.io",http://arxiv.org/pdf/2307.01928v2,cs.RO
2023-07-03 15:08:28+00:00,Empirically Validating Conformal Prediction on Modern Vision Architectures Under Distribution Shift and Long-tailed Data,"['Kevin Kasa', 'Graham W. Taylor']","Conformal prediction has emerged as a rigorous means of providing deep
learning models with reliable uncertainty estimates and safety guarantees. Yet,
its performance is known to degrade under distribution shift and long-tailed
class distributions, which are often present in real world applications. Here,
we characterize the performance of several post-hoc and training-based
conformal prediction methods under these settings, providing the first
empirical evaluation on large-scale datasets and models. We show that across
numerous conformal methods and neural network families, performance greatly
degrades under distribution shifts violating safety guarantees. Similarly, we
show that in long-tailed settings the guarantees are frequently violated on
many classes. Understanding the limitations of these methods is necessary for
deployment in real world and safety-critical applications.",http://arxiv.org/pdf/2307.01088v1,cs.LG
2023-06-30 17:26:49+00:00,Bayesian Optimization with Formal Safety Guarantees via Online Conformal Prediction,"['Yunchuan Zhang', 'Sangwoo Park', 'Osvaldo Simeone']","Black-box zero-th order optimization is a central primitive for applications
in fields as diverse as finance, physics, and engineering. In a common
formulation of this problem, a designer sequentially attempts candidate
solutions, receiving noisy feedback on the value of each attempt from the
system. In this paper, we study scenarios in which feedback is also provided on
the safety of the attempted solution, and the optimizer is constrained to limit
the number of unsafe solutions that are tried throughout the optimization
process. Focusing on methods based on Bayesian optimization (BO), prior art has
introduced an optimization scheme -- referred to as SAFEOPT -- that is
guaranteed not to select any unsafe solution with a controllable probability
over feedback noise as long as strict assumptions on the safety constraint
function are met. In this paper, a novel BO-based approach is introduced that
satisfies safety requirements irrespective of properties of the constraint
function. This strong theoretical guarantee is obtained at the cost of allowing
for an arbitrary, controllable but non-zero, rate of violation of the safety
constraint. The proposed method, referred to as SAFE-BOCP, builds on online
conformal prediction (CP) and is specialized to the cases in which feedback on
the safety constraint is either noiseless or noisy. Experimental results on
synthetic and real-world data validate the advantages and flexibility of the
proposed SAFE-BOCP.",http://arxiv.org/pdf/2306.17815v1,cs.LG
2023-06-28 20:38:37+00:00,UTOPIA: Universally Trainable Optimal Prediction Intervals Aggregation,"['Jianqing Fan', 'Jiawei Ge', 'Debarghya Mukherjee']","Uncertainty quantification for prediction is an intriguing problem with
significant applications in various fields, such as biomedical science,
economic studies, and weather forecasts. Numerous methods are available for
constructing prediction intervals, such as quantile regression and conformal
predictions, among others. Nevertheless, model misspecification (especially in
high-dimension) or sub-optimal constructions can frequently result in biased or
unnecessarily-wide prediction intervals. In this paper, we propose a novel and
widely applicable technique for aggregating multiple prediction intervals to
minimize the average width of the prediction band along with coverage
guarantee, called Universally Trainable Optimal Predictive Intervals
Aggregation (UTOPIA). The method also allows us to directly construct
predictive bands based on elementary basis functions. Our approach is based on
linear or convex programming which is easy to implement. All of our proposed
methodologies are supported by theoretical guarantees on the coverage
probability and optimal average length, which are detailed in this paper. The
effectiveness of our approach is convincingly demonstrated by applying it to
synthetic data and two real datasets on finance and macroeconomics.",http://arxiv.org/pdf/2306.16549v1,stat.ME
2023-06-26 20:04:23+00:00,Action Anticipation with Goal Consistency,"['Olga Zatsarynna', 'Juergen Gall']","In this paper, we address the problem of short-term action anticipation,
i.e., we want to predict an upcoming action one second before it happens. We
propose to harness high-level intent information to anticipate actions that
will take place in the future. To this end, we incorporate an additional goal
prediction branch into our model and propose a consistency loss function that
encourages the anticipated actions to conform to the high-level goal pursued in
the video. In our experiments, we show the effectiveness of the proposed
approach and demonstrate that our method achieves state-of-the-art results on
two large-scale datasets: Assembly101 and COIN.",http://arxiv.org/pdf/2306.15045v1,cs.CV
2023-06-26 17:02:54+00:00,CoarsenConf: Equivariant Coarsening with Aggregated Attention for Molecular Conformer Generation,"['Danny Reidenbach', 'Aditi S. Krishnapriyan']","Molecular conformer generation (MCG) is an important task in cheminformatics
and drug discovery. The ability to efficiently generate low-energy 3D
structures can avoid expensive quantum mechanical simulations, leading to
accelerated virtual screenings and enhanced structural exploration. Several
generative models have been developed for MCG, but many struggle to
consistently produce high-quality conformers. To address these issues, we
introduce CoarsenConf, which coarse-grains molecular graphs based on torsional
angles and integrates them into an SE(3)-equivariant hierarchical variational
autoencoder. Through equivariant coarse-graining, we aggregate the fine-grained
atomic coordinates of subgraphs connected via rotatable bonds, creating a
variable-length coarse-grained latent representation. Our model uses a novel
aggregated attention mechanism to restore fine-grained coordinates from the
coarse-grained latent representation, enabling efficient generation of accurate
conformers. Furthermore, we evaluate the chemical and biochemical quality of
our generated conformers on multiple downstream applications, including
property prediction and oracle-based protein docking. Overall, CoarsenConf
generates more accurate conformer ensembles compared to prior generative
models.",http://arxiv.org/pdf/2306.14852v2,cs.LG
2023-06-26 13:38:49+00:00,Conformal link prediction to control the error rate,['Ariane Marandon'],"Most link prediction methods return estimates of the connection probability
of missing edges in a graph. Such output can be used to rank the missing edges,
from most to least likely to be a true edge, but it does not directly provide a
classification into true and non-existent. In this work, we consider the
problem of identifying a set of true edges with a control of the false
discovery rate (FDR). We propose a novel method based on high-level ideas from
the literature on conformal inference. The graph structure induces intricate
dependence in the data, which we carefully take into account, as this makes the
setup different from the usual setup in conformal inference, where
exchangeability is assumed. The FDR control is empirically demonstrated for
both simulated and real data.",http://arxiv.org/pdf/2306.14693v1,stat.ME
2023-06-20 16:58:19+00:00,MoleCLUEs: Optimizing Molecular Conformers by Minimization of Differentiable Uncertainty,"['Michael Maser', 'Natasa Tagasovska', 'Jae Hyeon Lee', 'Andrew Watkins']","Structure-based models in the molecular sciences can be highly sensitive to
input geometries and give predictions with large variance under subtle
coordinate perturbations. We present an approach to mitigate this failure mode
by generating conformations that explicitly minimize uncertainty in a
predictive model. To achieve this, we compute differentiable estimates of
aleatoric \textit{and} epistemic uncertainties directly from learned
embeddings. We then train an optimizer that iteratively samples embeddings to
reduce these uncertainties according to their gradients. As our predictive
model is constructed as a variational autoencoder, the new embeddings can be
decoded to their corresponding inputs, which we call \textit{MoleCLUEs}, or
(molecular) counterfactual latent uncertainty explanations
\citep{antoran2020getting}. We provide results of our algorithm for the task of
predicting drug properties with maximum confidence as well as analysis of the
differentiable structure simulations.",http://arxiv.org/pdf/2306.11681v1,cs.LG
2023-06-19 19:03:53+00:00,CAMMARL: Conformal Action Modeling in Multi Agent Reinforcement Learning,"['Nikunj Gupta', 'Samira Ebrahimi Kahou']","Before taking actions in an environment with more than one intelligent agent,
an autonomous agent may benefit from reasoning about the other agents and
utilizing a notion of a guarantee or confidence about the behavior of the
system. In this article, we propose a novel multi-agent reinforcement learning
(MARL) algorithm CAMMARL, which involves modeling the actions of other agents
in different situations in the form of confident sets, i.e., sets containing
their true actions with a high probability. We then use these estimates to
inform an agent's decision-making. For estimating such sets, we use the concept
of conformal predictions, by means of which, we not only obtain an estimate of
the most probable outcome but get to quantify the operable uncertainty as well.
For instance, we can predict a set that provably covers the true predictions
with high probabilities (e.g., 95%). Through several experiments in two fully
cooperative multi-agent tasks, we show that CAMMARL elevates the capabilities
of an autonomous agent in MARL by modeling conformal prediction sets over the
behavior of other agents in the environment and utilizing such estimates to
enhance its policy learning. All developed codes can be found here:
https://github.com/Nikunj-Gupta/conformal-agent-modelling.",http://arxiv.org/pdf/2306.11128v1,cs.LG
2023-06-16 21:55:08+00:00,Conformal Language Modeling,"['Victor Quach', 'Adam Fisch', 'Tal Schuster', 'Adam Yala', 'Jae Ho Sohn', 'Tommi S. Jaakkola', 'Regina Barzilay']","We propose a novel approach to conformal prediction for generative language
models (LMs). Standard conformal prediction produces prediction sets -- in
place of single predictions -- that have rigorous, statistical performance
guarantees. LM responses are typically sampled from the model's predicted
distribution over the large, combinatorial output space of natural language.
Translating this process to conformal prediction, we calibrate a stopping rule
for sampling different outputs from the LM that get added to a growing set of
candidates until we are confident that the output set is sufficient. Since some
samples may be low-quality, we also simultaneously calibrate and apply a
rejection rule for removing candidates from the output set to reduce noise.
Similar to conformal prediction, we prove that the sampled set returned by our
procedure contains at least one acceptable answer with high probability, while
still being empirically precise (i.e., small) on average. Furthermore, within
this set of candidate responses, we show that we can also accurately identify
subsets of individual components -- such as phrases or sentences -- that are
each independently correct (e.g., that are not ""hallucinations""), again with
statistical guarantees. We demonstrate the promise of our approach on multiple
tasks in open-domain question answering, text summarization, and radiology
report generation using different LM variants.",http://arxiv.org/pdf/2306.10193v1,cs.CL
2023-06-15 17:59:02+00:00,Class-Conditional Conformal Prediction with Many Classes,"['Tiffany Ding', 'Anastasios N. Angelopoulos', 'Stephen Bates', 'Michael I. Jordan', 'Ryan J. Tibshirani']","Standard conformal prediction methods provide a marginal coverage guarantee,
which means that for a random test point, the conformal prediction set contains
the true label with a user-specified probability. In many classification
problems, we would like to obtain a stronger guarantee--that for test points of
a specific class, the prediction set contains the true label with the same
user-chosen probability. For the latter goal, existing conformal prediction
methods do not work well when there is a limited amount of labeled data per
class, as is often the case in real applications where the number of classes is
large. We propose a method called clustered conformal prediction that clusters
together classes having ""similar"" conformal scores and performs conformal
prediction at the cluster level. Based on empirical evaluation across four
image data sets with many (up to 1000) classes, we find that clustered
conformal typically outperforms existing methods in terms of class-conditional
coverage and set size metrics.",http://arxiv.org/pdf/2306.09335v2,stat.ML
2023-06-15 13:15:26+00:00,On Certified Generalization in Structured Prediction,"['Bastian Boll', 'Christoph Schnörr']","In structured prediction, target objects have rich internal structure which
does not factorize into independent components and violates common i.i.d.
assumptions. This challenge becomes apparent through the exponentially large
output space in applications such as image segmentation or scene graph
generation. We present a novel PAC-Bayesian risk bound for structured
prediction wherein the rate of generalization scales not only with the number
of structured examples but also with their size. The underlying assumption,
conforming to ongoing research on generative models, is that data are generated
by the Knothe-Rosenblatt rearrangement of a factorizing reference measure. This
allows to explicitly distill the structure between random output variables into
a Wasserstein dependency matrix. Our work makes a preliminary step towards
leveraging powerful generative models to establish generalization bounds for
discriminative downstream tasks in the challenging setting of structured
prediction.",http://arxiv.org/pdf/2306.09112v2,stat.ML
2023-06-14 18:28:53+00:00,Integrating Uncertainty Awareness into Conformalized Quantile Regression,"['Raphael Rossellini', 'Rina Foygel Barber', 'Rebecca Willett']","Conformalized Quantile Regression (CQR) is a recently proposed method for
constructing prediction intervals for a response $Y$ given covariates $X$,
without making distributional assumptions. However, as we demonstrate
empirically, existing constructions of CQR can be ineffective for problems
where the quantile regressors perform better in certain parts of the feature
space than others. The reason is that the prediction intervals of CQR do not
distinguish between two forms of uncertainty: first, the variability of the
conditional distribution of $Y$ given $X$ (i.e., aleatoric uncertainty), and
second, our uncertainty in estimating this conditional distribution (i.e.,
epistemic uncertainty). This can lead to uneven coverage, with intervals that
are overly wide (or overly narrow) in regions where epistemic uncertainty is
low (or high). To address this, we propose a new variant of the CQR
methodology, Uncertainty-Aware CQR (UACQR), that explicitly separates these two
sources of uncertainty to adjust quantile regressors differentially across the
feature space. Compared to CQR, our methods enjoy the same distribution-free
theoretical guarantees for coverage properties, while demonstrating in our
experiments stronger conditional coverage in simulated settings and tighter
intervals on a range of real-world data sets.",http://arxiv.org/pdf/2306.08693v1,stat.ME
2023-06-12 17:22:57+00:00,On the Expected Size of Conformal Prediction Sets,"['Guneet S. Dhillon', 'George Deligiannidis', 'Tom Rainforth']","While conformal predictors reap the benefits of rigorous statistical
guarantees for their error frequency, the size of their corresponding
prediction sets is critical to their practical utility. Unfortunately, there is
currently a lack of finite-sample analysis and guarantees for their prediction
set sizes. To address this shortfall, we theoretically quantify the expected
size of the prediction set under the split conformal prediction framework. As
this precise formulation cannot usually be calculated directly, we further
derive point estimates and high probability intervals that can be easily
computed, providing a practical method for characterizing the expected
prediction set size across different possible realizations of the test and
calibration data. Additionally, we corroborate the efficacy of our results with
experiments on real-world datasets, for both regression and classification
problems.",http://arxiv.org/pdf/2306.07254v1,stat.ML
2023-06-11 22:14:21+00:00,A Neural Network Implementation for Free Energy Principle,['Jingwei Liu'],"The free energy principle (FEP), as an encompassing framework and a unified
brain theory, has been widely applied to account for various problems in fields
such as cognitive science, neuroscience, social interaction, and hermeneutics.
As a computational model deeply rooted in math and statistics, FEP posits an
optimization problem based on variational Bayes, which is solved either by
dynamic programming or expectation maximization in practice. However, there
seems to be a bottleneck in extending the FEP to machine learning and
implementing such models with neural networks. This paper gives a preliminary
attempt at bridging FEP and machine learning, via a classical neural network
model, the Helmholtz machine. As a variational machine learning model, the
Helmholtz machine is optimized by minimizing its free energy, the same
objective as FEP. Although the Helmholtz machine is not temporal, it gives an
ideal parallel to the vanilla FEP and the hierarchical model of the brain,
under which the active inference and predictive coding could be formulated
coherently. Besides a detailed theoretical discussion, the paper also presents
a preliminary experiment to validate the hypothesis. By fine-tuning the trained
neural network through active inference, the model performance is promoted to
accuracy above 99\%. In the meantime, the data distribution is continuously
deformed to a salience that conforms to the model representation, as a result
of active sampling.",http://arxiv.org/pdf/2306.06792v1,cs.NE
2023-06-09 19:36:18+00:00,Conformalizing Machine Translation Evaluation,"['Chrysoula Zerva', 'André F. T. Martins']","Several uncertainty estimation methods have been recently proposed for
machine translation evaluation. While these methods can provide a useful
indication of when not to trust model predictions, we show in this paper that
the majority of them tend to underestimate model uncertainty, and as a result
they often produce misleading confidence intervals that do not cover the ground
truth. We propose as an alternative the use of conformal prediction, a
distribution-free method to obtain confidence intervals with a theoretically
established guarantee on coverage. First, we demonstrate that split conformal
prediction can ``correct'' the confidence intervals of previous methods to
yield a desired coverage level. Then, we highlight biases in estimated
confidence intervals, both in terms of the translation language pairs and the
quality of translations. We apply conditional conformal prediction techniques
to obtain calibration subsets for each data subgroup, leading to equalized
coverage.",http://arxiv.org/pdf/2306.06221v1,cs.CL
2023-06-08 11:54:58+00:00,Conformal Prediction for Federated Uncertainty Quantification Under Label Shift,"['Vincent Plassier', 'Mehdi Makni', 'Aleksandr Rubashevskii', 'Eric Moulines', 'Maxim Panov']","Federated Learning (FL) is a machine learning framework where many clients
collaboratively train models while keeping the training data decentralized.
Despite recent advances in FL, the uncertainty quantification topic (UQ)
remains partially addressed. Among UQ methods, conformal prediction (CP)
approaches provides distribution-free guarantees under minimal assumptions. We
develop a new federated conformal prediction method based on quantile
regression and take into account privacy constraints. This method takes
advantage of importance weighting to effectively address the label shift
between agents and provides theoretical guarantees for both valid coverage of
the prediction sets and differential privacy. Extensive experimental studies
demonstrate that this method outperforms current competitors.",http://arxiv.org/pdf/2306.05131v2,stat.ML
2023-06-06 18:00:09+00:00,Designing Decision Support Systems Using Counterfactual Prediction Sets,"['Eleni Straitouri', 'Manuel Gomez Rodriguez']","Decision support systems for classification tasks are predominantly designed
to predict the value of the ground truth labels. However, since their
predictions are not perfect, these systems also need to make human experts
understand when and how to use these predictions to update their own
predictions. Unfortunately, this has been proven challenging. In this context,
it has been recently argued that an alternative type of decision support
systems may circumvent this challenge. Rather than providing a single label
prediction, these systems provide a set of label prediction values constructed
using a conformal predictor, namely a prediction set, and forcefully ask
experts to predict a label value from the prediction set. However, the design
and evaluation of these systems have so far relied on stylized expert models,
questioning their promise. In this paper, we revisit the design of this type of
systems from the perspective of online learning and develop a methodology that
does not require, nor assumes, an expert model. Our methodology leverages the
nested structure of the prediction sets provided by any conformal predictor and
a natural counterfactual monotonicity assumption on the experts' predictions
over the prediction sets to achieve an exponential improvement in regret in
comparison with vanilla bandit algorithms. We conduct a large-scale human
subject study ($n = 2{,}751$) to verify our counterfactual monotonicity
assumption and compare our methodology to several competitive baselines. The
results suggest that decision support systems that limit experts' level of
agency may be practical and may offer greater performance than those allowing
experts to always exercise their own agency.",http://arxiv.org/pdf/2306.03928v1,cs.LG
2023-06-05 13:57:23+00:00,On training locally adaptive CP,['Nicolo Colombo'],"We address the problem of making Conformal Prediction (CP) intervals locally
adaptive. Most existing methods focus on approximating the object-conditional
validity of the intervals by partitioning or re-weighting the calibration set.
Our strategy is new and conceptually different. Instead of re-weighting the
calibration data, we redefine the conformity measure through a trainable change
of variables, $A \to \phi_X(A)$, that depends explicitly on the object
attributes, $X$. Under certain conditions and if $\phi_X$ is monotonic in $A$
for any $X$, the transformations produce prediction intervals that are
guaranteed to be marginally valid and have $X$-dependent sizes. We describe how
to parameterize and train $\phi_X$ to maximize the interval efficiency.
Contrary to other CP-aware training methods, the objective function is smooth
and can be minimized through standard gradient methods without approximations.",http://arxiv.org/pdf/2306.04648v1,cs.LG
2023-06-05 09:33:39+00:00,A Large-Scale Study of Probabilistic Calibration in Neural Network Regression,"['Victor Dheur', 'Souhaib Ben Taieb']","Accurate probabilistic predictions are essential for optimal decision making.
While neural network miscalibration has been studied primarily in
classification, we investigate this in the less-explored domain of regression.
We conduct the largest empirical study to date to assess the probabilistic
calibration of neural networks. We also analyze the performance of
recalibration, conformal, and regularization methods to enhance probabilistic
calibration. Additionally, we introduce novel differentiable recalibration and
regularization methods, uncovering new insights into their effectiveness. Our
findings reveal that regularization methods offer a favorable tradeoff between
calibration and sharpness. Post-hoc methods exhibit superior probabilistic
calibration, which we attribute to the finite-sample coverage guarantee of
conformal prediction. Furthermore, we demonstrate that quantile recalibration
can be considered as a specific case of conformal prediction. Our study is
fully reproducible and implemented in a common code base for fair comparisons.",http://arxiv.org/pdf/2306.02738v2,cs.LG
2023-06-05 09:28:03+00:00,Conformal Prediction with Missing Values,"['Margaux Zaffran', 'Aymeric Dieuleveut', 'Julie Josse', 'Yaniv Romano']","Conformal prediction is a theoretically grounded framework for constructing
predictive intervals. We study conformal prediction with missing values in the
covariates -- a setting that brings new challenges to uncertainty
quantification. We first show that the marginal coverage guarantee of conformal
prediction holds on imputed data for any missingness distribution and almost
all imputation functions. However, we emphasize that the average coverage
varies depending on the pattern of missing values: conformal methods tend to
construct prediction intervals that under-cover the response conditionally to
some missing patterns. This motivates our novel generalized conformalized
quantile regression framework, missing data augmentation, which yields
prediction intervals that are valid conditionally to the patterns of missing
values, despite their exponential number. We then show that a universally
consistent quantile regression algorithm trained on the imputed data is Bayes
optimal for the pinball risk, thus achieving valid coverage conditionally to
any given data point. Moreover, we examine the case of a linear model, which
demonstrates the importance of our proposal in overcoming the
heteroskedasticity induced by missing values. Using synthetic and data from
critical care, we corroborate our theory and report improved performance of our
methods.",http://arxiv.org/pdf/2306.02732v1,stat.ML
2023-06-05 02:55:17+00:00,Conformal Predictive Safety Filter for RL Controllers in Dynamic Environments,"['Kegan J. Strawn', 'Nora Ayanian', 'Lars Lindemann']","The interest in using reinforcement learning (RL) controllers in
safety-critical applications such as robot navigation around pedestrians
motivates the development of additional safety mechanisms. Running RL-enabled
systems among uncertain dynamic agents may result in high counts of collisions
and failures to reach the goal. The system could be safer if the pre-trained RL
policy was uncertainty-informed. For that reason, we propose conformal
predictive safety filters that: 1) predict the other agents' trajectories, 2)
use statistical techniques to provide uncertainty intervals around these
predictions, and 3) learn an additional safety filter that closely follows the
RL controller but avoids the uncertainty intervals. We use conformal prediction
to learn uncertainty-informed predictive safety filters, which make no
assumptions about the agents' distribution. The framework is modular and
outperforms the existing controllers in simulation. We demonstrate our approach
with multiple experiments in a collision avoidance gym environment and show
that our approach minimizes the number of collisions without making
overly-conservative predictions.",http://arxiv.org/pdf/2306.02551v2,cs.RO
2023-06-02 13:56:30+00:00,Evaluating Machine Translation Quality with Conformal Predictive Distributions,['Patrizio Giovannotti'],"This paper presents a new approach for assessing uncertainty in machine
translation by simultaneously evaluating translation quality and providing a
reliable confidence score. Our approach utilizes conformal predictive
distributions to produce prediction intervals with guaranteed coverage, meaning
that for any given significance level $\epsilon$, we can expect the true
quality score of a translation to fall out of the interval at a rate of
$1-\epsilon$. In this paper, we demonstrate how our method outperforms a
simple, but effective baseline on six different language pairs in terms of
coverage and sharpness. Furthermore, we validate that our approach requires the
data exchangeability assumption to hold for optimal performance.",http://arxiv.org/pdf/2306.01549v1,cs.CL
2023-06-01 23:10:15+00:00,Conformal Prediction with Partially Labeled Data,"['Alireza Javanmardi', 'Yusuf Sale', 'Paul Hofman', 'Eyke Hüllermeier']","While the predictions produced by conformal prediction are set-valued, the
data used for training and calibration is supposed to be precise. In the
setting of superset learning or learning from partial labels, a variant of
weakly supervised learning, it is exactly the other way around: training data
is possibly imprecise (set-valued), but the model induced from this data yields
precise predictions. In this paper, we combine the two settings by making
conformal prediction amenable to set-valued training data. We propose a
generalization of the conformal prediction procedure that can be applied to
set-valued training and calibration data. We prove the validity of the proposed
method and present experimental studies in which it compares favorably to
natural baselines.",http://arxiv.org/pdf/2306.01191v1,cs.LG
2023-06-01 16:37:50+00:00,Quantifying Deep Learning Model Uncertainty in Conformal Prediction,"['Hamed Karimi', 'Reza Samavi']","Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.",http://arxiv.org/pdf/2306.00876v1,cs.LG
2023-06-01 14:02:46+00:00,UNGOML: Automated Classification of unsafe Usages in Go,"['Anna-Katharina Wickert', 'Clemens Damke', 'Lars Baumgärtner', 'Eyke Hüllermeier', 'Mira Mezini']","The Go programming language offers strong protection from memory corruption.
As an escape hatch of these protections, it provides the unsafe package.
Previous studies identified that this unsafe package is frequently used in
real-world code for several purposes, e.g., serialization or casting types. Due
to the variety of these reasons, it may be possible to refactor specific usages
to avoid potential vulnerabilities. However, the classification of unsafe
usages is challenging and requires the context of the call and the program's
structure. In this paper, we present the first automated classifier for unsafe
usages in Go, UNGOML, to identify what is done with the unsafe package and why
it is used. For UNGOML, we built four custom deep learning classifiers trained
on a manually labeled data set. We represent Go code as enriched control-flow
graphs (CFGs) and solve the label prediction task with one single-vertex and
three context-aware classifiers. All three context-aware classifiers achieve a
top-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore,
in a set-valued conformal prediction setting, we achieve accuracies of more
than 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML can
be used to efficiently filter unsafe usages for use cases such as refactoring
or a security audit. UNGOML: https://github.com/stg-tud/ungoml Artifact:
https://dx.doi.org/10.6084/m9.figshare.22293052",http://arxiv.org/pdf/2306.00694v1,cs.SE
2023-06-01 04:11:22+00:00,Enterprise Disk Drive Scrubbing Based on Mondrian Conformal Predictors,"['Rahul Vishwakarma', 'Jinha Hwang', 'Soundouss Messoudi', 'Ava Hedayatipour']","Disk scrubbing is a process aimed at resolving read errors on disks by
reading data from the disk. However, scrubbing the entire storage array at once
can adversely impact system performance, particularly during periods of high
input/output operations. Additionally, the continuous reading of data from
disks when scrubbing can result in wear and tear, especially on larger capacity
disks, due to the significant time and energy consumption involved. To address
these issues, we propose a selective disk scrubbing method that enhances the
overall reliability and power efficiency in data centers. Our method employs a
Machine Learning model based on Mondrian Conformal prediction to identify
specific disks for scrubbing, by proactively predicting the health status of
each disk in the storage pool, forecasting n-days in advance, and using an
open-source dataset. For disks predicted as non-healthy, we mark them for
replacement without further action. For healthy drives, we create a set and
quantify their relative health across the entire storage pool based on the
predictor's confidence. This enables us to prioritize selective scrubbing for
drives with established scrubbing frequency based on the scrub cycle. The
method we propose provides an efficient and dependable solution for managing
enterprise disk drives. By scrubbing just 22.7% of the total storage disks, we
can achieve optimized energy consumption and reduce the carbon footprint of the
data center.",http://arxiv.org/pdf/2306.17169v1,cs.DC
2023-05-31 14:32:26+00:00,Adaptive Conformal Regression with Jackknife+ Rescaled Scores,"['Nicolas Deutschmann', 'Mattia Rigotti', 'Maria Rodriguez Martinez']","Conformal regression provides prediction intervals with global coverage
guarantees, but often fails to capture local error distributions, leading to
non-homogeneous coverage. We address this with a new adaptive method based on
rescaling conformal scores with an estimate of local score distribution,
inspired by the Jackknife+ method, which enables the use of calibration data in
conformal scores without breaking calibration-test exchangeability. Our
approach ensures formal global coverage guarantees and is supported by new
theoretical results on local coverage, including an a posteriori bound on any
calibration score. The strength of our approach lies in achieving local
coverage without sacrificing calibration set size, improving the applicability
of conformal prediction intervals in various settings. As a result, our method
provides prediction intervals that outperform previous methods, particularly in
the low-data regime, making it especially relevant for real-world applications
such as healthcare and biomedical domains where uncertainty needs to be
quantified accurately despite low sample data.",http://arxiv.org/pdf/2305.19901v1,cs.LG
2023-05-28 15:26:10+00:00,Conformal Prediction with Large Language Models for Multi-Choice Question Answering,"['Bhawesh Kumar', 'Charlie Lu', 'Gauri Gupta', 'Anil Palepu', 'David Bellamy', 'Ramesh Raskar', 'Andrew Beam']","As large language models continue to be widely developed, robust uncertainty
quantification techniques will become crucial for their safe deployment in
high-stakes scenarios. In this work, we explore how conformal prediction can be
used to provide uncertainty quantification in language models for the specific
task of multiple-choice question-answering. We find that the uncertainty
estimates from conformal prediction are tightly correlated with prediction
accuracy. This observation can be useful for downstream applications such as
selective classification and filtering out low-quality predictions. We also
investigate the exchangeability assumption required by conformal prediction to
out-of-subject questions, which may be a more realistic scenario for many
practical applications. Our work contributes towards more trustworthy and
reliable usage of large language models in safety-critical situations, where
robust guarantees of error rate are required.",http://arxiv.org/pdf/2305.18404v3,cs.CL
2023-05-27 19:57:27+00:00,Federated Conformal Predictors for Distributed Uncertainty Quantification,"['Charles Lu', 'Yaodong Yu', 'Sai Praneeth Karimireddy', 'Michael I. Jordan', 'Ramesh Raskar']","Conformal prediction is emerging as a popular paradigm for providing rigorous
uncertainty quantification in machine learning since it can be easily applied
as a post-processing step to already trained models. In this paper, we extend
conformal prediction to the federated learning setting. The main challenge we
face is data heterogeneity across the clients - this violates the fundamental
tenet of exchangeability required for conformal prediction. We propose a weaker
notion of partial exchangeability, better suited to the FL setting, and use it
to develop the Federated Conformal Prediction (FCP) framework. We show FCP
enjoys rigorous theoretical guarantees and excellent empirical performance on
several computer vision and medical imaging datasets. Our results demonstrate a
practical approach to incorporating meaningful uncertainty quantification in
distributed and heterogeneous environments. We provide code used in our
experiments https://github.com/clu5/federated-conformal.",http://arxiv.org/pdf/2305.17564v2,cs.LG
2023-05-26 02:15:26+00:00,Detecting Errors in Numerical Data via any Regression Model,"['Hang Zhou', 'Jonas Mueller', 'Mayank Kumar', 'Jane-Ling Wang', 'Jing Lei']","Noise plagues many numerical datasets, where the recorded values in the data
may fail to match the true underlying values due to reasons including:
erroneous sensors, data entry/processing mistakes, or imperfect human
estimates. Here we consider estimating which data values are incorrect along a
numerical column. We present a model-agnostic approach that can utilize any
regressor (i.e. statistical or machine learning model) which was fit to predict
values in this column based on the other variables in the dataset. By
accounting for various uncertainties, our approach distinguishes between
genuine anomalies and natural data fluctuations, conditioned on the available
information in the dataset. We establish theoretical guarantees for our method
and show that other approaches like conformal inference struggle to detect
errors. We also contribute a new error detection benchmark involving 5
regression datasets with real-world numerical errors (for which the true values
are also known). In this benchmark and additional simulation studies, our
method identifies incorrect values with better precision/recall than other
approaches.",http://arxiv.org/pdf/2305.16583v2,stat.ML
2023-05-25 17:39:13+00:00,CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graphs,"['Guangyao Zhai', 'Evin Pınar Örnek', 'Shun-Cheng Wu', 'Yan Di', 'Federico Tombari', 'Nassir Navab', 'Benjamin Busam']","Controllable scene synthesis aims to create interactive environments for
various industrial use cases. Scene graphs provide a highly suitable interface
to facilitate these applications by abstracting the scene context in a compact
manner. Existing methods, reliant on retrieval from extensive databases or
pre-trained shape embeddings, often overlook scene-object and object-object
relationships, leading to inconsistent results due to their limited generation
capacity. To address this issue, we present CommonScenes, a fully generative
model that converts scene graphs into corresponding controllable 3D scenes,
which are semantically realistic and conform to commonsense. Our pipeline
consists of two branches, one predicting the overall scene layout via a
variational auto-encoder and the other generating compatible shapes via latent
diffusion, capturing global scene-object and local inter-object relationships
while preserving shape diversity. The generated scenes can be manipulated by
editing the input scene graph and sampling the noise in the diffusion model.
Due to lacking a scene graph dataset offering high-quality object-level meshes
with relations, we also construct SG-FRONT, enriching the off-the-shelf indoor
dataset 3D-FRONT with additional scene graph labels. Extensive experiments are
conducted on SG-FRONT where CommonScenes shows clear advantages over other
methods regarding generation consistency, quality, and diversity. Codes and the
dataset will be released upon acceptance.",http://arxiv.org/pdf/2305.16283v4,cs.CV
2023-05-23 22:24:44+00:00,Sources of Hallucination by Large Language Models on Inference Tasks,"['Nick McKenna', 'Tianyi Li', 'Liang Cheng', 'Mohammad Javad Hosseini', 'Mark Johnson', 'Mark Steedman']","Large Language Models (LLMs) are claimed to be capable of Natural Language
Inference (NLI), necessary for applied tasks like question answering and
summarization. We present a series of behavioral studies on several LLM
families (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled
experiments. We establish two biases originating from pretraining which predict
much of their behavior, and show that these are major sources of hallucination
in generative LLMs. First, memorization at the level of sentences: we show
that, regardless of the premise, models falsely label NLI test samples as
entailing when the hypothesis is attested in training data, and that entities
are used as ``indices'' to access the memorized data. Second, statistical
patterns of usage learned at the level of corpora: we further show a similar
effect when the premise predicate is less frequent than that of the hypothesis
in the training data, a bias following from previous studies. We demonstrate
that LLMs perform significantly worse on NLI test samples which do not conform
to these biases than those which do, and we offer these as valuable controls
for future LLM evaluation.",http://arxiv.org/pdf/2305.14552v2,cs.CL
2023-05-23 21:38:23+00:00,Uncertainty Quantification over Graph with Conformalized Graph Neural Networks,"['Kexin Huang', 'Ying Jin', 'Emmanuel Candès', 'Jure Leskovec']","Graph Neural Networks (GNNs) are powerful machine learning prediction models
on graph-structured data. However, GNNs lack rigorous uncertainty estimates,
limiting their reliable deployment in settings where the cost of errors is
significant. We propose conformalized GNN (CF-GNN), extending conformal
prediction (CP) to graph-based models for guaranteed uncertainty estimates.
Given an entity in the graph, CF-GNN produces a prediction set/interval that
provably contains the true label with pre-defined coverage probability (e.g.
90%). We establish a permutation invariance condition that enables the validity
of CP on graph data and provide an exact characterization of the test-time
coverage. Moreover, besides valid coverage, it is crucial to reduce the
prediction set size/interval length for practical use. We observe a key
connection between non-conformity scores and network structures, which
motivates us to develop a topology-aware output correction model that learns to
update the prediction and produces more efficient prediction sets/intervals.
Extensive experiments show that CF-GNN achieves any pre-defined target marginal
coverage while significantly reducing the prediction set/interval size by up to
74% over the baselines. It also empirically achieves satisfactory conditional
coverage over various raw and network features.",http://arxiv.org/pdf/2305.14535v2,cs.LG
2023-05-23 17:24:04+00:00,Amortized Variational Inference with Coverage Guarantees,"['Yash Patel', 'Declan McNamara', 'Jackson Loper', 'Jeffrey Regier', 'Ambuj Tewari']","Amortized variational inference produces a posterior approximation that can
be rapidly computed given any new observation. Unfortunately, there are few
guarantees about the quality of these approximate posteriors. We propose
Conformalized Amortized Neural Variational Inference (CANVI), a procedure that
is scalable, easily implemented, and provides guaranteed marginal coverage.
Given a collection of candidate amortized posterior approximators, CANVI
constructs conformalized predictors based on each candidate, compares the
predictors using a metric known as predictive efficiency, and returns the most
efficient predictor. CANVI ensures that the resulting predictor constructs
regions that contain the truth with a user-specified level of probability.
CANVI is agnostic to design decisions in formulating the candidate
approximators and only requires access to samples from the forward model,
permitting its use in likelihood-free settings. We prove lower bounds on the
predictive efficiency of the regions produced by CANVI and explore how the
quality of a posterior approximation relates to the predictive efficiency of
prediction regions based on that approximation. Finally, we demonstrate the
accurate calibration and high predictive efficiency of CANVI on a suite of
simulation-based inference benchmark tasks and an important scientific task:
analyzing galaxy emission spectra.",http://arxiv.org/pdf/2305.14275v2,stat.ME
2023-05-22 03:48:38+00:00,Conformal Inference for Invariant Risk Minimization,"['Wenlu Tang', 'Zicheng Liu']","The application of machine learning models can be significantly impeded by
the occurrence of distributional shifts, as the assumption of homogeneity
between the population of training and testing samples in machine learning and
statistics may not be feasible in practical situations. One way to tackle this
problem is to use invariant learning, such as invariant risk minimization
(IRM), to acquire an invariant representation that aids in generalization with
distributional shifts. This paper develops methods for obtaining
distribution-free prediction regions to describe uncertainty estimates for
invariant representations, accounting for the distribution shifts of data from
different environments. Our approach involves a weighted conformity score that
adapts to the specific environment in which the test sample is situated. We
construct an adaptive conformal interval using the weighted conformity score
and prove its conditional average under certain conditions. To demonstrate the
effectiveness of our approach, we conduct several numerical experiments,
including simulation studies and a practical example using real-world data.",http://arxiv.org/pdf/2305.12686v1,stat.ML
2023-05-22 00:49:49+00:00,Conformal Prediction With Conditional Guarantees,"['Isaac Gibbs', 'John J. Cherian', 'Emmanuel J. Candès']","We consider the problem of constructing distribution-free prediction sets
with finite-sample conditional guarantees. Prior work has shown that it is
impossible to provide exact conditional coverage universally in finite samples.
Thus, most popular methods only provide marginal coverage over the covariates.
This paper bridges this gap by defining a spectrum of problems that interpolate
between marginal and conditional validity. We motivate these problems by
reformulating conditional coverage as coverage over a class of covariate
shifts. When the target class of shifts is finite dimensional, we show how to
simultaneously obtain exact finite sample coverage over all possible shifts.
For example, given a collection of protected subgroups, our algorithm outputs
intervals with exact coverage over each group. For more flexible, infinite
dimensional classes where exact coverage is impossible, we provide a simple
procedure for quantifying the gap between the coverage of our algorithm and the
target level. Moreover, by tuning a single hyperparameter, we allow the
practitioner to control the size of this gap across shifts of interest. Our
methods can be easily incorporated into existing split conformal inference
pipelines, and thus can be used to quantify the uncertainty of modern black-box
algorithms without distributional assumptions.",http://arxiv.org/pdf/2305.12616v2,stat.ME
2023-05-21 05:06:46+00:00,Are Your Explanations Reliable? Investigating the Stability of LIME in Explaining Text Classifiers by Marrying XAI and Adversarial Attack,"['Christopher Burger', 'Lingwei Chen', 'Thai Le']","LIME has emerged as one of the most commonly referenced tools in explainable
AI (XAI) frameworks that is integrated into critical machine learning
applications--e.g., healthcare and finance. However, its stability remains
little explored, especially in the context of text data, due to the unique
text-space constraints. To address these challenges, in this paper, we first
evaluate the inherent instability of LIME on text data to establish a baseline,
and then propose a novel algorithm XAIFooler to perturb text inputs and
manipulate explanations that casts investigation on the stability of LIME as a
text perturbation optimization problem. XAIFooler conforms to the constraints
to preserve text semantics and original prediction with small perturbations,
and introduces Rank-biased Overlap (RBO) as a key part to guide the
optimization of XAIFooler that satisfies all the requirements for explanation
similarity measure. Extensive experiments on real-world text datasets
demonstrate that XAIFooler significantly outperforms all baselines by large
margins in its ability to manipulate LIME's explanations with high semantic
preservability.",http://arxiv.org/pdf/2305.12351v2,cs.LG
2023-05-20 21:31:51+00:00,Distribution-Free Model-Agnostic Regression Calibration via Nonparametric Methods,"['Shang Liu', 'Zhongze Cai', 'Xiaocheng Li']","In this paper, we consider the uncertainty quantification problem for
regression models. Specifically, we consider an individual calibration
objective for characterizing the quantiles of the prediction model. While such
an objective is well-motivated from downstream tasks such as newsvendor cost,
the existing methods have been largely heuristic and lack of statistical
guarantee in terms of individual calibration. We show via simple examples that
the existing methods focusing on population-level calibration guarantees such
as average calibration or sharpness can lead to harmful and unexpected results.
We propose simple nonparametric calibration methods that are agnostic of the
underlying prediction model and enjoy both computational efficiency and
statistical consistency. Our approach enables a better understanding of the
possibility of individual calibration, and we establish matching upper and
lower bounds for the calibration error of our proposed methods. Technically,
our analysis combines the nonparametric analysis with a covering number
argument for parametric analysis, which advances the existing theoretical
analyses in the literature of nonparametric density estimation and quantile
bandit problems. Importantly, the nonparametric perspective sheds new
theoretical insights into regression calibration in terms of the curse of
dimensionality and reconciles the existing results on the impossibility of
individual calibration. To our knowledge, we make the first effort to reach
both individual calibration and finite-sample guarantee with minimal
assumptions in terms of conformal prediction. Numerical experiments show the
advantage of such a simple approach under various metrics, and also under
covariates shift. We hope our work provides a simple benchmark and a starting
point of theoretical ground for future research on regression calibration.",http://arxiv.org/pdf/2305.12283v2,cs.LG
2023-05-19 12:44:34+00:00,Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern,"['Meijia Shao', 'Yuan Zhang']","This paper studies the open problem of conformalized entry prediction in a
row/column-exchangeable matrix. The matrix setting presents novel and unique
challenges, but there exists little work on this interesting topic. We
meticulously define the problem, differentiate it from closely related
problems, and rigorously delineate the boundary between achievable and
impossible goals. We then propose two practical algorithms. The first method
provides a fast emulation of the full conformal prediction, while the second
method leverages the technique of algorithmic stability for acceleration. Both
methods are computationally efficient and can effectively safeguard coverage
validity in presence of arbitrary missing pattern. Further, we quantify the
impact of missingness on prediction accuracy and establish fundamental limit
results. Empirical evidence from synthetic and real-world data sets
corroborates the superior performance of our proposed methods.",http://arxiv.org/pdf/2305.11640v2,cs.LG
2023-05-18 22:11:04+00:00,SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction,"['Jiechen Chen', 'Sangwoo Park', 'Osvaldo Simeone']","Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident''. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision's ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).",http://arxiv.org/pdf/2305.11322v3,cs.NE
2023-05-18 01:21:05+00:00,Conformalized matrix completion,"['Yu Gui', 'Rina Foygel Barber', 'Cong Ma']","Matrix completion aims to estimate missing entries in a data matrix, using
the assumption of a low-complexity structure (e.g., low rank) so that
imputation is possible. While many effective estimation algorithms exist in the
literature, uncertainty quantification for this problem has proved to be
challenging, and existing methods are extremely sensitive to model
misspecification. In this work, we propose a distribution-free method for
predictive inference in the matrix completion problem. Our method adapts the
framework of conformal prediction, which provides confidence intervals with
guaranteed distribution-free validity in the setting of regression, to the
problem of matrix completion. Our resulting method, conformalized matrix
completion (cmc), offers provable predictive coverage regardless of the
accuracy of the low-rank model. Empirical results on simulated and real data
demonstrate that cmc is robust to model misspecification while matching the
performance of existing model-based methods when the model is correct.",http://arxiv.org/pdf/2305.10637v3,stat.ME
2023-05-17 14:12:14+00:00,Agent Heterogeneity Mediates Extremism in an Adaptive Social Network Model,"['Seth Bullock', 'Hiroki Sayama']","An existing model of opinion dynamics on an adaptive social network is
extended to introduce update policy heterogeneity, representing the fact that
individual differences between social animals can affect their tendency to
form, and be influenced by, their social bonds with other animals. As in the
original model, the opinions and social connections of a population of model
agents change due to three social processes: conformity, homophily and
neophily. Here, however, we explore the case in which each node's
susceptibility to these three processes is parameterised by node-specific
values drawn independently at random from some distribution. This introduction
of heterogeneity increases both the degree of extremism and connectedness in
the final population (relative to comparable homogeneous networks) and leads to
significant assortativity with respect to node update policy parameters as well
as node opinions. Each node's update policy parameters also predict properties
of the community that they will belong to in the final network configuration.
These results suggest that update policy heterogeneity in social populations
may have a significant impact on the formation of extremist communities in
real-world populations.",http://arxiv.org/pdf/2305.10230v1,cs.SI
2023-05-16 11:17:54+00:00,Blind Image Quality Assessment via Transformer Predicted Error Map and Perceptual Quality Token,"['Jinsong Shi', 'Pan Gao', 'Aljosa Smolic']","Image quality assessment is a fundamental problem in the field of image
processing, and due to the lack of reference images in most practical
scenarios, no-reference image quality assessment (NR-IQA), has gained
increasing attention recently. With the development of deep learning
technology, many deep neural network-based NR-IQA methods have been developed,
which try to learn the image quality based on the understanding of database
information. Currently, Transformer has achieved remarkable progress in various
vision tasks. Since the characteristics of the attention mechanism in
Transformer fit the global perceptual impact of artifacts perceived by a human,
Transformer is thus well suited for image quality assessment tasks. In this
paper, we propose a Transformer based NR-IQA model using a predicted objective
error map and perceptual quality token. Specifically, we firstly generate the
predicted error map by pre-training one model consisting of a Transformer
encoder and decoder, in which the objective difference between the distorted
and the reference images is used as supervision. Then, we freeze the parameters
of the pre-trained model and design another branch using the vision Transformer
to extract the perceptual quality token for feature fusion with the predicted
error map. Finally, the fused features are regressed to the final image quality
score. Extensive experiments have shown that our proposed method outperforms
the current state-of-the-art in both authentic and synthetic image databases.
Moreover, the attentional map extracted by the perceptual quality token also
does conform to the characteristics of the human visual system.",http://arxiv.org/pdf/2305.09353v1,cs.CV
2023-05-06 00:38:29+00:00,SINCERE: Sequential Interaction Networks representation learning on Co-Evolving RiEmannian manifolds,"['Junda Ye', 'Zhongbao Zhang', 'Li Sun', 'Yang Yan', 'Feiyang Wang', 'Fuxin Ren']","Sequential interaction networks (SIN) have been commonly adopted in many
applications such as recommendation systems, search engines and social networks
to describe the mutual influence between users and items/products. Efforts on
representing SIN are mainly focused on capturing the dynamics of networks in
Euclidean space, and recently plenty of work has extended to hyperbolic
geometry for implicit hierarchical learning. Previous approaches which learn
the embedding trajectories of users and items achieve promising results.
However, there are still a range of fundamental issues remaining open. For
example, is it appropriate to place user and item nodes in one identical space
regardless of their inherent discrepancy? Instead of residing in a single fixed
curvature space, how will the representation spaces evolve when new interaction
occurs? To explore these issues for sequential interaction networks, we propose
SINCERE, a novel method representing Sequential Interaction Networks on
Co-Evolving RiEmannian manifolds. SIN- CERE not only takes the user and item
embedding trajectories in respective spaces into account, but also emphasizes
on the space evolvement that how curvature changes over time. Specifically, we
introduce a fresh cross-geometry aggregation which allows us to propagate
information across different Riemannian manifolds without breaking conformal
invariance, and a curvature estimator which is delicately designed to predict
global curvatures effectively according to current local Ricci curvatures.
Extensive experiments on several real-world datasets demonstrate the promising
performance of SINCERE over the state-of-the-art sequential interaction
prediction methods.",http://arxiv.org/pdf/2305.03883v1,cs.LG
2023-05-04 08:11:57+00:00,Conformal Nucleus Sampling,"['Shauli Ravfogel', 'Yoav Goldberg', 'Jacob Goldberger']","Language models generate text based on successively sampling the next word. A
decoding procedure based on nucleus (top-$p$) sampling chooses from the
smallest possible set of words whose cumulative probability exceeds the
probability $p$. In this work, we assess whether a top-$p$ set is indeed
aligned with its probabilistic meaning in various linguistic contexts. We
employ conformal prediction, a calibration procedure that focuses on the
construction of minimal prediction sets according to a desired confidence
level, to calibrate the parameter $p$ as a function of the entropy of the next
word distribution. We find that OPT models are overconfident, and that
calibration shows a moderate inverse scaling with model size.",http://arxiv.org/pdf/2305.02633v1,cs.CL
2023-04-17 10:32:15+00:00,The Standard Problem,['Enrico Coiera'],"Objective: This paper proposes a framework to support the scientific research
of standards so that they can be better measured, evaluated, and designed.
Methods: Beginning with the notion of common models, the framework describes
the general standard problem - the seeming impossibility of creating a
singular, persistent and definitive standard which is not subject to change
over time in an open system. Results: The standard problem arises from
uncertainty driven by variations in operating context, standard quality,
differences in implementation, and drift over time. As a result, fitting work
using conformance services is needed to repair these gaps between a standard
and what is required for real-world use. To guide standards design and repair,
a framework for measuring performance in context is suggested, based on signal
detection theory and technomarkers. Based on the type of common model in
operation, different conformance strategies are identified: (a) Universal
conformance (all agents access the same standard); (b) Mediated conformance (an
interoperability layer supports heterogeneous agents) and (c) Localized
conformance (autonomous adaptive agents manage their own needs). Conformance
methods include incremental design, modular design, adaptors, and creating
interactive and adaptive agents. Discussion: Machine learning should have a
major role in adaptive fitting. Research to guide the choice and design of
conformance services may focus on the stability and homogeneity of shared
tasks, and whether common models are shared ahead of time or adjusted at task
time. Conclusion: This analysis conceptually decouples interoperability and
standardization. While standards facilitate interoperability, interoperability
is achievable without standardization.",http://arxiv.org/pdf/2304.09114v2,cs.OH
2023-04-12 20:56:43+00:00,Post-selection Inference for Conformal Prediction: Trading off Coverage for Precision,"['Siddhaarth Sarkar', 'Arun Kumar Kuchibhotla']","Conformal inference has played a pivotal role in providing uncertainty
quantification for black-box ML prediction algorithms with finite sample
guarantees. Traditionally, conformal prediction inference requires a
data-independent specification of miscoverage level. In practical applications,
one might want to update the miscoverage level after computing the prediction
set. For example, in the context of binary classification, the analyst might
start with a 95$\%$ prediction sets and see that most prediction sets contain
all outcome classes. Prediction sets with both classes being undesirable, the
analyst might desire to consider, say 80$\%$ prediction set. Construction of
prediction sets that guarantee coverage with data-dependent miscoverage level
can be considered as a post-selection inference problem. In this work, we
develop simultaneous conformal inference to account for data-dependent
miscoverage levels. Under the assumption of independent and identically
distributed observations, our proposed methods have a finite sample
simultaneous guarantee over all miscoverage levels. This allows practitioners
to trade freely coverage probability for the quality of the prediction set by
any criterion of their choice (say size of prediction set) while maintaining
the finite sample guarantees similar to traditional conformal inference.",http://arxiv.org/pdf/2304.06158v3,stat.ME
2023-04-12 20:21:31+00:00,A Quasi-Conforming Embedded Reproducing Kernel Particle Method for Heterogeneous Materials,"['Ryan T. Schlinkman', 'Jonghyuk Baek', 'Frank N. Beckwith', 'Stacy M. Nelson', 'J. S. Chen']","We present a quasi-conforming embedded reproducing kernel particle method
(QCE-RKPM) for modeling heterogeneous materials that makes use of techniques
not available to mesh-based methods such as the finite element method (FEM) and
avoids many of the drawbacks in current embedded and immersed formulations
which are based on meshed methods. The different material domains are
discretized independently thus avoiding time-consuming, conformal meshing. In
this approach, the superposition of foreground (inclusion) and background
(matrix) domain integration smoothing cells are corrected by a quasi-conforming
quadtree subdivision on the background integration smoothing cells. Due to the
non-conforming nature of the background integration smoothing cells near the
material interfaces, a variationally consistent (VC) correction for domain
integration is introduced to restore integration constraints and thus optimal
convergence rates at a minor computational cost. Additional interface
integration smoothing cells with area (volume) correction, while
non-conforming, can be easily introduced to further enhance the accuracy and
stability of the Galerkin solution using VC integration on non-conforming
cells. To properly approximate the weak discontinuity across the material
interface by a penalty-free Nitsche's method with enhanced coercivity, the
interface nodes on the surface of the foreground discretization are also shared
with the background discretization. As such, there are no tunable parameters,
such as those involved in the penalty type method, to enforce interface
compatibility in this approach. The advantage of this meshfree formulation is
that it avoids many of the instabilities in mesh-based immersed and embedded
methods. The effectiveness of QCE-RKPM is illustrated with several examples.",http://arxiv.org/pdf/2304.06150v1,cs.CE
2023-04-12 08:10:13+00:00,Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling,"['Léo Andéol', 'Thomas Fel', 'Florence De Grancey', 'Luca Mossina']","Deploying deep learning models in real-world certified systems requires the
ability to provide confidence estimates that accurately reflect their
uncertainty. In this paper, we demonstrate the use of the conformal prediction
framework to construct reliable and trustworthy predictors for detecting
railway signals. Our approach is based on a novel dataset that includes images
taken from the perspective of a train operator and state-of-the-art object
detectors. We test several conformal approaches and introduce a new method
based on conformal risk control. Our findings demonstrate the potential of the
conformal prediction framework to evaluate model performance and provide
practical guidance for achieving formally guaranteed uncertainty bounds.",http://arxiv.org/pdf/2304.06052v2,cs.LG
2023-04-11 12:46:55+00:00,Individualized Conformal,"['Fernando Delbianco', 'Fernando Tohmé']","The problem of individualized prediction can be addressed using variants of
conformal prediction, obtaining the intervals to which the actual values of the
variables of interest belong. Here we present a method based on detecting the
observations that may be relevant for a given question and then using simulated
controls to yield the intervals for the predicted values. This method is shown
to be adaptive and able to detect the presence of latent relevant variables.",http://arxiv.org/pdf/2304.05189v1,stat.ME
2023-04-06 19:56:47+00:00,Conformal Regression in Calorie Prediction for Team Jumbo-Visma,"['Kristian van Kuijk', 'Mark Dirksen', 'Christof Seiler']","UCI WorldTour races, the premier men's elite road cycling tour, are grueling
events that put physical fitness and endurance of riders to the test. The
coaches of Team Jumbo-Visma have long been responsible for predicting the
energy needs of each rider of the Dutch team for every race on the calendar.
Those must be estimated to ensure riders have the energy and resources
necessary to maintain a high level of performance throughout a race. This task,
however, is both time-consuming and challenging, as it requires precise
estimates of race speed and power output. Traditionally, the approach to
predicting energy needs has relied on judgement and experience of coaches, but
this method has its limitations and often leads to inaccurate predictions. In
this paper, we propose a new, more effective approach to predicting energy
needs for cycling races. By predicting the speed and power with regression
models, we provide the coaches with calorie needs estimates for each individual
rider per stage instantly. In addition, we compare methods to quantify
uncertainty using conformal prediction. The empirical analysis of the
jackknife+, jackknife-minmax, jackknife-minmax-after-bootstrap, CV+, CV-minmax,
conformalized quantile regression, and inductive conformal prediction methods
in conformal prediction reveals that all methods achieve valid prediction
intervals. All but minmax-based methods also produce sufficiently narrow
prediction intervals for decision-making. Furthermore, methods computing
prediction intervals of fixed size produce tighter intervals for low
significance values. Among the methods computing intervals of varying length
across the input space, inductive conformal prediction computes narrower
prediction intervals at larger significance level.",http://arxiv.org/pdf/2304.03778v3,cs.LG
2023-04-05 16:45:11+00:00,Conformal Off-Policy Evaluation in Markov Decision Processes,"['Daniele Foffano', 'Alessio Russo', 'Alexandre Proutiere']","Reinforcement Learning aims at identifying and evaluating efficient control
policies from data. In many real-world applications, the learner is not allowed
to experiment and cannot gather data in an online manner (this is the case when
experimenting is expensive, risky or unethical). For such applications, the
reward of a given policy (the target policy) must be estimated using historical
data gathered under a different policy (the behavior policy). Most methods for
this learning task, referred to as Off-Policy Evaluation (OPE), do not come
with accuracy and certainty guarantees. We present a novel OPE method based on
Conformal Prediction that outputs an interval containing the true reward of the
target policy with a prescribed level of certainty. The main challenge in OPE
stems from the distribution shift due to the discrepancies between the target
and the behavior policies. We propose and empirically evaluate different ways
to deal with this shift. Some of these methods yield conformalized intervals
with reduced length compared to existing approaches, while maintaining the same
certainty level.",http://arxiv.org/pdf/2304.02574v2,cs.LG
2023-04-04 00:20:26+00:00,Conformalized Unconditional Quantile Regression,"['Ahmed M. Alaa', 'Zeshan Hussain', 'David Sontag']","We develop a predictive inference procedure that combines conformal
prediction (CP) with unconditional quantile regression (QR) -- a commonly used
tool in econometrics that involves regressing the recentered influence function
(RIF) of the quantile functional over input covariates. Unlike the more
widely-known conditional QR, unconditional QR explicitly captures the impact of
changes in covariate distribution on the quantiles of the marginal distribution
of outcomes. Leveraging this property, our procedure issues adaptive predictive
intervals with localized frequentist coverage guarantees. It operates by
fitting a machine learning model for the RIFs using training data, and then
applying the CP procedure for any test covariate with respect to a
``hypothetical'' covariate distribution localized around the new instance.
Experiments show that our procedure is adaptive to heteroscedasticity, provides
transparent coverage guarantees that are relevant to the test instance at hand,
and performs competitively with existing methods in terms of efficiency.",http://arxiv.org/pdf/2304.01426v1,cs.LG
2023-04-03 14:44:52+00:00,Integrated Decision-Making and Control for Urban Autonomous Driving with Traffic Rules Compliance,"['Haichao Liu', 'Kai Chen', 'Yulin Li', 'Zhenmin Huang', 'Jianghua Duan', 'Jun Ma']","In urban driving scenarios, autonomous vehicles are expected to conform to
traffic rules covering traffic lights, traversable and non-traversable traffic
lines, etc. In this article, we propose an optimization-based integrated
decision-making and control scheme for urban autonomous driving. Inherently, to
ensure the compliance with traffic rules, an innovative design of potential
functions (PFs) is presented to characterize various traffic rules that are
commonly encountered in urban driving scenarios, and these PFs are further
incorporated as part of the model predictive control (MPC) formulation. In this
sense, it circumvents the necessity of typical hand-crafted rule design, and
high-level decision-making is attained implicitly along with control as an
integrated architecture, facilitating flexible maneuvers with safety
guarantees. As demonstrated from a series of simulations in CARLA, it is
noteworthy that the proposed framework admits real-time performance and high
generalizability.",http://arxiv.org/pdf/2304.01041v1,cs.RO
2023-03-25 03:32:01+00:00,Collaborative Multi-Object Tracking with Conformal Uncertainty Propagation,"['Sanbao Su', 'Songyang Han', 'Yiming Li', 'Zhili Zhang', 'Chen Feng', 'Caiwen Ding', 'Fei Miao']","Object detection and multiple object tracking (MOT) are essential components
of self-driving systems. Accurate detection and uncertainty quantification are
both critical for onboard modules, such as perception, prediction, and
planning, to improve the safety and robustness of autonomous vehicles.
Collaborative object detection (COD) has been proposed to improve detection
accuracy and reduce uncertainty by leveraging the viewpoints of multiple
agents. However, little attention has been paid on how to leverage the
uncertainty quantification from COD to enhance MOT performance. In this paper,
as the first attempt, we design the uncertainty propagation framework to
address this challenge, called MOT-CUP. Our framework first quantifies the
uncertainty of COD through direct modeling and conformal prediction, and
propogates this uncertainty information during the motion prediction and
association steps. MOT-CUP is designed to work with different collaborative
object detectors and baseline MOT algorithms. We evaluate MOT-CUP on V2X-Sim, a
comprehensive collaborative perception dataset, and demonstrate a 2%
improvement in accuracy and a 2.67X reduction in uncertainty compared to the
baselines, e.g., SORT and ByteTrack. MOT-CUP demonstrates the importance of
uncertainty quantification in both COD and MOT, and provides the first attempt
to improve the accuracy and reduce the uncertainty in MOT based on COD through
uncertainty propogation.",http://arxiv.org/pdf/2303.14346v1,cs.CV
2023-03-22 17:52:54+00:00,Conformal Prediction for Time Series with Modern Hopfield Networks,"['Andreas Auer', 'Martin Gauch', 'Daniel Klotz', 'Sepp Hochreiter']","To quantify uncertainty, conformal prediction methods are gaining
continuously more interest and have already been successfully applied to
various domains. However, they are difficult to apply to time series as the
autocorrelative structure of time series violates basic assumptions required by
conformal prediction. We propose HopCPT, a novel conformal prediction approach
for time series that not only copes with temporal structures but leverages
them. We show that our approach is theoretically well justified for time series
where temporal dependencies are present. In experiments, we demonstrate that
our new approach outperforms state-of-the-art conformal prediction methods on
multiple real-world time series datasets from four different domains.",http://arxiv.org/pdf/2303.12783v2,cs.LG
2023-03-22 16:42:19+00:00,Adaptive Conformal Prediction by Reweighting Nonconformity Score,"['Salim I. Amoukou', 'Nicolas J. B Brunel']","Despite attractive theoretical guarantees and practical successes, Predictive
Interval (PI) given by Conformal Prediction (CP) may not reflect the
uncertainty of a given model. This limitation arises from CP methods using a
constant correction for all test points, disregarding their individual
uncertainties, to ensure coverage properties. To address this issue, we propose
using a Quantile Regression Forest (QRF) to learn the distribution of
nonconformity scores and utilizing the QRF's weights to assign more importance
to samples with residuals similar to the test point. This approach results in
PI lengths that are more aligned with the model's uncertainty. In addition, the
weights learnt by the QRF provide a partition of the features space, allowing
for more efficient computations and improved adaptiveness of the PI through
groupwise conformalization. Our approach enjoys an assumption-free finite
sample marginal and training-conditional coverage, and under suitable
assumptions, it also ensures conditional coverage. Our methods work for any
nonconformity score and are available as a Python package. We conduct
experiments on simulated and real-world data that demonstrate significant
improvements compared to existing methods.",http://arxiv.org/pdf/2303.12695v2,stat.ML
2023-03-22 00:55:53+00:00,Object Pose Estimation with Statistical Guarantees: Conformal Keypoint Detection and Geometric Uncertainty Propagation,"['Heng Yang', 'Marco Pavone']","The two-stage object pose estimation paradigm first detects semantic
keypoints on the image and then estimates the 6D pose by minimizing
reprojection errors. Despite performing well on standard benchmarks, existing
techniques offer no provable guarantees on the quality and uncertainty of the
estimation. In this paper, we inject two fundamental changes, namely conformal
keypoint detection and geometric uncertainty propagation, into the two-stage
paradigm and propose the first pose estimator that endows an estimation with
provable and computable worst-case error bounds. On one hand, conformal
keypoint detection applies the statistical machinery of inductive conformal
prediction to convert heuristic keypoint detections into circular or elliptical
prediction sets that cover the groundtruth keypoints with a user-specified
marginal probability (e.g., 90%). Geometric uncertainty propagation, on the
other, propagates the geometric constraints on the keypoints to the 6D object
pose, leading to a Pose UnceRtainty SEt (PURSE) that guarantees coverage of the
groundtruth pose with the same probability. The PURSE, however, is a nonconvex
set that does not directly lead to estimated poses and uncertainties.
Therefore, we develop RANdom SAmple averaGing (RANSAG) to compute an average
pose and apply semidefinite relaxation to upper bound the worst-case errors
between the average pose and the groundtruth. On the LineMOD Occlusion dataset
we demonstrate: (i) the PURSE covers the groundtruth with valid probabilities;
(ii) the worst-case error bounds provide correct uncertainty quantification;
and (iii) the average pose achieves better or similar accuracy as
representative methods based on sparse keypoints.",http://arxiv.org/pdf/2303.12246v1,cs.CV
2023-03-19 15:56:50+00:00,Improving Uncertainty Quantification of Deep Classifiers via Neighborhood Conformal Prediction: Novel Algorithm and Theoretical Analysis,"['Subhankar Ghosh', 'Taha Belkhouja', 'Yan Yan', 'Janardhan Rao Doppa']","Safe deployment of deep neural networks in high-stake real-world applications
requires theoretically sound uncertainty quantification. Conformal prediction
(CP) is a principled framework for uncertainty quantification of deep models in
the form of prediction set for classification tasks with a user-specified
coverage (i.e., true class label is contained with high probability). This
paper proposes a novel algorithm referred to as Neighborhood Conformal
Prediction (NCP) to improve the efficiency of uncertainty quantification from
CP for deep classifiers (i.e., reduce prediction set size). The key idea behind
NCP is to use the learned representation of the neural network to identify k
nearest-neighbors calibration examples for a given testing input and assign
them importance weights proportional to their distance to create adaptive
prediction sets. We theoretically show that if the learned data representation
of the neural network satisfies some mild conditions, NCP will produce smaller
prediction sets than traditional CP algorithms. Our comprehensive experiments
on CIFAR-10, CIFAR-100, and ImageNet datasets using diverse deep neural
networks strongly demonstrate that NCP leads to significant reduction in
prediction set size over prior CP methods.",http://arxiv.org/pdf/2303.10694v1,cs.LG
2023-03-14 12:04:16+00:00,Prioritized Planning for Target-Oriented Manipulation via Hierarchical Stacking Relationship Prediction,"['Zewen Wu', 'Jian Tang', 'Xingyu Chen', 'Chengzhong Ma', 'Xuguang Lan', 'Nanning Zheng']","In scenarios involving the grasping of multiple targets, the learning of
stacking relationships between objects is fundamental for robots to execute
safely and efficiently. However, current methods lack subdivision for the
hierarchy of stacking relationship types. In scenes where objects are mostly
stacked in an orderly manner, they are incapable of performing human-like and
high-efficient grasping decisions. This paper proposes a perception-planning
method to distinguish different stacking types between objects and generate
prioritized manipulation order decisions based on given target designations. We
utilize a Hierarchical Stacking Relationship Network (HSRN) to discriminate the
hierarchy of stacking and generate a refined Stacking Relationship Tree (SRT)
for relationship description. Considering that objects with high stacking
stability can be grasped together if necessary, we introduce an elaborate
decision-making planner based on the Partially Observable Markov Decision
Process (POMDP), which leverages observations and generates the least
grasp-consuming decision chain with robustness and is suitable for
simultaneously specifying multiple targets. To verify our work, we set the
scene to the dining table and augment the REGRAD dataset with a set of common
tableware models for network training. Experiments show that our method
effectively generates grasping decisions that conform to human requirements,
and improves the implementation efficiency compared with existing methods on
the basis of guaranteeing the success rate.",http://arxiv.org/pdf/2303.07828v2,cs.RO
2023-03-09 13:32:18+00:00,Probabilistic 3d regression with projected huber distribution,"['David Mohlin', 'Josephine Sullivan']","Estimating probability distributions which describe where an object is likely
to be from camera data is a task with many applications. In this work we
describe properties which we argue such methods should conform to. We also
design a method which conform to these properties. In our experiments we show
that our method produces uncertainties which correlate well with empirical
errors. We also show that the mode of the predicted distribution outperform our
regression baselines. The code for our implementation is available online.",http://arxiv.org/pdf/2303.05245v1,cs.CV
2023-03-08 05:05:01+00:00,HappyMap: A Generalized Multi-calibration Method,"['Zhun Deng', 'Cynthia Dwork', 'Linjun Zhang']","Multi-calibration is a powerful and evolving concept originating in the field
of algorithmic fairness. For a predictor $f$ that estimates the outcome $y$
given covariates $x$, and for a function class $\mathcal{C}$, multi-calibration
requires that the predictor $f(x)$ and outcome $y$ are indistinguishable under
the class of auditors in $\mathcal{C}$. Fairness is captured by incorporating
demographic subgroups into the class of functions~$\mathcal{C}$. Recent work
has shown that, by enriching the class $\mathcal{C}$ to incorporate appropriate
propensity re-weighting functions, multi-calibration also yields
target-independent learning, wherein a model trained on a source domain
performs well on unseen, future, target domains(approximately) captured by the
re-weightings.
  Formally, multi-calibration with respect to $\mathcal{C}$ bounds
$\big|\mathbb{E}_{(x,y)\sim \mathcal{D}}[c(f(x),x)\cdot(f(x)-y)]\big|$ for all
$c \in \mathcal{C}$. In this work, we view the term $(f(x)-y)$ as just one
specific mapping, and explore the power of an enriched class of mappings. We
propose \textit{HappyMap}, a generalization of multi-calibration, which yields
a wide range of new applications, including a new fairness notion for
uncertainty quantification (conformal prediction), a novel technique for
conformal prediction under covariate shift, and a different approach to
analyzing missing data, while also yielding a unified understanding of several
existing seemingly disparate algorithmic fairness notions and
target-independent learning approaches.
  We give a single \textit{HappyMap} meta-algorithm that captures all these
results, together with a sufficiency condition for its success.",http://arxiv.org/pdf/2303.04379v1,cs.LG
2023-03-07 15:51:03+00:00,Group conditional validity via multi-group learning,"['Samuel Deng', 'Navid Ardeshir', 'Daniel Hsu']","We consider the problem of distribution-free conformal prediction and the
criterion of group conditional validity. This criterion is motivated by many
practical scenarios including hidden stratification and group fairness.
Existing methods achieve such guarantees under either restrictive grouping
structure or distributional assumptions, or they are overly-conservative under
heteroskedastic noise. We propose a simple reduction to the problem of
achieving validity guarantees for individual populations by leveraging
algorithms for a problem called multi-group learning. This allows us to port
theoretical guarantees from multi-group learning to obtain obtain sample
complexity guarantees for conformal prediction. We also provide a new algorithm
for multi-group learning for groups with hierarchical structure. Using this
algorithm in our reduction leads to improved sample complexity guarantees with
a simpler predictor structure.",http://arxiv.org/pdf/2303.03995v2,cs.LG
2023-03-07 00:46:04+00:00,Learning When to Treat Business Processes: Prescriptive Process Monitoring with Causal Inference and Reinforcement Learning,"['Zahra Dasht Bozorgi', 'Marlon Dumas', 'Marcello La Rosa', 'Artem Polyvyanyy', 'Mahmoud Shoush', 'Irene Teinemaa']","Increasing the success rate of a process, i.e. the percentage of cases that
end in a positive outcome, is a recurrent process improvement goal. At runtime,
there are often certain actions (a.k.a. treatments) that workers may execute to
lift the probability that a case ends in a positive outcome. For example, in a
loan origination process, a possible treatment is to issue multiple loan offers
to increase the probability that the customer takes a loan. Each treatment has
a cost. Thus, when defining policies for prescribing treatments to cases,
managers need to consider the net gain of the treatments. Also, the effect of a
treatment varies over time: treating a case earlier may be more effective than
later in a case. This paper presents a prescriptive monitoring method that
automates this decision-making task. The method combines causal inference and
reinforcement learning to learn treatment policies that maximize the net gain.
The method leverages a conformal prediction technique to speed up the
convergence of the reinforcement learning mechanism by separating cases that
are likely to end up in a positive or negative outcome, from uncertain cases.
An evaluation on two real-life datasets shows that the proposed method
outperforms a state-of-the-art baseline.",http://arxiv.org/pdf/2303.03572v1,cs.LG
2023-03-03 21:15:22+00:00,Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials,"['Yuyang Wang', 'Changwen Xu', 'Zijie Li', 'Amir Barati Farimani']","Recent advances in equivariant graph neural networks (GNNs) have made deep
learning amenable to developing fast surrogate models to expensive ab initio
quantum mechanics (QM) approaches for molecular potential predictions. However,
building accurate and transferable potential models using GNNs remains
challenging, as the data is greatly limited by the expensive computational
costs and level of theory of QM methods, especially for large and complex
molecular systems. In this work, we propose denoise pretraining on
nonequilibrium molecular conformations to achieve more accurate and
transferable GNN potential predictions. Specifically, atomic coordinates of
sampled nonequilibrium conformations are perturbed by random noises and GNNs
are pretrained to denoise the perturbed molecular conformations which recovers
the original coordinates. Rigorous experiments on multiple benchmarks reveal
that pretraining significantly improves the accuracy of neural potentials.
Furthermore, we show that the proposed pretraining approach is model-agnostic,
as it improves the performance of different invariant and equivariant GNNs.
Notably, our models pretrained on small molecules demonstrate remarkable
transferability, improving performance when fine-tuned on diverse molecular
systems, including different elements, charged molecules, biomolecules, and
larger systems. These results highlight the potential for leveraging denoise
pretraining approaches to build more generalizable neural potentials for
complex molecular systems.",http://arxiv.org/pdf/2303.02216v2,cs.LG
2023-03-03 20:37:55+00:00,"Lightweight, Uncertainty-Aware Conformalized Visual Odometry","['Alex C. Stutts', 'Danilo Erricolo', 'Theja Tulabandhula', 'Amit Ranjan Trivedi']","Data-driven visual odometry (VO) is a critical subroutine for autonomous edge
robotics, and recent progress in the field has produced highly accurate point
predictions in complex environments. However, emerging autonomous edge robotics
devices like insect-scale drones and surgical robots lack a computationally
efficient framework to estimate VO's predictive uncertainties. Meanwhile, as
edge robotics continue to proliferate into mission-critical application spaces,
awareness of model's the predictive uncertainties has become crucial for
risk-aware decision-making. This paper addresses this challenge by presenting a
novel, lightweight, and statistically robust framework that leverages conformal
inference (CI) to extract VO's uncertainty bands. Our approach represents the
uncertainties using flexible, adaptable, and adjustable prediction intervals
that, on average, guarantee the inclusion of the ground truth across all
degrees of freedom (DOF) of pose estimation. We discuss the architectures of
generative deep neural networks for estimating multivariate uncertainty bands
along with point (mean) prediction. We also present techniques to improve the
uncertainty estimation accuracy, such as leveraging Monte Carlo dropout
(MC-dropout) for data augmentation. Finally, we propose a novel training loss
function that combines interval scoring and calibration loss with traditional
training metrics--mean-squared error and KL-divergence--to improve
uncertainty-aware learning. Our simulation results demonstrate that the
presented framework consistently captures true uncertainty in pose estimations
across different datasets, estimation models, and applied noise types,
indicating its wide applicability.",http://arxiv.org/pdf/2303.02207v1,cs.CV
2023-03-02 17:15:31+00:00,Design-based conformal prediction,['Jerzy Wieczorek'],"Conformal prediction is an assumption-lean approach to generating
distribution-free prediction intervals or sets, for nearly arbitrary predictive
models, with guaranteed finite-sample coverage. Conformal methods are an active
research topic in statistics and machine learning, but only recently have they
been extended to non-exchangeable data. In this paper, we invite survey
methodologists to begin using and contributing to conformal methods. We
introduce how conformal prediction can be applied to data from several common
complex sample survey designs, under a framework of design-based inference for
a finite population, and we point out gaps where survey methodologists could
fruitfully apply their expertise. Our simulations empirically bear out the
theoretical guarantees of finite-sample coverage, and our real-data example
demonstrates how conformal prediction can be applied to complex sample survey
data in practice.",http://arxiv.org/pdf/2303.01422v2,stat.ME
2023-03-01 03:12:27+00:00,Joint Coverage Regions: Simultaneous Confidence and Prediction Sets,"['Edgar Dobriban', 'Zhanran Lin']","We introduce Joint Coverage Regions (JCRs), which unify confidence intervals
and prediction regions in frequentist statistics. Specifically, joint coverage
regions aim to cover a pair formed by an unknown fixed parameter (such as the
mean of a distribution), and an unobserved random datapoint (such as the
outcomes associated to a new test datapoint). The first corresponds to a
confidence component, while the second corresponds to a prediction part. In
particular, our notion unifies classical statistical methods such as the Wald
confidence interval with distribution-free prediction methods such as conformal
prediction. We show how to construct finite-sample valid JCRs when a
conditional pivot is available; under the same conditions where exact
finite-sample confidence and prediction sets are known to exist. We further
develop efficient JCR algorithms, including split-data versions by introducing
adequate sets to reduce the cost of repeated computation. We illustrate the use
of JCRs in statistical problems such as constructing efficient prediction sets
when the parameter space is structured.",http://arxiv.org/pdf/2303.00203v2,stat.ME
2023-02-24 18:59:51+00:00,Improving Massively Multilingual ASR With Auxiliary CTC Objectives,"['William Chen', 'Brian Yan', 'Jiatong Shi', 'Yifan Peng', 'Soumi Maiti', 'Shinji Watanabe']","Multilingual Automatic Speech Recognition (ASR) models have extended the
usability of speech technologies to a wide variety of languages. With how many
languages these models have to handle, however, a key to understanding their
imbalanced performance across different languages is to examine if the model
actually knows which language it should transcribe. In this paper, we introduce
our work on improving performance on FLEURS, a 102-language open ASR benchmark,
by conditioning the entire model on language identity (LID). We investigate
techniques inspired from recent Connectionist Temporal Classification (CTC)
studies to help the model handle the large number of languages, conditioning on
the LID predictions of auxiliary tasks. Our experimental results demonstrate
the effectiveness of our technique over standard CTC/Attention-based hybrid
models. Furthermore, our state-of-the-art systems using self-supervised models
with the Conformer architecture improve over the results of prior work on
FLEURS by a relative 28.4% CER. Trained models and reproducible recipes are
available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1 .",http://arxiv.org/pdf/2302.12829v2,cs.CL
2023-02-23 18:57:14+00:00,Improving Adaptive Conformal Prediction Using Self-Supervised Learning,"['Nabeel Seedat', 'Alan Jeffares', 'Fergus Imrie', 'Mihaela van der Schaar']","Conformal prediction is a powerful distribution-free tool for uncertainty
quantification, establishing valid prediction intervals with finite-sample
guarantees. To produce valid intervals which are also adaptive to the
difficulty of each instance, a common approach is to compute normalized
nonconformity scores on a separate calibration set. Self-supervised learning
has been effectively utilized in many domains to learn general representations
for downstream predictors. However, the use of self-supervision beyond model
pretraining and representation learning has been largely unexplored. In this
work, we investigate how self-supervised pretext tasks can improve the quality
of the conformal regressors, specifically by improving the adaptability of
conformal intervals. We train an auxiliary model with a self-supervised pretext
task on top of an existing predictive model and use the self-supervised error
as an additional feature to estimate nonconformity scores. We empirically
demonstrate the benefit of the additional information using both synthetic and
real data on the efficiency (width), deficit, and excess of conformal
prediction intervals.",http://arxiv.org/pdf/2302.12238v1,cs.LG
2023-02-20 16:58:16+00:00,Conformal Prediction for Network-Assisted Regression,"['Robert Lunde', 'Elizaveta Levina', 'Ji Zhu']","An important problem in network analysis is predicting a node attribute using
both network covariates, such as graph embedding coordinates or local subgraph
counts, and conventional node covariates, such as demographic characteristics.
While standard regression methods that make use of both types of covariates may
be used for prediction, statistical inference is complicated by the fact that
the nodal summary statistics are often dependent in complex ways. We show that
under a mild joint exchangeability assumption, a network analog of conformal
prediction achieves finite sample validity for a wide range of network
covariates. We also show that a form of asymptotic conditional validity is
achievable. The methods are illustrated on both simulated networks and a
citation network dataset.",http://arxiv.org/pdf/2302.10095v3,stat.ME
2023-02-17 18:27:14+00:00,MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation,"['Clement Vignac', 'Nagham Osman', 'Laura Toni', 'Pascal Frossard']","This work introduces MiDi, a novel diffusion model for jointly generating
molecular graphs and their corresponding 3D arrangement of atoms. Unlike
existing methods that rely on predefined rules to determine molecular bonds
based on the 3D conformation, MiDi offers an end-to-end differentiable approach
that streamlines the molecule generation process. Our experimental results
demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS
dataset, MiDi generates 92% of stable molecules, against 6% for the previous
EDM model that uses interatomic distances for bond prediction, and 40% using
EDM followed by an algorithm that directly optimize bond orders for validity.
Our code is available at github.com/cvignac/MiDi.",http://arxiv.org/pdf/2302.09048v2,cs.LG
2023-02-15 18:59:30+00:00,Improved Online Conformal Prediction via Strongly Adaptive Online Learning,"['Aadyot Bhatnagar', 'Huan Wang', 'Caiming Xiong', 'Yu Bai']","We study the problem of uncertainty quantification via prediction sets, in an
online setting where the data distribution may vary arbitrarily over time.
Recent work develops online conformal prediction techniques that leverage
regret minimization algorithms from the online learning literature to learn
prediction sets with approximately valid coverage and small regret. However,
standard regret minimization could be insufficient for handling changing
environments, where performance guarantees may be desired not only over the
full time horizon but also in all (sub-)intervals of time. We develop new
online conformal prediction methods that minimize the strongly adaptive regret,
which measures the worst-case regret over all intervals of a fixed length. We
prove that our methods achieve near-optimal strongly adaptive regret for all
interval lengths simultaneously, and approximately valid coverage. Experiments
show that our methods consistently obtain better coverage and smaller
prediction sets than existing methods on real-world tasks, such as time series
forecasting and image classification under distribution shift.",http://arxiv.org/pdf/2302.07869v1,cs.LG
2023-02-15 15:56:23+00:00,SupSiam: Non-contrastive Auxiliary Loss for Learning from Molecular Conformers,"['Michael Maser', 'Ji Won Park', 'Joshua Yao-Yu Lin', 'Jae Hyeon Lee', 'Nathan C. Frey', 'Andrew Watkins']","We investigate Siamese networks for learning related embeddings for augmented
samples of molecular conformers. We find that a non-contrastive (positive-pair
only) auxiliary task aids in supervised training of Euclidean neural networks
(E3NNs) and increases manifold smoothness (MS) around point-cloud geometries.
We demonstrate this property for multiple drug-activity prediction tasks while
maintaining relevant performance metrics, and propose an extension of MS to
probabilistic and regression settings. We provide an analysis of representation
collapse, finding substantial effects of task-weighting, latent dimension, and
regularization. We expect the presented protocol to aid in the development of
reliable E3NNs from molecular conformers, even for small-data drug discovery
programs.",http://arxiv.org/pdf/2302.07754v1,cs.LG
2023-02-14 19:21:44+00:00,Derandomized Novelty Detection with FDR Control via Conformal E-values,"['Meshi Bashari', 'Amir Epstein', 'Yaniv Romano', 'Matteo Sesia']","Conformal inference provides a general distribution-free method to rigorously
calibrate the output of any machine learning algorithm for novelty detection.
While this approach has many strengths, it has the limitation of being
randomized, in the sense that it may lead to different results when analyzing
twice the same data, and this can hinder the interpretation of any findings. We
propose to make conformal inferences more stable by leveraging suitable
conformal e-values instead of p-values to quantify statistical significance.
This solution allows the evidence gathered from multiple analyses of the same
data to be aggregated effectively while provably controlling the false
discovery rate. Further, we show that the proposed method can reduce randomness
without much loss of power compared to standard conformal inference, partly
thanks to an innovative way of weighting conformal e-values based on additional
side information carefully extracted from the same data. Simulations with
synthetic and real data confirm this solution can be effective at eliminating
random noise in the inferences obtained with state-of-the-art alternative
techniques, sometimes also leading to higher power.",http://arxiv.org/pdf/2302.07294v3,cs.LG
2023-02-13 12:46:39+00:00,One-Shot Federated Conformal Prediction,"['Pierre Humbert', 'Batiste Le Bars', 'Aurélien Bellet', 'Sylvain Arlot']","In this paper, we introduce a conformal prediction method to construct
prediction sets in a oneshot federated learning setting. More specifically, we
define a quantile-of-quantiles estimator and prove that for any distribution,
it is possible to output prediction sets with desired coverage in only one
round of communication. To mitigate privacy issues, we also describe a locally
differentially private version of our estimator. Finally, over a wide range of
experiments, we show that our method returns prediction sets with coverage and
length very similar to those obtained in a centralized setting. Overall, these
results demonstrate that our method is particularly well-suited to perform
conformal predictions in a one-shot federated learning setting.",http://arxiv.org/pdf/2302.06322v2,stat.ML
2023-02-10 15:05:09+00:00,From Group-Differences to Single-Subject Probability: Conformal Prediction-based Uncertainty Estimation for Brain-Age Modeling,"['Jan Ernsting', 'Nils R. Winter', 'Ramona Leenings', 'Kelvin Sarink', 'Carlotta B. C. Barkhau', 'Lukas Fisch', 'Daniel Emden', 'Vincent Holstein', 'Jonathan Repple', 'Dominik Grotegerd', 'Susanne Meinert', 'NAKO Investigators', 'Klaus Berger', 'Benjamin Risse', 'Udo Dannlowski', 'Tim Hahn']","The brain-age gap is one of the most investigated risk markers for brain
changes across disorders. While the field is progressing towards large-scale
models, recently incorporating uncertainty estimates, no model to date provides
the single-subject risk assessment capability essential for clinical
application. In order to enable the clinical use of brain-age as a biomarker,
we here combine uncertainty-aware deep Neural Networks with conformal
prediction theory. This approach provides statistical guarantees with respect
to single-subject uncertainty estimates and allows for the calculation of an
individual's probability for accelerated brain-aging. Building on this, we show
empirically in a sample of N=16,794 participants that 1. a lower or comparable
error as state-of-the-art, large-scale brain-age models, 2. the statistical
guarantees regarding single-subject uncertainty estimation indeed hold for
every participant, and 3. that the higher individual probabilities of
accelerated brain-aging derived from our model are associated with Alzheimer's
Disease, Bipolar Disorder and Major Depressive Disorder.",http://arxiv.org/pdf/2302.05304v1,cs.LG
2023-02-10 12:27:48+00:00,CGA-PoseNet: Camera Pose Regression via a 1D-Up Approach to Conformal Geometric Algebra,"['Alberto Pepe', 'Joan Lasenby']","We introduce CGA-PoseNet, which uses the 1D-Up approach to Conformal
Geometric Algebra (CGA) to represent rotations and translations with a single
mathematical object, the motor, for camera pose regression. We do so starting
from PoseNet, which successfully predicts camera poses from small datasets of
RGB frames. State-of-the-art methods, however, require expensive tuning to
balance the orientational and translational components of the camera pose.This
is usually done through complex, ad-hoc loss function to be minimized, and in
some cases also requires 3D points as well as images. Our approach has the
advantage of unifying the camera position and orientation through the motor.
Consequently, the network searches for a single object which lives in a
well-behaved 4D space with a Euclidean signature. This means that we can
address the case of image-only datasets and work efficiently with a simple loss
function, namely the mean squared error (MSE) between the predicted and ground
truth motors. We show that it is possible to achieve high accuracy camera pose
regression with a significantly simpler problem formulation. This 1D-Up
approach to CGA can be employed to overcome the dichotomy between translational
and orientational components in camera pose regression in a compact and elegant
way.",http://arxiv.org/pdf/2302.05211v1,cs.CV
2023-02-08 12:29:38+00:00,Fortuna: A Library for Uncertainty Quantification in Deep Learning,"['Gianluca Detommaso', 'Alberto Gasparin', 'Michele Donini', 'Matthias Seeger', 'Andrew Gordon Wilson', 'Cedric Archambeau']","We present Fortuna, an open-source library for uncertainty quantification in
deep learning. Fortuna supports a range of calibration techniques, such as
conformal prediction that can be applied to any trained neural network to
generate reliable uncertainty estimates, and scalable Bayesian inference
methods that can be applied to Flax-based deep neural networks trained from
scratch for improved uncertainty quantification and accuracy. By providing a
coherent framework for advanced uncertainty quantification methods, Fortuna
simplifies the process of benchmarking and helps practitioners build robust AI
systems.",http://arxiv.org/pdf/2302.04019v1,cs.LG
2023-02-07 23:01:16+00:00,How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control,"['Jacopo Teneggi', 'Matthew Tivnan', 'J. Webster Stayman', 'Jeremias Sulam']","Score-based generative modeling, informally referred to as diffusion models,
continue to grow in popularity across several important domains and tasks.
While they provide high-quality and diverse samples from empirical
distributions, important questions remain on the reliability and
trustworthiness of these sampling procedures for their responsible use in
critical scenarios. Conformal prediction is a modern tool to construct
finite-sample, distribution-free uncertainty guarantees for any black-box
predictor. In this work, we focus on image-to-image regression tasks and we
present a generalization of the Risk-Controlling Prediction Sets (RCPS)
procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise
calibrated intervals for future samples of any diffusion model, and $(ii)$
control a certain notion of risk with respect to a ground truth image with
minimal mean interval length. Differently from existing conformal risk control
procedures, ours relies on a novel convex optimization approach that allows for
multidimensional risk control while provably minimizing the mean interval
length. We illustrate our approach on two real-world image denoising problems:
on natural images of faces as well as on computed tomography (CT) scans of the
abdomen, demonstrating state of the art performance.",http://arxiv.org/pdf/2302.03791v2,stat.ML
2023-02-04 20:53:07+00:00,Conformalized semi-supervised random forest for classification and abnormality detection,"['Yujin Han', 'Mingwenchan Xu', 'Leying Guan']","Traditional classifiers infer labels under the premise that the training and
test samples are generated from the same distribution. This assumption can be
problematic for safety-critical applications such as medical diagnosis and
network attack detection. In this paper, we consider the multi-class
classification problem when the training data and the test data may have
different distributions. We propose conformalized semi-supervised random forest
(CSForest), which constructs set-valued predictions $C(x)$ to include the
correct class label with desired probability while detecting outliers
efficiently. We compare the proposed method to other state-of-art methods in
both a synthetic example and a real data application to demonstrate the
strength of our proposal.",http://arxiv.org/pdf/2302.02237v1,cs.LG
2023-02-03 07:27:50+00:00,Controlling for Stereotypes in Multimodal Language Model Evaluation,"['Manuj Malik', 'Richard Johansson']","We propose a methodology and design two benchmark sets for measuring to what
extent language-and-vision language models use the visual signal in the
presence or absence of stereotypes. The first benchmark is designed to test for
stereotypical colors of common objects, while the second benchmark considers
gender stereotypes. The key idea is to compare predictions when the image
conforms to the stereotype to predictions when it does not.
  Our results show that there is significant variation among multimodal models:
the recent Transformer-based FLAVA seems to be more sensitive to the choice of
image and less affected by stereotypes than older CNN-based models such as
VisualBERT and LXMERT. This effect is more discernible in this type of
controlled setting than in traditional evaluations where we do not know whether
the model relied on the stereotype or the visual signal.",http://arxiv.org/pdf/2302.01582v1,cs.CL
2023-02-02 12:45:30+00:00,Physics Constrained Motion Prediction with Uncertainty Quantification,"['Renukanandan Tumu', 'Lars Lindemann', 'Truong Nghiem', 'Rahul Mangharam']","Predicting the motion of dynamic agents is a critical task for guaranteeing
the safety of autonomous systems. A particular challenge is that motion
prediction algorithms should obey dynamics constraints and quantify prediction
uncertainty as a measure of confidence. We present a physics-constrained
approach for motion prediction which uses a surrogate dynamical model to ensure
that predicted trajectories are dynamically feasible. We propose a two-step
integration consisting of intent and trajectory prediction subject to dynamics
constraints. We also construct prediction regions that quantify uncertainty and
are tailored for autonomous driving by using conformal prediction, a popular
statistical tool. Physics Constrained Motion Prediction achieves a 41% better
ADE, 56% better FDE, and 19% better IoU over a baseline in experiments using an
autonomous racing dataset.",http://arxiv.org/pdf/2302.01060v3,cs.RO
2023-02-02 04:46:14+00:00,Reliable Prediction Intervals with Directly Optimized Inductive Conformal Regression for Deep Learning,"['Haocheng Lei', 'Anthony Bellotti']","By generating prediction intervals (PIs) to quantify the uncertainty of each
prediction in deep learning regression, the risk of wrong predictions can be
effectively controlled. High-quality PIs need to be as narrow as possible,
whilst covering a preset proportion of real labels. At present, many approaches
to improve the quality of PIs can effectively reduce the width of PIs, but they
do not ensure that enough real labels are captured. Inductive Conformal
Predictor (ICP) is an algorithm that can generate effective PIs which is
theoretically guaranteed to cover a preset proportion of data. However,
typically ICP is not directly optimized to yield minimal PI width. However, in
this study, we use Directly Optimized Inductive Conformal Regression (DOICR)
that takes only the average width of PIs as the loss function and increases the
quality of PIs through an optimized scheme under the validity condition that
sufficient real labels are captured in the PIs. Benchmark experiments show that
DOICR outperforms current state-of-the-art algorithms for regression problems
using underlying Deep Neural Network structures for both tabular and image
data.",http://arxiv.org/pdf/2302.00872v1,cs.LG
2023-02-02 02:53:12+00:00,Fast Online Value-Maximizing Prediction Sets with Conformal Cost Control,"['Zhen Lin', 'Shubhendu Trivedi', 'Cao Xiao', 'Jimeng Sun']","Many real-world multi-label prediction problems involve set-valued
predictions that must satisfy specific requirements dictated by downstream
usage. We focus on a typical scenario where such requirements, separately
encoding $\textit{value}$ and $\textit{cost}$, compete with each other. For
instance, a hospital might expect a smart diagnosis system to capture as many
severe, often co-morbid, diseases as possible (the value), while maintaining
strict control over incorrect predictions (the cost). We present a general
pipeline, dubbed as FavMac, to maximize the value while controlling the cost in
such scenarios. FavMac can be combined with almost any multi-label classifier,
affording distribution-free theoretical guarantees on cost control. Moreover,
unlike prior works, it can handle real-world large-scale applications via a
carefully designed online update mechanism, which is of independent interest.
Our methodological and theoretical contributions are supported by experiments
on several healthcare tasks and synthetic datasets - FavMac furnishes higher
value compared with several variants and baselines while maintaining strict
cost control. Our code is available at https://github.com/zlin7/FavMac",http://arxiv.org/pdf/2302.00839v3,cs.LG
2023-01-28 02:48:20+00:00,Pre-Training Protein Encoder via Siamese Sequence-Structure Diffusion Trajectory Prediction,"['Zuobai Zhang', 'Minghao Xu', 'Aurélie Lozano', 'Vijil Chenthamarakshan', 'Payel Das', 'Jian Tang']","Self-supervised pre-training methods on proteins have recently gained
attention, with most approaches focusing on either protein sequences or
structures, neglecting the exploration of their joint distribution, which is
crucial for a comprehensive understanding of protein functions by integrating
co-evolutionary information and structural characteristics. In this work,
inspired by the success of denoising diffusion models in generative tasks, we
propose the DiffPreT approach to pre-train a protein encoder by
sequence-structure joint diffusion modeling. DiffPreT guides the encoder to
recover the native protein sequences and structures from the perturbed ones
along the joint diffusion trajectory, which acquires the joint distribution of
sequences and structures. Considering the essential protein conformational
variations, we enhance DiffPreT by a method called Siamese Diffusion Trajectory
Prediction (SiamDiff) to capture the correlation between different conformers
of a protein. SiamDiff attains this goal by maximizing the mutual information
between representations of diffusion trajectories of structurally-correlated
conformers. We study the effectiveness of DiffPreT and SiamDiff on both atom-
and residue-level structure-based protein understanding tasks. Experimental
results show that the performance of DiffPreT is consistently competitive on
all tasks, and SiamDiff achieves new state-of-the-art performance, considering
the mean ranks on all tasks. Our implementation is available at
https://github.com/DeepGraphLearning/SiamDiff.",http://arxiv.org/pdf/2301.12068v2,cs.LG
2023-01-27 06:43:07+00:00,Conformal inference is (almost) free for neural networks trained with early stopping,"['Ziyi Liang', 'Yanfei Zhou', 'Matteo Sesia']","Early stopping based on hold-out data is a popular regularization technique
designed to mitigate overfitting and increase the predictive accuracy of neural
networks. Models trained with early stopping often provide relatively accurate
predictions, but they generally still lack precise statistical guarantees
unless they are further calibrated using independent hold-out data. This paper
addresses the above limitation with conformalized early stopping: a novel
method that combines early stopping with conformal calibration while
efficiently recycling the same hold-out data. This leads to models that are
both accurate and able to provide exact predictive inferences without multiple
data splits nor overly conservative adjustments. Practical implementations are
developed for different learning tasks -- outlier detection, multi-class
classification, regression -- and their competitive performance is demonstrated
on real data.",http://arxiv.org/pdf/2301.11556v2,stat.ML
2023-01-26 14:40:49+00:00,Conformal Prediction for Trustworthy Detection of Railway Signals,"['Léo Andéol', 'Thomas Fel', 'Florence De Grancey', 'Luca Mossina']","We present an application of conformal prediction, a form of uncertainty
quantification with guarantees, to the detection of railway signals.
State-of-the-art architectures are tested and the most promising one undergoes
the process of conformalization, where a correction is applied to the predicted
bounding boxes (i.e. to their height and width) such that they comply with a
predefined probability of success. We work with a novel exploratory dataset of
images taken from the perspective of a train operator, as a first step to build
and validate future trustworthy machine learning models for the detection of
railway signals.",http://arxiv.org/pdf/2301.11136v1,stat.ML
2023-01-20 11:00:34+00:00,Machine learning and reduced order modelling for the simulation of braided stent deployment,"['Beatrice Bisighini', 'Miquel Aguirre', 'Marco Evangelos Biancolini', 'Federica Trovalusci', 'David Perrin', 'Stephane Avril', 'Baptiste Pierrat']","Endoluminal reconstruction using flow diverters represents a novel paradigm
for the minimally invasive treatment of intracranial aneurysms. The
configuration assumed by these very dense braided stents once deployed within
the parent vessel is not easily predictable and medical volumetric images alone
may be insufficient to plan the treatment satisfactorily. Therefore, here we
propose a fast and accurate machine learning and reduced order modelling
framework, based on finite element simulations, to assist practitioners in the
planning and interventional stages. It consists of a first classification step
to determine a priori whether a simulation will be successful (good conformity
between stent and vessel) or not from a clinical perspective, followed by a
regression step that provides an approximated solution of the deployed stent
configuration. The latter is achieved using a non-intrusive reduced order
modelling scheme that combines the proper orthogonal decomposition algorithm
and Gaussian process regression. The workflow was validated on an idealised
intracranial artery with a saccular aneurysm and the effect of six geometrical
and surgical parameters on the outcome of stent deployment was studied. The
two-step workflow allows the classification of deployment conditions with up to
95% accuracy and real-time prediction of the stent deployed configuration with
an average prediction error never greater than the spatial resolution of 3D
rotational angiography (0.15 mm). These results are promising as they
demonstrate the ability of these techniques to achieve simulations within a few
milliseconds while retaining the mechanical realism and predictability of the
stent deployed configuration.",http://arxiv.org/pdf/2301.08511v1,cs.CE
2023-01-13 02:20:51+00:00,Randomization Tests for Adaptively Collected Data,"['Yash Nair', 'Lucas Janson']","Randomization testing is a fundamental method in statistics, enabling
inferential tasks such as testing for (conditional) independence of random
variables, constructing confidence intervals in semiparametric location models,
and constructing (by inverting a permutation test) model-free prediction
intervals via conformal inference. Randomization tests are exactly valid for
any sample size, but their use is generally confined to exchangeable data. Yet
in many applications, data is routinely collected adaptively via, e.g.,
(contextual) bandit and reinforcement learning algorithms or adaptive
experimental designs. In this paper we present a general framework for
randomization testing on adaptively collected data (despite its
non-exchangeability) that uses a novel weighted randomization test, for which
we also present novel computationally tractable resampling algorithms for
various popular adaptive assignment algorithms, data-generating environments,
and types of inferential tasks. Finally, we demonstrate via a range of
simulations the efficacy of our framework for both testing and
confidence/prediction interval construction.",http://arxiv.org/pdf/2301.05365v2,stat.ME
2023-01-11 09:44:55+00:00,Loss-Controlling Calibration for Predictive Models,"['Di Wang', 'Junzhi Shi', 'Pingping Wang', 'Shuo Zhuang', 'Hongyue Li']","We propose a learning framework for calibrating predictive models to make
loss-controlling prediction for exchangeable data, which extends our recently
proposed conformal loss-controlling prediction for more general cases. By
comparison, the predictors built by the proposed loss-controlling approach are
not limited to set predictors, and the loss function can be any measurable
function without the monotone assumption. To control the loss values in an
efficient way, we introduce transformations preserving exchangeability to prove
finite-sample controlling guarantee when the test label is obtained, and then
develop an approximation approach to construct predictors. The transformations
can be built on any predefined function, which include using optimization
algorithms for parameter searching. This approach is a natural extension of
conformal loss-controlling prediction, since it can be reduced to the latter
when the set predictors have the nesting property and the loss functions are
monotone. Our proposed method is applied to selective regression and
high-impact weather forecasting problems, which demonstrates its effectiveness
for general loss-controlling prediction.",http://arxiv.org/pdf/2301.04378v2,cs.LG
2023-01-06 08:58:49+00:00,Conformal Loss-Controlling Prediction,"['Di Wang', 'Ping Wang', 'Zhong Ji', 'Xiaojun Yang', 'Hongyue Li']","Conformal prediction is a learning framework controlling prediction coverage
of prediction sets, which can be built on any learning algorithm for point
prediction. This work proposes a learning framework named conformal
loss-controlling prediction, which extends conformal prediction to the
situation where the value of a loss function needs to be controlled. Different
from existing works about risk-controlling prediction sets and conformal risk
control with the purpose of controlling the expected values of loss functions,
the proposed approach in this paper focuses on the loss for any test object,
which is an extension of conformal prediction from miscoverage loss to some
general loss. The controlling guarantee is proved under the assumption of
exchangeability of data in finite-sample cases and the framework is tested
empirically for classification with a class-varying loss and statistical
postprocessing of numerical weather forecasting applications, which are
introduced as point-wise classification and point-wise regression problems. All
theoretical analysis and experimental results confirm the effectiveness of our
loss-controlling approach.",http://arxiv.org/pdf/2301.02424v1,cs.LG
2023-01-05 13:59:29+00:00,"Towards Long-Term Time-Series Forecasting: Feature, Pattern, and Distribution","['Yan Li', 'Xinjiang Lu', 'Haoyi Xiong', 'Jian Tang', 'Jiantao Su', 'Bo Jin', 'Dejing Dou']","Long-term time-series forecasting (LTTF) has become a pressing demand in many
applications, such as wind power supply planning. Transformer models have been
adopted to deliver high prediction capacity because of the high computational
self-attention mechanism. Though one could lower the complexity of Transformers
by inducing the sparsity in point-wise self-attentions for LTTF, the limited
information utilization prohibits the model from exploring the complex
dependencies comprehensively. To this end, we propose an efficient
Transformerbased model, named Conformer, which differentiates itself from
existing methods for LTTF in three aspects: (i) an encoder-decoder architecture
incorporating a linear complexity without sacrificing information utilization
is proposed on top of sliding-window attention and Stationary and Instant
Recurrent Network (SIRN); (ii) a module derived from the normalizing flow is
devised to further improve the information utilization by inferring the outputs
with the latent variables in SIRN directly; (iii) the inter-series correlation
and temporal dynamics in time-series data are modeled explicitly to fuel the
downstream self-attention mechanism. Extensive experiments on seven real-world
datasets demonstrate that Conformer outperforms the state-of-the-art methods on
LTTF and generates reliable prediction results with uncertainty quantification.",http://arxiv.org/pdf/2301.02068v1,cs.LG
2023-01-04 05:36:56+00:00,Audio-Visual Efficient Conformer for Robust Speech Recognition,"['Maxime Burchi', 'Radu Timofte']","End-to-end Automatic Speech Recognition (ASR) systems based on neural
networks have seen large improvements in recent years. The availability of
large scale hand-labeled datasets and sufficient computing resources made it
possible to train powerful deep neural networks, reaching very low Word Error
Rate (WER) on academic benchmarks. However, despite impressive performance on
clean audio samples, a drop of performance is often observed on noisy speech.
In this work, we propose to improve the noise robustness of the recently
proposed Efficient Conformer Connectionist Temporal Classification (CTC)-based
architecture by processing both audio and visual modalities. We improve
previous lip reading methods using an Efficient Conformer back-end on top of a
ResNet-18 visual front-end and by adding intermediate CTC losses between
blocks. We condition intermediate block features on early predictions using
Inter CTC residual modules to relax the conditional independence assumption of
CTC-based models. We also replace the Efficient Conformer grouped attention by
a more efficient and simpler attention mechanism that we call patch attention.
We experiment with publicly available Lip Reading Sentences 2 (LRS2) and Lip
Reading Sentences 3 (LRS3) datasets. Our experiments show that using audio and
visual modalities allows to better recognize speech in the presence of
environmental noise and significantly accelerate training, reaching lower WER
with 4 times less training steps. Our Audio-Visual Efficient Conformer (AVEC)
model achieves state-of-the-art performance, reaching WER of 2.3% and 1.8% on
LRS2 and LRS3 test sets. Code and pretrained models are available at
https://github.com/burchim/AVEC.",http://arxiv.org/pdf/2301.01456v1,cs.CV
2023-01-02 10:27:10+00:00,Selective Conformal Inference with FCR Control,"['Yajie Bao', 'Yuyang Huo', 'Haojie Ren', 'Changliang Zou']","Conformal inference is a popular tool for constructing prediction intervals
(PI). We consider here the scenario of post-selection/selective conformal
inference, that is PIs are reported only for individuals selected from an
unlabeled test data. To account for multiplicity, we develop a general split
conformal framework to construct selective PIs with the false
coverage-statement rate (FCR) control. We first investigate the Benjamini and
Yekutieli (2005)'s FCR-adjusted method in the present setting, and show that it
is able to achieve FCR control but yields uniformly inflated PIs. We then
propose a novel solution to the problem, named as Selective COnditional
conformal Predictions (SCOP), which entails performing selection procedures on
both calibration set and test set and construct marginal conformal PIs on the
selected sets by the aid of conditional empirical distribution obtained by the
calibration set. Under a unified framework and exchangeable assumptions, we
show that the SCOP can exactly control the FCR. More importantly, we provide
non-asymptotic miscoverage bounds for a general class of selection procedures
beyond exchangeablity and discuss the conditions under which the SCOP is able
to control the FCR. As special cases, the SCOP with quantile-based selection or
conformal p-values-based multiple testing procedures enjoys valid coverage
guarantee under mild conditions. Numerical results confirm the effectiveness
and robustness of SCOP in FCR control and show that it achieves more narrowed
PIs over existing methods in many settings.",http://arxiv.org/pdf/2301.00584v4,stat.ME
2022-12-30 09:34:29+00:00,Conformal Prediction Intervals for Remaining Useful Lifetime Estimation,"['Alireza Javanmardi', 'Eyke Hüllermeier']","The main objective of Prognostics and Health Management is to estimate the
Remaining Useful Lifetime (RUL), namely, the time that a system or a piece of
equipment is still in working order before starting to function incorrectly. In
recent years, numerous machine learning algorithms have been proposed for RUL
estimation, mainly focusing on providing more accurate RUL predictions.
However, there are many sources of uncertainty in the problem, such as inherent
randomness of systems failure, lack of knowledge regarding their future states,
and inaccuracy of the underlying predictive models, making it infeasible to
predict the RULs precisely. Hence, it is of utmost importance to quantify the
uncertainty alongside the RUL predictions. In this work, we investigate the
conformal prediction (CP) framework that represents uncertainty by predicting
sets of possible values for the target variable (intervals in the case of RUL)
instead of making point predictions. Under very mild technical assumptions, CP
formally guarantees that the actual value (true RUL) is covered by the
predicted set with a degree of certainty that can be prespecified. We study
three CP algorithms to conformalize any single-point RUL predictor and turn it
into a valid interval predictor. Finally, we conformalize two single-point RUL
predictors, deep convolutional neural networks and gradient boosting, and
illustrate their performance on the Commercial Modular Aero-Propulsion System
Simulation (C-MAPSS) data sets.",http://arxiv.org/pdf/2212.14612v1,cs.LG
2022-12-23 17:45:38+00:00,Posterior-Variance-Based Error Quantification for Inverse Problems in Imaging,"['Dominik Narnhofer', 'Andreas Habring', 'Martin Holler', 'Thomas Pock']","In this work, a method for obtaining pixel-wise error bounds in Bayesian
regularization of inverse imaging problems is introduced. The proposed method
employs estimates of the posterior variance together with techniques from
conformal prediction in order to obtain coverage guarantees for the error
bounds, without making any assumption on the underlying data distribution. It
is generally applicable to Bayesian regularization approaches, independent,
e.g., of the concrete choice of the prior. Furthermore, the coverage guarantees
can also be obtained in case only approximate sampling from the posterior is
possible. With this in particular, the proposed framework is able to
incorporate any learned prior in a black-box manner. Guaranteed coverage
without assumptions on the underlying distributions is only achievable since
the magnitude of the error bounds is, in general, unknown in advance.
Nevertheless, experiments with multiple regularization approaches presented in
the paper confirm that in practice, the obtained error bounds are rather tight.
For realizing the numerical experiments, also a novel primal-dual Langevin
algorithm for sampling from non-smooth distributions is introduced in this
work.",http://arxiv.org/pdf/2212.12499v1,cs.CV
2022-12-23 00:50:41+00:00,Anomaly Detection using Ensemble Classification and Evidence Theory,"['Fernando Arévalo', 'Tahasanul Ibrahim', 'Christian Alison M. Piolo', 'Andreas Schwung']","Multi-class ensemble classification remains a popular focus of investigation
within the research community. The popularization of cloud services has sped up
their adoption due to the ease of deploying large-scale machine-learning
models. It has also drawn the attention of the industrial sector because of its
ability to identify common problems in production. However, there are
challenges to conform an ensemble classifier, namely a proper selection and
effective training of the pool of classifiers, the definition of a proper
architecture for multi-class classification, and uncertainty quantification of
the ensemble classifier. The robustness and effectiveness of the ensemble
classifier lie in the selection of the pool of classifiers, as well as in the
learning process. Hence, the selection and the training procedure of the pool
of classifiers play a crucial role. An (ensemble) classifier learns to detect
the classes that were used during the supervised training. However, when
injecting data with unknown conditions, the trained classifier will intend to
predict the classes learned during the training. To this end, the uncertainty
of the individual and ensemble classifier could be used to assess the learning
capability. We present a novel approach for novel detection using ensemble
classification and evidence theory. A pool selection strategy is presented to
build a solid ensemble classifier. We present an architecture for multi-class
ensemble classification and an approach to quantify the uncertainty of the
individual classifiers and the ensemble classifier. We use uncertainty for the
anomaly detection approach. Finally, we use the benchmark Tennessee Eastman to
perform experiments to test the ensemble classifier's prediction and anomaly
detection capabilities.",http://arxiv.org/pdf/2212.12092v1,cs.LG
2022-12-16 21:51:51+00:00,Distribution-aware Goal Prediction and Conformant Model-based Planning for Safe Autonomous Driving,"['Jonathan Francis', 'Bingqing Chen', 'Weiran Yao', 'Eric Nyberg', 'Jean Oh']","The feasibility of collecting a large amount of expert demonstrations has
inspired growing research interests in learning-to-drive settings, where models
learn by imitating the driving behaviour from experts. However, exclusively
relying on imitation can limit agents' generalisability to novel scenarios that
are outside the support of the training data. In this paper, we address this
challenge by factorising the driving task, based on the intuition that modular
architectures are more generalisable and more robust to changes in the
environment compared to monolithic, end-to-end frameworks. Specifically, we
draw inspiration from the trajectory forecasting community and reformulate the
learning-to-drive task as obstacle-aware perception and grounding,
distribution-aware goal prediction, and model-based planning. Firstly, we train
the obstacle-aware perception module to extract salient representation of the
visual context. Then, we learn a multi-modal goal distribution by performing
conditional density-estimation using normalising flow. Finally, we ground
candidate trajectory predictions road geometry, and plan the actions based on
on vehicle dynamics. Under the CARLA simulator, we report state-of-the-art
results on the CARNOVEL benchmark.",http://arxiv.org/pdf/2212.08729v1,cs.RO
2022-12-16 09:58:11+00:00,Easy Uncertainty Quantification (EasyUQ): Generating Predictive Distributions from Single-valued Model Output,"['Eva-Maria Walz', 'Alexander Henzi', 'Johanna Ziegel', 'Tilmann Gneiting']","How can we quantify uncertainty if our favorite computational tool - be it a
numerical, a statistical, or a machine learning approach, or just any computer
model - provides single-valued output only? In this article, we introduce the
Easy Uncertainty Quantification (EasyUQ) technique, which transforms
real-valued model output into calibrated statistical distributions, based
solely on training data of model output-outcome pairs, without any need to
access model input. In its basic form, EasyUQ is a special case of the recently
introduced Isotonic Distributional Regression (IDR) technique that leverages
the pool-adjacent-violators algorithm for nonparametric isotonic regression.
EasyUQ yields discrete predictive distributions that are calibrated and optimal
in finite samples, subject to stochastic monotonicity. The workflow is fully
automated, without any need for tuning. The Smooth EasyUQ approach supplements
IDR with kernel smoothing, to yield continuous predictive distributions that
preserve key properties of the basic form, including both, stochastic
monotonicity with respect to the original model output, and asymptotic
consistency. For the selection of kernel parameters, we introduce multiple
one-fit grid search, a computationally much less demanding approximation to
leave-one-out cross-validation. We use simulation examples and forecast data
from weather prediction to illustrate the techniques. In a study of benchmark
problems from machine learning, we show how EasyUQ and Smooth EasyUQ can be
integrated into the workflow of neural network learning and hyperparameter
tuning, and find EasyUQ to be competitive with conformal prediction, as well as
more elaborate input-based approaches.",http://arxiv.org/pdf/2212.08376v2,stat.ME
2022-12-15 12:52:23+00:00,Calibrating AI Models for Wireless Communications via Conformal Prediction,"['Kfir M. Cohen', 'Sangwoo Park', 'Osvaldo Simeone', 'Shlomo Shamai']","When used in complex engineered systems, such as communication networks,
artificial intelligence (AI) models should be not only as accurate as possible,
but also well calibrated. A well-calibrated AI model is one that can reliably
quantify the uncertainty of its decisions, assigning high confidence levels to
decisions that are likely to be correct and low confidence levels to decisions
that are likely to be erroneous. This paper investigates the application of
conformal prediction as a general framework to obtain AI models that produce
decisions with formal calibration guarantees. Conformal prediction transforms
probabilistic predictors into set predictors that are guaranteed to contain the
correct answer with a probability chosen by the designer. Such formal
calibration guarantees hold irrespective of the true, unknown, distribution
underlying the generation of the variables of interest, and can be defined in
terms of ensemble or time-averaged probabilities. In this paper, conformal
prediction is applied for the first time to the design of AI for communication
systems in conjunction to both frequentist and Bayesian learning, focusing on
demodulation, modulation classification, and channel prediction.",http://arxiv.org/pdf/2212.07775v1,cs.LG
2022-12-15 10:34:15+00:00,Classification-Based Opinion Formation Model Embedding Agents' Psychological Traits,"['Carlos Andres Devia', 'Giulia Giordano']","We propose an agent-based opinion formation model characterised by a two-fold
novelty. First, we realistically assume that each agent cannot measure the
opinion of its neighbours with infinite resolution and accuracy, and hence it
can only classify the opinion of others as agreeing much more, or more, or
comparably, or less, or much less (than itself) with a given statement. This
leads to a classification-based rule for opinion update. Second, we consider
three complementary agent traits suggested by significant sociological and
psychological research: conformism, radicalism and stubbornness. We rely on
World Values Survey data to show that the proposed model has the potential to
predict the evolution of opinions in real life: the classification-based
approach and complementary agent traits produce rich collective behaviours,
such as polarisation, consensus, and clustering, which can yield predicted
opinions similar to survey results.",http://arxiv.org/pdf/2212.07709v1,cs.SI
2022-12-10 10:19:53+00:00,Image augmentation with conformal mappings for a convolutional neural network,"['Oona Rainio', 'Mohamed M. S. Nasser', 'Matti Vuorinen', 'Riku Klén']","For augmentation of the square-shaped image data of a convolutional neural
network (CNN), we introduce a new method, in which the original images are
mapped onto a disk with a conformal mapping, rotated around the center of this
disk and mapped under such a M\""obius transformation that preserves the disk,
and then mapped back onto their original square shape. This process does not
result the loss of information caused by removing areas from near the edges of
the original images unlike the typical transformations used in the data
augmentation for a CNN. We offer here the formulas of all the mappings needed
together with detailed instructions how to write a code for transforming the
images. The new method is also tested with simulated data and, according the
results, using this method to augment the training data of 10 images into 40
images decreases the amount of the error in the predictions by a CNN for a test
set of 160 images in a statistically significant way (p-value=0.0360).",http://arxiv.org/pdf/2212.05258v2,cs.CV
2022-12-07 15:29:21+00:00,Intervening With Confidence: Conformal Prescriptive Monitoring of Business Processes,"['Mahmoud Shoush', 'Marlon Dumas']","Prescriptive process monitoring methods seek to improve the performance of a
process by selectively triggering interventions at runtime (e.g., offering a
discount to a customer) to increase the probability of a desired case outcome
(e.g., a customer making a purchase). The backbone of a prescriptive process
monitoring method is an intervention policy, which determines for which cases
and when an intervention should be executed. Existing methods in this field
rely on predictive models to define intervention policies; specifically, they
consider policies that trigger an intervention when the estimated probability
of a negative outcome exceeds a threshold. However, the probabilities computed
by a predictive model may come with a high level of uncertainty (low
confidence), leading to unnecessary interventions and, thus, wasted effort.
This waste is particularly problematic when the resources available to execute
interventions are limited. To tackle this shortcoming, this paper proposes an
approach to extend existing prescriptive process monitoring methods with
so-called conformal predictions, i.e., predictions with confidence guarantees.
An empirical evaluation using real-life public datasets shows that conformal
predictions enhance the net gain of prescriptive process monitoring methods
under limited resources.",http://arxiv.org/pdf/2212.03710v1,cs.LG
2022-12-07 05:07:27+00:00,Sequential Predictive Conformal Inference for Time Series,"['Chen Xu', 'Yao Xie']","We present a new distribution-free conformal prediction algorithm for
sequential data (e.g., time series), called the \textit{sequential predictive
conformal inference} (\texttt{SPCI}). We specifically account for the nature
that time series data are non-exchangeable, and thus many existing conformal
prediction algorithms are not applicable. The main idea is to adaptively
re-estimate the conditional quantile of non-conformity scores (e.g., prediction
residuals), upon exploiting the temporal dependence among them. More precisely,
we cast the problem of conformal prediction interval as predicting the quantile
of a future residual, given a user-specified point prediction algorithm.
Theoretically, we establish asymptotic valid conditional coverage upon
extending consistency analyses in quantile regression. Using simulation and
real-data experiments, we demonstrate a significant reduction in interval width
of \texttt{SPCI} compared to other existing methods under the desired empirical
coverage.",http://arxiv.org/pdf/2212.03463v3,stat.ML
2022-12-06 19:32:06+00:00,Copula Conformal Prediction for Multi-step Time Series Forecasting,"['Sophia Sun', 'Rose Yu']","Accurate uncertainty measurement is a key step to building robust and
reliable machine learning systems. Conformal prediction is a distribution-free
uncertainty quantification algorithm popular for its ease of implementation,
statistical coverage guarantees, and versatility for underlying forecasters.
However, existing conformal prediction algorithms for time series are limited
to single-step prediction without considering the temporal dependency. In this
paper we propose a Copula Conformal Prediction algorithm for multivariate,
multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has
finite sample validity guarantee. On several synthetic and real-world
multivariate time series datasets, we show that CopulaCPTS produces more
calibrated and sharp confidence intervals for multi-step prediction tasks than
existing techniques.",http://arxiv.org/pdf/2212.03281v2,cs.LG
2022-12-05 07:44:32+00:00,NBC2: Multichannel Speech Separation with Revised Narrow-band Conformer,"['Changsheng Quan', 'Xiaofei Li']","This work proposes a multichannel narrow-band speech separation network. In
the short-time Fourier transform (STFT) domain, the proposed network processes
each frequency independently, and all frequencies use a shared network. For
each frequency, the network performs end-to-end speech separation, namely
taking as input the STFT coefficients of microphone signals, and predicting the
separated STFT coefficients of multiple speakers. The proposed network learns
to cluster the frame-wise spatial/steering vectors that belong to different
speakers. It is mainly composed of three components. First, a self-attention
network. Clustering of spatial vectors shares a similar principle with the
self-attention mechanism in the sense of computing the similarity of vectors
and then aggregating similar vectors. Second, a convolutional feed-forward
network. The convolutional layers are employed for signal smoothing and
reverberation processing. Third, a novel hidden-layer normalization method,
i.e. group batch normalization (GBN), is especially designed for the proposed
narrow-band network to maintain the distribution of hidden units over
frequencies. Overall, the proposed network is named NBC2, as it is a revised
version of our previous NBC (narrow-band conformer) network. Experiments show
that 1) the proposed network outperforms other state-of-the-art methods by a
large margin, 2) the proposed GBN improves the signal-to-distortion ratio by 3
dB, relative to other normalization methods, such as batch/layer/group
normalization, 3) the proposed narrow-band network is spectrum-agnostic, as it
does not learn spectral patterns, and 4) the proposed network is indeed
performing frame clustering (demonstrated by the attention maps).",http://arxiv.org/pdf/2212.02076v1,cs.SD
2022-12-01 04:54:22+00:00,Adaptive Conformal Prediction for Motion Planning among Dynamic Agents,"['Anushri Dixit', 'Lars Lindemann', 'Skylar Wei', 'Matthew Cleaveland', 'George J. Pappas', 'Joel W. Burdick']","This paper proposes an algorithm for motion planning among dynamic agents
using adaptive conformal prediction. We consider a deterministic control system
and use trajectory predictors to predict the dynamic agents' future motion,
which is assumed to follow an unknown distribution. We then leverage ideas from
adaptive conformal prediction to dynamically quantify prediction uncertainty
from an online data stream. Particularly, we provide an online algorithm uses
delayed agent observations to obtain uncertainty sets for multistep-ahead
predictions with probabilistic coverage. These uncertainty sets are used within
a model predictive controller to safely navigate among dynamic agents. While
most existing data-driven prediction approached quantify prediction uncertainty
heuristically, we quantify the true prediction uncertainty in a
distribution-free, adaptive manner that even allows to capture changes in
prediction quality and the agents' motion. We empirically evaluate of our
algorithm on a simulation case studies where a drone avoids a flying frisbee.",http://arxiv.org/pdf/2212.00278v1,cs.RO
2022-11-29 18:41:20+00:00,Will My Robot Achieve My Goals? Predicting the Probability that an MDP Policy Reaches a User-Specified Behavior Target,"['Alexander Guyer', 'Thomas G. Dietterich']","As an autonomous system performs a task, it should maintain a calibrated
estimate of the probability that it will achieve the user's goal. If that
probability falls below some desired level, it should alert the user so that
appropriate interventions can be made. This paper considers settings where the
user's goal is specified as a target interval for a real-valued performance
summary, such as the cumulative reward, measured at a fixed horizon $H$. At
each time $t \in \{0, \ldots, H-1\}$, our method produces a calibrated estimate
of the probability that the final cumulative reward will fall within a
user-specified target interval $[y^-,y^+].$ Using this estimate, the autonomous
system can raise an alarm if the probability drops below a specified threshold.
We compute the probability estimates by inverting conformal prediction. Our
starting point is the Conformalized Quantile Regression (CQR) method of Romano
et al., which applies split-conformal prediction to the results of quantile
regression. CQR is not invertible, but by using the conditional cumulative
distribution function (CDF) as the non-conformity measure, we show how to
obtain an invertible modification that we call \textbf{P}robability-space
\textbf{C}onformalized \textbf{Q}uantile \textbf{R}egression (PCQR). Like CQR,
PCQR produces well-calibrated conditional prediction intervals with
finite-sample marginal guarantees. By inverting PCQR, we obtain marginal
guarantees for the probability that the cumulative reward of an autonomous
system will fall within an arbitrary user-specified target intervals.
Experiments on two domains confirm that these probabilities are
well-calibrated.",http://arxiv.org/pdf/2211.16462v1,cs.LG
2022-11-29 14:21:49+00:00,A Cross-Conformal Predictor for Multi-label Classification,['Harris Papadopoulos'],"Unlike the typical classification setting where each instance is associated
with a single class, in multi-label learning each instance is associated with
multiple classes simultaneously. Therefore the learning task in this setting is
to predict the subset of classes to which each instance belongs. This work
examines the application of a recently developed framework called Conformal
Prediction (CP) to the multi-label learning setting. CP complements the
predictions of machine learning algorithms with reliable measures of
confidence. As a result the proposed approach instead of just predicting the
most likely subset of classes for a new unseen instance, also indicates the
likelihood of each predicted subset being correct. This additional information
is especially valuable in the multi-label setting where the overall uncertainty
is extremely high.",http://arxiv.org/pdf/2211.16238v1,cs.LG
2022-11-27 21:17:48+00:00,Applying Deep Reinforcement Learning to the HP Model for Protein Structure Prediction,"['Kaiyuan Yang', 'Houjing Huang', 'Olafs Vandans', 'Adithya Murali', 'Fujia Tian', 'Roland H. C. Yap', 'Liang Dai']","A central problem in computational biophysics is protein structure
prediction, i.e., finding the optimal folding of a given amino acid sequence.
This problem has been studied in a classical abstract model, the HP model,
where the protein is modeled as a sequence of H (hydrophobic) and P (polar)
amino acids on a lattice. The objective is to find conformations maximizing H-H
contacts. It is known that even in this reduced setting, the problem is
intractable (NP-hard). In this work, we apply deep reinforcement learning (DRL)
to the two-dimensional HP model. We can obtain the conformations of best known
energies for benchmark HP sequences with lengths from 20 to 50. Our DRL is
based on a deep Q-network (DQN). We find that a DQN based on long short-term
memory (LSTM) architecture greatly enhances the RL learning ability and
significantly improves the search process. DRL can sample the state space
efficiently, without the need of manual heuristics. Experimentally we show that
it can find multiple distinct best-known solutions per trial. This study
demonstrates the effectiveness of deep reinforcement learning in the HP model
for protein folding.",http://arxiv.org/pdf/2211.14939v2,cs.LG
2022-11-26 12:54:45+00:00,Distribution Free Prediction Sets for Node Classification,['Jase Clarkson'],"Graph Neural Networks (GNNs) are able to achieve high classification accuracy
on many important real world datasets, but provide no rigorous notion of
predictive uncertainty. Quantifying the confidence of GNN models is difficult
due to the dependence between datapoints induced by the graph structure.
  We leverage recent advances in conformal prediction to construct prediction
sets for node classification in inductive learning scenarios. We do this by
taking an existing approach for conformal classification that relies on
\textit{exchangeable} data and modifying it by appropriately weighting the
conformal scores to reflect the network structure. We show through experiments
on standard benchmark datasets using popular GNN models that our approach
provides tighter and better calibrated prediction sets than a naive application
of conformal prediction.",http://arxiv.org/pdf/2211.14555v2,stat.ML
2022-11-21 13:04:37+00:00,Sequentially Sampled Chunk Conformer for Streaming End-to-End ASR,"['Fangyuan Wang', 'Bo Xu']","This paper presents an in-depth study on a Sequentially Sampled Chunk
Conformer, SSC-Conformer, for streaming End-to-End (E2E) ASR. The SSC-Conformer
first demonstrates the significant performance gains from using the
sequentially sampled chunk-wise multi-head self-attention (SSC-MHSA) in the
Conformer encoder by allowing efficient cross-chunk interactions while keeping
linear complexities. Furthermore, it explores taking advantage of chunked
convolution to make use of the chunk-wise future context and integrates with
casual convolution in the convolution layers to further reduce CER. We verify
the proposed SSC-Conformer on the AISHELL-1 benchmark and experimental results
show that a state-of-the-art performance for streaming E2E ASR is achieved with
CER 5.33% without LM rescoring. And, owing to its linear complexity, the
SSC-Conformer can train with large batch sizes and infer more efficiently.",http://arxiv.org/pdf/2211.11419v3,cs.SD
2022-11-09 12:37:52+00:00,ChromoSkein: Untangling Three-Dimensional Chromatin Fiber With a Web-Based Visualization Framework,"['Matúš Talčík', 'Filip Opálený', 'Tereza Clarence', 'Katarína Furmanová', 'Jan Byška', 'Barbora Kozlíková', 'David Kouřil']","We present ChromoSkein, a web-based tool for visualizing three-dimensional
chromatin models. The spatial organization of chromatin is essential to its
function. Experimental methods, namely Hi-C, reveal the spatial conformation
but output a 2D matrix representation. Biologists leverage simulation to bring
this information back to 3D, assembling a 3D chromatin shape prediction using
the 2D matrices as constraints. Our overview of existing chromatin
visualization software shows that the available tools limit the utility of 3D
through ineffective shading and a lack of advanced interactions. We designed
ChromoSkein to encourage analytical work directly with the 3D representation.
Our tool features a 3D view that supports understanding the shape of the highly
tangled chromatin fiber and the spatial relationships of its parts. Users can
explore and filter the 3D model using two interactions. First, they can manage
occlusion both by toggling the visibility of semantic parts and by adding
cutting planes. Second, they can segment the model through the creation of
custom selections. To complement the 3D view, we link the spatial
representation with non-spatial genomic data, such as 2D Hi-C maps and 1D
genomic signals. We demonstrate the utility of ChromoSkein in two exemplary use
cases that examine functional genomic loci in the spatial context of
chromosomes and the whole genome.",http://arxiv.org/pdf/2211.05125v1,cs.HC
2022-11-02 16:07:53+00:00,Conformalized survival analysis with adaptive cutoffs,"['Yu Gui', 'Rohan Hore', 'Zhimei Ren', 'Rina Foygel Barber']","This paper introduces a method that constructs valid and efficient lower
predictive bounds (LPBs) for survival times with censored data. Traditional
methods for survival analysis often assume a parametric model for the
distribution of survival time as a function of the measured covariates, or
assume that this conditional distribution is captured well with a
non-parametric method such as random forests; however, these methods may lead
to undercoverage if their assumptions are not satisfied. In this paper, we
build on recent work by Cand\`es et al. (2021), which offers a more
assumption-lean approach to the problem. Their approach first subsets the data
to discard any data points with early censoring times and then uses a
reweighting technique (namely, weighted conformal inference (Tibshirani et al.,
2019)) to correct for the distribution shift introduced by this subsetting
procedure. For our new method, instead of constraining to a fixed threshold for
the censoring time when subsetting the data, we allow for a covariate-dependent
and data-adaptive subsetting step, which is better able to capture the
heterogeneity of the censoring mechanism. As a result, our method can lead to
LPBs that are less conservative and give more accurate information. We show
that in the Type I right-censoring setting, if either of the censoring
mechanism or the conditional quantile of survival time is well estimated, our
proposed procedure achieves approximately exact marginal coverage, where in the
latter case we additionally have approximate conditional coverage. We evaluate
the validity and efficiency of our proposed algorithm in numerical experiments,
illustrating its advantage when compared with other competing methods. Finally,
our method is applied to a real dataset to generate LPBs for users' active
times on a mobile app.",http://arxiv.org/pdf/2211.01227v2,stat.ME
2022-10-31 15:41:13+00:00,Exact and Approximate Conformal Inference in Multiple Dimensions,"['Chancellor Johnstone', 'Eugene Ndiaye']","It is common in machine learning to estimate a response y given covariate
information x. However, these predictions alone do not quantify any uncertainty
associated with said predictions. One way to overcome this deficiency is with
conformal inference methods, which construct a set containing the unobserved
response y with a prescribed probability. Unfortunately, even with
one-dimensional responses, conformal inference is computationally expensive
despite recent encouraging advances. In this paper, we explore the
multidimensional response case within a regression setting, delivering exact
derivations of conformal inference p-values when the predictive model can be
described as a linear function of y. Additionally, we propose different
efficient ways of approximating the conformal prediction region for non-linear
predictors while preserving computational advantages. We also provide empirical
justification for these approaches using a real-world data example.",http://arxiv.org/pdf/2210.17405v1,stat.ML
2022-10-31 07:32:12+00:00,Do Charge Prediction Models Learn Legal Theory?,"['Zhenwei An', 'Quzhe Huang', 'Cong Jiang', 'Yansong Feng', 'Dongyan Zhao']","The charge prediction task aims to predict the charge for a case given its
fact description. Recent models have already achieved impressive accuracy in
this task, however, little is understood about the mechanisms they use to
perform the judgment.For practical applications, a charge prediction model
should conform to the certain legal theory in civil law countries, as under the
framework of civil law, all cases are judged according to certain local legal
theories. In China, for example, nearly all criminal judges make decisions
based on the Four Elements Theory (FET).In this paper, we argue that
trustworthy charge prediction models should take legal theories into
consideration, and standing on prior studies in model interpretation, we
propose three principles for trustworthy models should follow in this task,
which are sensitive, selective, and presumption of innocence.We further design
a new framework to evaluate whether existing charge prediction models learn
legal theories. Our findings indicate that, while existing charge prediction
models meet the selective principle on a benchmark dataset, most of them are
still not sensitive enough and do not satisfy the presumption of innocence. Our
code and dataset are released at https://github.com/ZhenweiAn/EXP_LJP.",http://arxiv.org/pdf/2210.17108v1,cs.CL
2022-10-30 21:27:29+00:00,"Learning to Defer to Multiple Experts: Consistent Surrogate Losses, Confidence Calibration, and Conformal Ensembles","['Rajeev Verma', 'Daniel Barrejón', 'Eric Nalisnick']","We study the statistical properties of learning to defer (L2D) to multiple
experts. In particular, we address the open problems of deriving a consistent
surrogate loss, confidence calibration, and principled ensembling of experts.
Firstly, we derive two consistent surrogates -- one based on a softmax
parameterization, the other on a one-vs-all (OvA) parameterization -- that are
analogous to the single expert losses proposed by Mozannar and Sontag (2020)
and Verma and Nalisnick (2022), respectively. We then study the frameworks'
ability to estimate P( m_j = y | x ), the probability that the jth expert will
correctly predict the label for x. Theory shows the softmax-based loss causes
mis-calibration to propagate between the estimates while the OvA-based loss
does not (though in practice, we find there are trade offs). Lastly, we propose
a conformal inference technique that chooses a subset of experts to query when
the system defers. We perform empirical validation on tasks for galaxy, skin
lesion, and hate speech classification.",http://arxiv.org/pdf/2210.16955v2,stat.ML
2022-10-27 23:52:14+00:00,Towards Reliable Zero Shot Classification in Self-Supervised Models with Conformal Prediction,"['Bhawesh Kumar', 'Anil Palepu', 'Rudraksh Tuwani', 'Andrew Beam']","Self-supervised models trained with a contrastive loss such as CLIP have
shown to be very powerful in zero-shot classification settings. However, to be
used as a zero-shot classifier these models require the user to provide new
captions over a fixed set of labels at test time. In many settings, it is hard
or impossible to know if a new query caption is compatible with the source
captions used to train the model. We address these limitations by framing the
zero-shot classification task as an outlier detection problem and develop a
conformal prediction procedure to assess when a given test caption may be
reliably used. On a real-world medical example, we show that our proposed
conformal procedure improves the reliability of CLIP-style models in the
zero-shot classification setting, and we provide an empirical analysis of the
factors that may affect its performance.",http://arxiv.org/pdf/2210.15805v1,cs.LG
2022-10-26 14:12:24+00:00,Distribution-Free Finite-Sample Guarantees and Split Conformal Prediction,['Roel Hulsman'],"Modern black-box predictive models are often accompanied by weak performance
guarantees that only hold asymptotically in the size of the dataset or require
strong parametric assumptions. In response to this, split conformal prediction
represents a promising avenue to obtain finite-sample guarantees under minimal
distribution-free assumptions. Although prediction set validity most often
concerns marginal coverage, we explore the related but different guarantee of
tolerance regions, reformulating known results in the language of nested
prediction sets and extending on the duality between marginal coverage and
tolerance regions. Furthermore, we highlight the connection between split
conformal prediction and classical tolerance predictors developed in the 1940s,
as well as recent developments in distribution-free risk control. One result
that transfers from classical tolerance predictors is that the coverage of a
prediction set based on order statistics, conditional on the calibration set,
is a random variable stochastically dominating the Beta distribution. We
demonstrate the empirical effectiveness of our findings on synthetic and real
datasets using a popular split conformal prediction procedure called
conformalized quantile regression (CQR).",http://arxiv.org/pdf/2210.14735v1,stat.ML
2022-10-23 05:19:50+00:00,Conformal Predictor for Improving Zero-shot Text Classification Efficiency,"['Prafulla Kumar Choubey', 'Yu Bai', 'Chien-Sheng Wu', 'Wenhao Liu', 'Nazneen Rajani']","Pre-trained language models (PLMs) have been shown effective for zero-shot
(0shot) text classification. 0shot models based on natural language inference
(NLI) and next sentence prediction (NSP) employ cross-encoder architecture and
infer by making a forward pass through the model for each label-text pair
separately. This increases the computational cost to make inferences linearly
in the number of labels. In this work, we improve the efficiency of such
cross-encoder-based 0shot models by restricting the number of likely labels
using another fast base classifier-based conformal predictor (CP) calibrated on
samples labeled by the 0shot model. Since a CP generates prediction sets with
coverage guarantees, it reduces the number of target labels without excluding
the most probable label based on the 0shot model. We experiment with three
intent and two topic classification datasets. With a suitable CP for each
dataset, we reduce the average inference time for NLI- and NSP-based models by
25.6% and 22.2% respectively, without dropping performance below the predefined
error rate of 1%.",http://arxiv.org/pdf/2210.12619v1,cs.CL
2022-10-22 17:01:05+00:00,Bayesian Optimization with Conformal Prediction Sets,"['Samuel Stanton', 'Wesley Maddox', 'Andrew Gordon Wilson']","Bayesian optimization is a coherent, ubiquitous approach to decision-making
under uncertainty, with applications including multi-arm bandits, active
learning, and black-box optimization. Bayesian optimization selects decisions
(i.e. objective function queries) with maximal expected utility with respect to
the posterior distribution of a Bayesian model, which quantifies reducible,
epistemic uncertainty about query outcomes. In practice, subjectively
implausible outcomes can occur regularly for two reasons: 1) model
misspecification and 2) covariate shift. Conformal prediction is an uncertainty
quantification method with coverage guarantees even for misspecified models and
a simple mechanism to correct for covariate shift. We propose conformal
Bayesian optimization, which directs queries towards regions of search space
where the model predictions have guaranteed validity, and investigate its
behavior on a suite of black-box optimization tasks and tabular ranking tasks.
In many cases we find that query coverage can be significantly improved without
harming sample-efficiency.",http://arxiv.org/pdf/2210.12496v3,cs.LG
2022-10-19 02:25:33+00:00,Safe Planning in Dynamic Environments using Conformal Prediction,"['Lars Lindemann', 'Matthew Cleaveland', 'Gihyun Shim', 'George J. Pappas']","We propose a framework for planning in unknown dynamic environments with
probabilistic safety guarantees using conformal prediction. Particularly, we
design a model predictive controller (MPC) that uses i) trajectory predictions
of the dynamic environment, and ii) prediction regions quantifying the
uncertainty of the predictions. To obtain prediction regions, we use conformal
prediction, a statistical tool for uncertainty quantification, that requires
availability of offline trajectory data - a reasonable assumption in many
applications such as autonomous driving. The prediction regions are valid,
i.e., they hold with a user-defined probability, so that the MPC is provably
safe. We illustrate the results in the self-driving car simulator CARLA at a
pedestrian-filled intersection. The strength of our approach is compatibility
with state of the art trajectory predictors, e.g., RNNs and LSTMs, while making
no assumptions on the underlying trajectory-generating distribution. To the
best of our knowledge, these are the first results that provide valid safety
guarantees in such a setting.",http://arxiv.org/pdf/2210.10254v2,cs.RO
2022-10-18 20:59:48+00:00,Nonparametric Quantile Regression: Non-Crossing Constraints and Conformal Prediction,"['Wenlu Tang', 'Guohao Shen', 'Yuanyuan Lin', 'Jian Huang']","We propose a nonparametric quantile regression method using deep neural
networks with a rectified linear unit penalty function to avoid quantile
crossing. This penalty function is computationally feasible for enforcing
non-crossing constraints in multi-dimensional nonparametric quantile
regression. We establish non-asymptotic upper bounds for the excess risk of the
proposed nonparametric quantile regression function estimators. Our error
bounds achieve optimal minimax rate of convergence for the Holder class, and
the prefactors of the error bounds depend polynomially on the dimension of the
predictor, instead of exponentially. Based on the proposed non-crossing
penalized deep quantile regression, we construct conformal prediction intervals
that are fully adaptive to heterogeneity. The proposed prediction interval is
shown to have good properties in terms of validity and accuracy under
reasonable conditions. We also derive non-asymptotic upper bounds for the
difference of the lengths between the proposed non-crossing conformal
prediction interval and the theoretically oracle prediction interval. Numerical
experiments including simulation studies and a real data example are conducted
to demonstrate the effectiveness of the proposed method.",http://arxiv.org/pdf/2210.10161v1,stat.ML
2022-10-17 15:42:26+00:00,Sub-8-bit quantization for on-device speech recognition: a regularization-free approach,"['Kai Zhen', 'Martin Radfar', 'Hieu Duy Nguyen', 'Grant P. Strimel', 'Nathan Susanj', 'Athanasios Mouchtaris']","For on-device automatic speech recognition (ASR), quantization aware training
(QAT) is ubiquitous to achieve the trade-off between model predictive
performance and efficiency. Among existing QAT methods, one major drawback is
that the quantization centroids have to be predetermined and fixed. To overcome
this limitation, we introduce a regularization-free, ""soft-to-hard"" compression
mechanism with self-adjustable centroids in a mu-Law constrained space,
resulting in a simpler yet more versatile quantization scheme, called General
Quantizer (GQ). We apply GQ to ASR tasks using Recurrent Neural Network
Transducer (RNN-T) and Conformer architectures on both LibriSpeech and
de-identified far-field datasets. Without accuracy degradation, GQ can compress
both RNN-T and Conformer into sub-8-bit, and for some RNN-T layers, to 1-bit
for fast and accurate inference. We observe a 30.73% memory footprint saving
and 31.75% user-perceived latency reduction compared to 8-bit QAT via physical
device benchmarking.",http://arxiv.org/pdf/2210.09188v2,cs.SD
2022-10-17 03:56:02+00:00,Forecasting Human Trajectory from Scene History,"['Mancheng Meng', 'Ziyan Wu', 'Terrence Chen', 'Xiran Cai', 'Xiang Sean Zhou', 'Fan Yang', 'Dinggang Shen']","Predicting the future trajectory of a person remains a challenging problem,
due to randomness and subjectivity of human movement. However, the moving
patterns of human in a constrained scenario typically conform to a limited
number of regularities to a certain extent, because of the scenario
restrictions and person-person or person-object interactivity. Thus, an
individual person in this scenario should follow one of the regularities as
well. In other words, a person's subsequent trajectory has likely been traveled
by others. Based on this hypothesis, we propose to forecast a person's future
trajectory by learning from the implicit scene regularities. We call the
regularities, inherently derived from the past dynamics of the people and the
environment in the scene, scene history. We categorize scene history
information into two types: historical group trajectory and
individual-surroundings interaction. To exploit these two types of information
for trajectory prediction, we propose a novel framework Scene History
Excavating Network (SHENet), where the scene history is leveraged in a simple
yet effective approach. In particular, we design two components: the group
trajectory bank module to extract representative group trajectories as the
candidate for future path, and the cross-modal interaction module to model the
interaction between individual past trajectory and its surroundings for
trajectory refinement. In addition, to mitigate the uncertainty in ground-truth
trajectory, caused by the aforementioned randomness and subjectivity of human
movement, we propose to include smoothness into the training process and
evaluation metrics. We conduct extensive evaluations to validate the efficacy
of our proposed framework on ETH, UCY, as well as a new, challenging benchmark
dataset PAV, demonstrating superior performance compared to state-of-the-art
methods.",http://arxiv.org/pdf/2210.08732v1,cs.CV
2022-10-09 04:46:00+00:00,Test-time Recalibration of Conformal Predictors Under Distribution Shift Based on Unlabeled Examples,"['Fatih Furkan Yilmaz', 'Reinhard Heckel']","Modern image classifiers are very accurate, but the predictions come without
uncertainty estimates. Conformal predictors provide uncertainty estimates by
computing a set of classes containing the correct class with a user-specified
probability based on the classifier's probability estimates. To provide such
sets, conformal predictors often estimate a cutoff threshold for the
probability estimates based on a calibration set. Conformal predictors
guarantee reliability only when the calibration set is from the same
distribution as the test set. Therefore, conformal predictors need to be
recalibrated for new distributions. However, in practice, labeled data from new
distributions is rarely available, making calibration infeasible. In this work,
we consider the problem of predicting the cutoff threshold for a new
distribution based on unlabeled examples. While it is impossible in general to
guarantee reliability when calibrating based on unlabeled examples, we propose
a method that provides excellent uncertainty estimates under natural
distribution shifts, and provably works for a specific model of a distribution
shift.",http://arxiv.org/pdf/2210.04166v2,cs.LG
2022-10-08 04:41:43+00:00,Spatial predictions on physically constrained domains: Applications to Arctic sea salinity data,"['Bora Jin', 'Amy H. Herring', 'David Dunson']","In this paper, we predict sea surface salinity (SSS) in the Arctic Ocean
based on satellite measurements. SSS is a crucial indicator for ongoing changes
in the Arctic Ocean and can offer important insights about climate change. We
particularly focus on areas of water mistakenly flagged as ice by satellite
algorithms. To remove bias in the retrieval of salinity near sea ice, the
algorithms use conservative ice masks, which result in considerable loss of
data. We aim to produce realistic SSS values for such regions to obtain more
complete understanding about the SSS surface over the Arctic Ocean and benefit
future applications that may require SSS measurements near edges of sea ice or
coasts. We propose a class of scalable nonstationary processes that can handle
large data from satellite products and complex geometries of the Arctic Ocean.
Barrier Overlap-Removal Acyclic directed graph GP (BORA-GP) constructs sparse
directed acyclic graphs (DAGs) with neighbors conforming to barriers and
boundaries, enabling characterization of dependence in constrained domains. The
BORA-GP models produce more sensible SSS values in regions without satellite
measurements and show improved performance in various constrained domains in
simulation studies compared to state-of-the-art alternatives. An R package is
available on https://github.com/jinbora0720/boraGP.",http://arxiv.org/pdf/2210.03913v2,stat.AP
2022-10-07 23:16:03+00:00,Constructing Prediction Intervals with Neural Networks: An Empirical Evaluation of Bootstrapping and Conformal Inference Methods,"['Alex Contarino', 'Christine Schubert Kabban', 'Chancellor Johnstone', 'Fairul Mohd-Zaid']","Artificial neural networks (ANNs) are popular tools for accomplishing many
machine learning tasks, including predicting continuous outcomes. However, the
general lack of confidence measures provided with ANN predictions limit their
applicability. Supplementing point predictions with prediction intervals (PIs)
is common for other learning algorithms, but the complex structure and training
of ANNs renders constructing PIs difficult. This work provides the network
design choices and inferential methods for creating better performing PIs with
ANNs. A two-step experiment is executed across 11 data sets, including an
imaged-based data set. Two distribution-free methods for constructing PIs,
bootstrapping and conformal inference, are considered. The results of the first
experimental step reveal that the choices inherent to building an ANN affect PI
performance. Guidance is provided for optimizing PI performance with respect to
each network feature and PI method. In the second step, 20 algorithms for
constructing PIs, each using the principles of bootstrapping or conformal
inference, are implemented to determine which provides the best performance
while maintaining reasonable computational burden. In general, this trade-off
is optimized when implementing the cross-conformal method, which maintained
interval coverage and efficiency with decreased computational burden.",http://arxiv.org/pdf/2210.05354v1,stat.ML
2022-10-06 17:21:03+00:00,Few-Shot Calibration of Set Predictors via Meta-Learned Cross-Validation-Based Conformal Prediction,"['Sangwoo Park', 'Kfir M. Cohen', 'Osvaldo Simeone']","Conventional frequentist learning is known to yield poorly calibrated models
that fail to reliably quantify the uncertainty of their decisions. Bayesian
learning can improve calibration, but formal guarantees apply only under
restrictive assumptions about correct model specification. Conformal prediction
(CP) offers a general framework for the design of set predictors with
calibration guarantees that hold regardless of the underlying data generation
mechanism. However, when training data are limited, CP tends to produce large,
and hence uninformative, predicted sets. This paper introduces a novel
meta-learning solution that aims at reducing the set prediction size. Unlike
prior work, the proposed meta-learning scheme, referred to as meta-XB, (i)
builds on cross-validation-based CP, rather than the less efficient
validation-based CP; and (ii) preserves formal per-task calibration guarantees,
rather than less stringent task-marginal guarantees. Finally, meta-XB is
extended to adaptive non-conformal scores, which are shown empirically to
further enhance marginal per-input calibration.",http://arxiv.org/pdf/2210.03067v1,stat.ML
2022-10-05 13:54:56+00:00,Extending Conformal Prediction to Hidden Markov Models with Exact Validity via de Finetti's Theorem for Markov Chains,"['Buddhika Nettasinghe', 'Samrat Chatterjee', 'Ramakrishna Tipireddy', 'Mahantesh Halappanavar']","Conformal prediction is a widely used method to quantify the uncertainty of a
classifier under the assumption of exchangeability (e.g., IID data). We
generalize conformal prediction to the Hidden Markov Model (HMM) framework
where the assumption of exchangeability is not valid. The key idea of the
proposed method is to partition the non-exchangeable Markovian data from the
HMM into exchangeable blocks by exploiting the de Finetti's Theorem for Markov
Chains discovered by Diaconis and Freedman (1980). The permutations of the
exchangeable blocks are viewed as randomizations of the observed Markovian data
from the HMM. The proposed method provably retains all desirable theoretical
guarantees offered by the classical conformal prediction framework in both
exchangeable and Markovian settings. In particular, while the lack of
exchangeability introduced by Markovian samples constitutes a violation of a
crucial assumption for classical conformal prediction, the proposed method
views it as an advantage that can be exploited to improve the performance
further. Detailed numerical and empirical results that complement the
theoretical conclusions are provided to illustrate the practical feasibility of
the proposed method.",http://arxiv.org/pdf/2210.02271v4,stat.ME
2022-10-05 04:04:15+00:00,Conformalized Fairness via Quantile Regression,"['Meichen Liu', 'Lei Ding', 'Dengdeng Yu', 'Wulong Liu', 'Linglong Kong', 'Bei Jiang']","Algorithmic fairness has received increased attention in socially sensitive
domains. While rich literature on mean fairness has been established, research
on quantile fairness remains sparse but vital. To fulfill great needs and
advocate the significance of quantile fairness, we propose a novel framework to
learn a real-valued quantile function under the fairness requirement of
Demographic Parity with respect to sensitive attributes, such as race or
gender, and thereby derive a reliable fair prediction interval. Using optimal
transport and functional synchronization techniques, we establish theoretical
guarantees of distribution-free coverage and exact fairness for the induced
prediction interval constructed by fair quantiles. A hands-on pipeline is
provided to incorporate flexible quantile regressions with an efficient
fairness adjustment post-processing algorithm. We demonstrate the superior
empirical performance of this approach on several benchmark datasets. Our
results show the model's ability to uncover the mechanism underlying the
fairness-accuracy trade-off in a wide range of societal and medical
applications.",http://arxiv.org/pdf/2210.02015v2,stat.ML
2022-10-04 06:34:49+00:00,Selection by Prediction with Conformal p-values,"['Ying Jin', 'Emmanuel J. Candès']","Decision making or scientific discovery pipelines such as job hiring and drug
discovery often involve multiple stages: before any resource-intensive step,
there is often an initial screening that uses predictions from a machine
learning model to shortlist a few candidates from a large pool. We study
screening procedures that aim to select candidates whose unobserved outcomes
exceed user-specified values. We develop a method that wraps around any
prediction model to produce a subset of candidates while controlling the
proportion of falsely selected units. Building upon the conformal inference
framework, our method first constructs p-values that quantify the statistical
evidence for large outcomes; it then determines the shortlist by comparing the
p-values to a threshold introduced in the multiple testing literature. In many
cases, the procedure selects candidates whose predictions are above a
data-dependent threshold. Our theoretical guarantee holds under mild
exchangeability conditions on the samples, generalizing existing results on
multiple conformal p-values. We demonstrate the empirical performance of our
method via simulations, and apply it to job hiring and drug discovery datasets.",http://arxiv.org/pdf/2210.01408v3,stat.ME
2022-10-02 17:54:54+00:00,Uncertainty estimations methods for a deep learning model to aid in clinical decision-making -- a clinician's perspective,"['Michael Dohopolski', 'Kai Wang', 'Biling Wang', 'Ti Bai', 'Dan Nguyen', 'David Sher', 'Steve Jiang', 'Jing Wang']","Prediction uncertainty estimation has clinical significance as it can
potentially quantify prediction reliability. Clinicians may trust 'blackbox'
models more if robust reliability information is available, which may lead to
more models being adopted into clinical practice. There are several deep
learning-inspired uncertainty estimation techniques, but few are implemented on
medical datasets -- fewer on single institutional datasets/models. We sought to
compare dropout variational inference (DO), test-time augmentation (TTA),
conformal predictions, and single deterministic methods for estimating
uncertainty using our model trained to predict feeding tube placement for 271
head and neck cancer patients treated with radiation. We compared the area
under the curve (AUC), sensitivity, specificity, positive predictive value
(PPV), and negative predictive value (NPV) trends for each method at various
cutoffs that sought to stratify patients into 'certain' and 'uncertain'
cohorts. These cutoffs were obtained by calculating the percentile
""uncertainty"" within the validation cohort and applied to the testing cohort.
Broadly, the AUC, sensitivity, and NPV increased as the predictions were more
'certain' -- i.e., lower uncertainty estimates. However, when a majority vote
(implementing 2/3 criteria: DO, TTA, conformal predictions) or a stricter
approach (3/3 criteria) were used, AUC, sensitivity, and NPV improved without a
notable loss in specificity or PPV. Especially for smaller, single
institutional datasets, it may be important to evaluate multiple estimations
techniques before incorporating a model into clinical practice.",http://arxiv.org/pdf/2210.00589v1,cs.LG
2022-10-01 02:57:37+00:00,Predictive Inference with Feature Conformal Prediction,"['Jiaye Teng', 'Chuan Wen', 'Dinghuai Zhang', 'Yoshua Bengio', 'Yang Gao', 'Yang Yuan']","Conformal prediction is a distribution-free technique for establishing valid
prediction intervals. Although conventionally people conduct conformal
prediction in the output space, this is not the only possibility. In this
paper, we propose feature conformal prediction, which extends the scope of
conformal prediction to semantic feature spaces by leveraging the inductive
bias of deep representation learning. From a theoretical perspective, we
demonstrate that feature conformal prediction provably outperforms regular
conformal prediction under mild assumptions. Our approach could be combined
with not only vanilla conformal prediction, but also other adaptive conformal
prediction methods. Apart from experiments on existing predictive inference
benchmarks, we also demonstrate the state-of-the-art performance of the
proposed methods on large-scale tasks such as ImageNet classification and
Cityscapes image segmentation.The code is available at
\url{https://github.com/AlvinWen428/FeatureCP}.",http://arxiv.org/pdf/2210.00173v4,cs.LG
2022-09-30 00:18:27+00:00,Batch Multivalid Conformal Prediction,"['Christopher Jung', 'Georgy Noarov', 'Ramya Ramalingam', 'Aaron Roth']","We develop fast distribution-free conformal prediction algorithms for
obtaining multivalid coverage on exchangeable data in the batch setting.
Multivalid coverage guarantees are stronger than marginal coverage guarantees
in two ways: (1) They hold even conditional on group membership -- that is, the
target coverage level $1-\alpha$ holds conditionally on membership in each of
an arbitrary (potentially intersecting) group in a finite collection
$\mathcal{G}$ of regions in the feature space. (2) They hold even conditional
on the value of the threshold used to produce the prediction set on a given
example. In fact multivalid coverage guarantees hold even when conditioning on
group membership and threshold value simultaneously.
  We give two algorithms: both take as input an arbitrary non-conformity score
and an arbitrary collection of possibly intersecting groups $\mathcal{G}$, and
then can equip arbitrary black-box predictors with prediction sets. Our first
algorithm (BatchGCP) is a direct extension of quantile regression, needs to
solve only a single convex minimization problem, and produces an estimator
which has group-conditional guarantees for each group in $\mathcal{G}$. Our
second algorithm (BatchMVP) is iterative, and gives the full guarantees of
multivalid conformal prediction: prediction sets that are valid conditionally
both on group membership and non-conformity threshold. We evaluate the
performance of both of our algorithms in an extensive set of experiments. Code
to replicate all of our experiments can be found at
https://github.com/ProgBelarus/BatchMultivalidConformal",http://arxiv.org/pdf/2209.15145v1,cs.LG
2022-09-29 16:47:57+00:00,Graph Anomaly Detection with Graph Neural Networks: Current Status and Challenges,"['Hwan Kim', 'Byung Suk Lee', 'Won-Yong Shin', 'Sungsu Lim']","Graphs are used widely to model complex systems, and detecting anomalies in a
graph is an important task in the analysis of complex systems. Graph anomalies
are patterns in a graph that do not conform to normal patterns expected of the
attributes and/or structures of the graph. In recent years, graph neural
networks (GNNs) have been studied extensively and have successfully performed
difficult machine learning tasks in node classification, link prediction, and
graph classification thanks to the highly expressive capability via message
passing in effectively learning graph representations. To solve the graph
anomaly detection problem, GNN-based methods leverage information about the
graph attributes (or features) and/or structures to learn to score anomalies
appropriately. In this survey, we review the recent advances made in detecting
graph anomalies using GNN models. Specifically, we summarize GNN-based methods
according to the graph type (i.e., static and dynamic), the anomaly type (i.e.,
node, edge, subgraph, and whole graph), and the network architecture (e.g.,
graph autoencoder, graph convolutional network). To the best of our knowledge,
this survey is the first comprehensive review of graph anomaly detection
methods based on GNNs.",http://arxiv.org/pdf/2209.14930v2,cs.LG
2022-09-28 17:59:35+00:00,Conformal Prediction is Robust to Dispersive Label Noise,"['Shai Feldman', 'Bat-Sheva Einbinder', 'Stephen Bates', 'Anastasios N. Angelopoulos', 'Asaf Gendler', 'Yaniv Romano']","We study the robustness of conformal prediction, a powerful tool for
uncertainty quantification, to label noise. Our analysis tackles both
regression and classification problems, characterizing when and how it is
possible to construct uncertainty sets that correctly cover the unobserved
noiseless ground truth labels. We further extend our theory and formulate the
requirements for correctly controlling a general loss function, such as the
false negative proportion, with noisy labels. Our theory and experiments
suggest that conformal prediction and risk-controlling techniques with noisy
labels attain conservative risk over the clean ground truth labels except in
adversarial cases. In such cases, we can also correct for noise of bounded size
in the conformal prediction algorithm in order to ensure achieving the correct
risk of the ground truth labels without score or data regularity.",http://arxiv.org/pdf/2209.14295v2,cs.LG
2022-09-28 09:06:42+00:00,Explainable classification of astronomical uncertain time series,"['Michael Franklin Mbouopda', 'Emille E O Ishida', 'Engelbert Mephu Nguifo', 'Emmanuel Gangler']","Exploring the expansion history of the universe, understanding its
evolutionary stages, and predicting its future evolution are important goals in
astrophysics. Today, machine learning tools are used to help achieving these
goals by analyzing transient sources, which are modeled as uncertain time
series. Although black-box methods achieve appreciable performance, existing
interpretable time series methods failed to obtain acceptable performance for
this type of data. Furthermore, data uncertainty is rarely taken into account
in these methods. In this work, we propose an uncertaintyaware subsequence
based model which achieves a classification comparable to that of
state-of-the-art methods. Unlike conformal learning which estimates model
uncertainty on predictions, our method takes data uncertainty as additional
input. Moreover, our approach is explainable-by-design, giving domain experts
the ability to inspect the model and explain its predictions. The
explainability of the proposed method has also the potential to inspire new
developments in theoretical astrophysics modeling by suggesting important
subsequences which depict details of light curve shapes. The dataset, the
source code of our experiment, and the results are made available on a public
repository.",http://arxiv.org/pdf/2210.00869v1,cs.LG
2022-09-27 06:47:42+00:00,Direct Speech Translation for Automatic Subtitling,"['Sara Papi', 'Marco Gaido', 'Alina Karakanta', 'Mauro Cettolo', 'Matteo Negri', 'Marco Turchi']","Automatic subtitling is the task of automatically translating the speech of
audiovisual content into short pieces of timed text, i.e. subtitles and their
corresponding timestamps. The generated subtitles need to conform to space and
time requirements, while being synchronised with the speech and segmented in a
way that facilitates comprehension. Given its considerable complexity, the task
has so far been addressed through a pipeline of components that separately deal
with transcribing, translating, and segmenting text into subtitles, as well as
predicting timestamps. In this paper, we propose the first direct ST model for
automatic subtitling that generates subtitles in the target language along with
their timestamps with a single model. Our experiments on 7 language pairs show
that our approach outperforms a cascade system in the same data condition, also
being competitive with production tools on both in-domain and newly-released
out-domain benchmarks covering new scenarios.",http://arxiv.org/pdf/2209.13192v2,cs.CL
2022-09-22 19:23:37+00:00,Scalable Gaussian Process Hyperparameter Optimization via Coverage Regularization,"['Killian Wood', 'Alec M. Dunton', 'Amanda Muyskens', 'Benjamin W. Priest']","Gaussian processes (GPs) are Bayesian non-parametric models popular in a
variety of applications due to their accuracy and native uncertainty
quantification (UQ). Tuning GP hyperparameters is critical to ensure the
validity of prediction accuracy and uncertainty; uniquely estimating multiple
hyperparameters in, e.g. the Matern kernel can also be a significant challenge.
Moreover, training GPs on large-scale datasets is a highly active area of
research: traditional maximum likelihood hyperparameter training requires
quadratic memory to form the covariance matrix and has cubic training
complexity. To address the scalable hyperparameter tuning problem, we present a
novel algorithm which estimates the smoothness and length-scale parameters in
the Matern kernel in order to improve robustness of the resulting prediction
uncertainties. Using novel loss functions similar to those in conformal
prediction algorithms in the computational framework provided by the
hyperparameter estimation algorithm MuyGPs, we achieve improved UQ over
leave-one-out likelihood maximization while maintaining a high degree of
scalability as demonstrated in numerical experiments.",http://arxiv.org/pdf/2209.11280v2,cs.LG
2022-09-20 13:38:27+00:00,Process Mining Meets Visual Analytics: The Case of Conformance Checking,"['Jana-Rebecca Rehse', 'Luise Pufahl', 'Michael Grohs', 'Lisa-Marie Klein']","Conformance checking is a major function of process mining, which allows
organizations to identify and alleviate potential deviations from the intended
process behavior. To fully leverage its benefits, it is important that
conformance checking results are visualized in a way that is approachable and
understandable for non-expert users. However, the visualization of conformance
checking results has so far not been widely considered in research. Therefore,
the goal of this paper is to develop an understanding of how conformance
checking results are visualized by process mining tools to provide a foundation
for further research on this topic. We conduct a systematic study, where we
analyze the visualization capabilities of nine academic and seven commercial
tools by means of a visual analytics framework. In this study, we find that the
''Why?'' aspect of conformance checking visualization seems already be
well-defined, but the ''What?'' and ''How?'' aspects require future research.",http://arxiv.org/pdf/2209.09712v1,cs.SE
2022-09-20 08:07:02+00:00,Declarative Guideline Conformance Checking of Clinical Treatments: A Case Study,"['Joscha Grüger', 'Tobias Geyer', 'Martin Kuhn', 'Stefan Braun', 'Ralph Bergmann']","Conformance checking is a process mining technique that allows verifying the
conformance of process instances to a given model. Thus, this technique is
predestined to be used in the medical context for the comparison of treatment
cases with clinical guidelines. However, medical processes are highly variable,
highly dynamic, and complex. This makes the use of imperative conformance
checking approaches in the medical domain difficult. Studies show that
declarative approaches can better address these characteristics. However, none
of the approaches has yet gained practical acceptance. Another challenge are
alignments, which usually do not add any value from a medical point of view.
For this reason, we investigate in a case study the usability of the HL7
standard Arden Syntax for declarative, rule-based conformance checking and the
use of manually modeled alignments. Using the approach, it was possible to
check the conformance of treatment cases and create medically meaningful
alignments for large parts of a medical guideline.",http://arxiv.org/pdf/2209.09535v1,cs.AI
2022-09-14 14:33:07+00:00,A Temporal Anomaly Detection System for Vehicles utilizing Functional Working Groups and Sensor Channels,"['Subash Neupane', 'Ivan A. Fernandez', 'Wilson Patterson', 'Sudip Mittal', 'Shahram Rahimi']","A modern vehicle fitted with sensors, actuators, and Electronic Control Units
(ECUs) can be divided into several operational subsystems called Functional
Working Groups (FWGs). Examples of these FWGs include the engine system,
transmission, fuel system, brakes, etc. Each FWG has associated sensor-channels
that gauge vehicular operating conditions. This data rich environment is
conducive to the development of Predictive Maintenance (PdM) technologies.
Undercutting various PdM technologies is the need for robust anomaly detection
models that can identify events or observations which deviate significantly
from the majority of the data and do not conform to a well defined notion of
normal vehicular operational behavior. In this paper, we introduce the Vehicle
Performance, Reliability, and Operations (VePRO) dataset and use it to create a
multi-phased approach to anomaly detection. Utilizing Temporal Convolution
Networks (TCN), our anomaly detection system can achieve 96% detection accuracy
and accurately predicts 91% of true anomalies. The performance of our anomaly
detection system improves when sensor channels from multiple FWGs are utilized.",http://arxiv.org/pdf/2209.06828v1,cs.LG
2022-09-08 06:08:48+00:00,Conformal Methods for Quantifying Uncertainty in Spatiotemporal Data: A Survey,['Sophia Sun'],"Machine learning methods are increasingly widely used in high-risk settings
such as healthcare, transportation, and finance. In these settings, it is
important that a model produces calibrated uncertainty to reflect its own
confidence and avoid failures. In this paper we survey recent works on
uncertainty quantification (UQ) for deep learning, in particular
distribution-free Conformal Prediction method for its mathematical properties
and wide applicability. We will cover the theoretical guarantees of conformal
methods, introduce techniques that improve calibration and efficiency for UQ in
the context of spatiotemporal data, and discuss the role of UQ in the context
of safe decision making.",http://arxiv.org/pdf/2209.03580v1,cs.AI
2022-09-07 15:40:23+00:00,Efficient Trajectory Planning and Control for USV with Vessel Dynamics and Differential Flatness,"['Tao Huang', 'Zhenfeng Xue', 'Zhe Chen', 'Yong Liu']","Unmanned surface vessels (USVs) are widely used in ocean exploration and
environmental protection fields. To ensure that USV can successfully perform
its mission, trajectory planning and motion tracking are the two most critical
technologies. In this paper, we propose a novel trajectory generation and
tracking method for USV based on optimization theory. Specifically, the USV
dynamic model is described with differential flatness, so that the trajectory
can be generated by dynamic RRT* in a linear invariant system expression form
under the objective of optimal boundary value. To reduce the sample number and
improve efficiency, we adjust the trajectory through local optimization. The
dynamic constraints are considered in the optimization process so that the
generated trajectory conforms to the kinematic characteristics of the
under-actuated hull, and makes it easier to be tracked. Finally, motion
tracking is added with model predictive control under a sequential quadratic
programming problem. Experimental results show the planned trajectory is more
in line with the kinematic characteristics of USV, and the tracking accuracy
remains a higher level.",http://arxiv.org/pdf/2209.03232v1,cs.RO
2022-09-07 03:12:02+00:00,Context Recovery and Knowledge Retrieval: A Novel Two-Stream Framework for Video Anomaly Detection,"['Congqi Cao', 'Yue Lu', 'Yanning Zhang']","Video anomaly detection aims to find the events in a video that do not
conform to the expected behavior. The prevalent methods mainly detect anomalies
by snippet reconstruction or future frame prediction error. However, the error
is highly dependent on the local context of the current snippet and lacks the
understanding of normality. To address this issue, we propose to detect
anomalous events not only by the local context, but also according to the
consistency between the testing event and the knowledge about normality from
the training data. Concretely, we propose a novel two-stream framework based on
context recovery and knowledge retrieval, where the two streams can complement
each other. For the context recovery stream, we propose a spatiotemporal U-Net
which can fully utilize the motion information to predict the future frame.
Furthermore, we propose a maximum local error mechanism to alleviate the
problem of large recovery errors caused by complex foreground objects. For the
knowledge retrieval stream, we propose an improved learnable locality-sensitive
hashing, which optimizes hash functions via a Siamese network and a mutual
difference loss. The knowledge about normality is encoded and stored in hash
tables, and the distance between the testing event and the knowledge
representation is used to reveal the probability of anomaly. Finally, we fuse
the anomaly scores from the two streams to detect anomalies. Extensive
experiments demonstrate the effectiveness and complementarity of the two
streams, whereby the proposed two-stream framework achieves state-of-the-art
performance on four datasets.",http://arxiv.org/pdf/2209.02899v1,cs.CV
2022-09-01 06:56:11+00:00,Deep Sparse Conformer for Speech Recognition,['Xianchao Wu'],"Conformer has achieved impressive results in Automatic Speech Recognition
(ASR) by leveraging transformer's capturing of content-based global
interactions and convolutional neural network's exploiting of local features.
In Conformer, two macaron-like feed-forward layers with half-step residual
connections sandwich the multi-head self-attention and convolution modules
followed by a post layer normalization. We improve Conformer's long-sequence
representation ability in two directions, \emph{sparser} and \emph{deeper}. We
adapt a sparse self-attention mechanism with $\mathcal{O}(L\text{log}L)$ in
time complexity and memory usage. A deep normalization strategy is utilized
when performing residual connections to ensure our training of hundred-level
Conformer blocks. On the Japanese CSJ-500h dataset, this deep sparse Conformer
achieves respectively CERs of 5.52\%, 4.03\% and 4.50\% on the three evaluation
sets and 4.16\%, 2.84\% and 3.20\% when ensembling five deep sparse Conformer
variants from 12 to 16, 17, 50, and finally 100 encoder layers.",http://arxiv.org/pdf/2209.00260v1,cs.CL
2022-08-23 17:52:20+00:00,Integrative conformal p-values for powerful out-of-distribution testing with labeled outliers,"['Ziyi Liang', 'Matteo Sesia', 'Wenguang Sun']","This paper develops novel conformal methods to test whether a new observation
was sampled from the same distribution as a reference set. Blending inductive
and transductive conformal inference in an innovative way, the described
methods can re-weight standard conformal p-values based on dependent side
information from known out-of-distribution data in a principled way, and can
automatically take advantage of the most powerful model from any collection of
one-class and binary classifiers. The solution can be implemented either
through sample splitting or via a novel transductive cross-validation+ scheme
which may also be useful in other applications of conformal inference, due to
tighter guarantees compared to existing cross-validation approaches. After
studying false discovery rate control and power within a multiple testing
framework with several possible outliers, the proposed solution is shown to
outperform standard conformal p-values through simulations as well as
applications to image recognition and tabular data.",http://arxiv.org/pdf/2208.11111v1,stat.ME
2022-08-20 10:23:17+00:00,Unsupervisedly Prompting AlphaFold2 for Few-Shot Learning of Accurate Folding Landscape and Protein Structure Prediction,"['Jun Zhang', 'Sirui Liu', 'Mengyun Chen', 'Haotian Chu', 'Min Wang', 'Zidong Wang', 'Jialiang Yu', 'Ningxi Ni', 'Fan Yu', 'Diqing Chen', 'Yi Isaac Yang', 'Boxin Xue', 'Lijiang Yang', 'Yuan Liu', 'Yi Qin Gao']","Data-driven predictive methods which can efficiently and accurately transform
protein sequences into biologically active structures are highly valuable for
scientific research and medical development. Determining accurate folding
landscape using co-evolutionary information is fundamental to the success of
modern protein structure prediction methods. As the state of the art,
AlphaFold2 has dramatically raised the accuracy without performing explicit
co-evolutionary analysis. Nevertheless, its performance still shows strong
dependence on available sequence homologs. Based on the interrogation on the
cause of such dependence, we presented EvoGen, a meta generative model, to
remedy the underperformance of AlphaFold2 for poor MSA targets. By prompting
the model with calibrated or virtually generated homologue sequences, EvoGen
helps AlphaFold2 fold accurately in low-data regime and even achieve
encouraging performance with single-sequence predictions. Being able to make
accurate predictions with few-shot MSA not only generalizes AlphaFold2 better
for orphan sequences, but also democratizes its use for high-throughput
applications. Besides, EvoGen combined with AlphaFold2 yields a probabilistic
structure generation method which could explore alternative conformations of
protein sequences, and the task-aware differentiable algorithm for sequence
generation will benefit other related tasks including protein design.",http://arxiv.org/pdf/2208.09652v2,cs.LG
2022-08-18 02:29:17+00:00,Using Conformal Win Probability to Predict the Winners of the Cancelled 2020 NCAA Basketball Tournaments,"['Chancellor Johnstone', 'Dan Nettleton']","The COVID-19 pandemic was responsible for the cancellation of both the men's
and women's 2020 National Collegiate Athletic Association (NCAA) Division 1
basketball tournaments. Starting from the point at which the Division 1
tournaments and any unfinished conference tournaments were cancelled, we
deliver closed-form probabilities for each team of making the Division 1
tournaments, had they not been cancelled, aided by use of conformal predictive
distributions. We also deliver probabilities of a team winning March Madness,
given a tournament bracket. We then compare single-game win probabilities
generated with conformal predictive distributions, aptly named conformal win
probabilities, to those generated through linear and logistic regression on
seven years of historical college basketball data, specifically from the
2014-2015 season through the 2020-2021 season. Conformal win probabilities are
shown to be better calibrated than other methods, resulting in more accurate
win probability estimates, while requiring fewer distributional assumptions.",http://arxiv.org/pdf/2208.08598v1,stat.AP
2022-08-17 20:00:54+00:00,Analyzing Robustness of End-to-End Neural Models for Automatic Speech Recognition,"['Goutham Rajendran', 'Wei Zou']","We investigate robustness properties of pre-trained neural models for
automatic speech recognition. Real life data in machine learning is usually
very noisy and almost never clean, which can be attributed to various factors
depending on the domain, e.g. outliers, random noise and adversarial noise.
Therefore, the models we develop for various tasks should be robust to such
kinds of noisy data, which led to the thriving field of robust machine
learning. We consider this important issue in the setting of automatic speech
recognition. With the increasing popularity of pre-trained models, it's an
important question to analyze and understand the robustness of such models to
noise. In this work, we perform a robustness analysis of the pre-trained neural
models wav2vec2, HuBERT and DistilHuBERT on the LibriSpeech and TIMIT datasets.
We use different kinds of noising mechanisms and measure the model performances
as quantified by the inference time and the standard Word Error Rate metric. We
also do an in-depth layer-wise analysis of the wav2vec2 model when injecting
noise in between layers, enabling us to predict at a high level what each layer
learns. Finally for this model, we visualize the propagation of errors across
the layers and compare how it behaves on clean versus noisy data. Our
experiments conform the predictions of Pasad et al. [2021] and also raise
interesting directions for future work.",http://arxiv.org/pdf/2208.08509v1,cs.CL
2022-08-17 16:51:12+00:00,Conformal Inference for Online Prediction with Arbitrary Distribution Shifts,"['Isaac Gibbs', 'Emmanuel Candès']","We consider the problem of forming prediction sets in an online setting where
the distribution generating the data is allowed to vary over time. Previous
approaches to this problem suffer from over-weighting historical data and thus
may fail to quickly react to the underlying dynamics. Here we correct this
issue and develop a novel procedure with provably small regret over all local
time intervals of a given width. We achieve this by modifying the adaptive
conformal inference (ACI) algorithm of Gibbs and Cand\`{e}s (2021) to contain
an additional step in which the step-size parameter of ACI's gradient descent
update is tuned over time. Crucially, this means that unlike ACI, which
requires knowledge of the rate of change of the data-generating mechanism, our
new procedure is adaptive to both the size and type of the distribution shift.
Our methods are highly flexible and can be used in combination with any
baseline predictive algorithm that produces point estimates or estimated
quantiles of the target without the need for distributional assumptions. We
test our techniques on two real-world datasets aimed at predicting stock market
volatility and COVID-19 case counts and find that they are robust and adaptive
to real-world distribution shifts.",http://arxiv.org/pdf/2208.08401v3,stat.ME
2022-08-17 02:43:23+00:00,LayoutFormer++: Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction,"['Zhaoyun Jiang', 'Jiaqi Guo', 'Shizhao Sun', 'Huayu Deng', 'Zhongkai Wu', 'Vuksan Mijovic', 'Zijiang James Yang', 'Jian-Guang Lou', 'Dongmei Zhang']","Conditional graphic layout generation, which generates realistic layouts
according to user constraints, is a challenging task that has not been
well-studied yet. First, there is limited discussion about how to handle
diverse user constraints flexibly and uniformly. Second, to make the layouts
conform to user constraints, existing work often sacrifices generation quality
significantly. In this work, we propose LayoutFormer++ to tackle the above
problems. First, to flexibly handle diverse constraints, we propose a
constraint serialization scheme, which represents different user constraints as
sequences of tokens with a predefined format. Then, we formulate conditional
layout generation as a sequence-to-sequence transformation, and leverage
encoder-decoder framework with Transformer as the basic architecture.
Furthermore, to make the layout better meet user requirements without harming
quality, we propose a decoding space restriction strategy. Specifically, we
prune the predicted distribution by ignoring the options that definitely
violate user constraints and likely result in low-quality layouts, and make the
model samples from the restricted distribution. Experiments demonstrate that
LayoutFormer++ outperforms existing approaches on all the tasks in terms of
both better generation quality and less constraint violation.",http://arxiv.org/pdf/2208.08037v2,cs.CV
2022-08-15 05:47:51+00:00,Automatic Landmark Detection and Registration of Brain Cortical Surfaces via Quasi-Conformal Geometry and Convolutional Neural Networks,"['Yuchen Guo', 'Qiguang Chen', 'Gary P. T. Choi', 'Lok Ming Lui']","In medical imaging, surface registration is extensively used for performing
systematic comparisons between anatomical structures, with a prime example
being the highly convoluted brain cortical surfaces. To obtain a meaningful
registration, a common approach is to identify prominent features on the
surfaces and establish a low-distortion mapping between them with the feature
correspondence encoded as landmark constraints. Prior registration works have
primarily focused on using manually labeled landmarks and solving highly
nonlinear optimization problems, which are time-consuming and hence hinder
practical applications. In this work, we propose a novel framework for the
automatic landmark detection and registration of brain cortical surfaces using
quasi-conformal geometry and convolutional neural networks. We first develop a
landmark detection network (LD-Net) that allows for the automatic extraction of
landmark curves given two prescribed starting and ending points based on the
surface geometry. We then utilize the detected landmarks and quasi-conformal
theory for achieving the surface registration. Specifically, we develop a
coefficient prediction network (CP-Net) for predicting the Beltrami
coefficients associated with the desired landmark-based registration and a
mapping network called the disk Beltrami solver network (DBS-Net) for
generating quasi-conformal mappings from the predicted Beltrami coefficients,
with the bijectivity guaranteed by quasi-conformal theory. Experimental results
are presented to demonstrate the effectiveness of our proposed framework.
Altogether, our work paves a new way for surface-based morphometry and medical
shape analysis.",http://arxiv.org/pdf/2208.07010v1,cs.CV
2022-08-04 17:59:44+00:00,Conformal Risk Control,"['Anastasios N. Angelopoulos', 'Stephen Bates', 'Adam Fisch', 'Lihua Lei', 'Tal Schuster']","We extend conformal prediction to control the expected value of any monotone
loss function. The algorithm generalizes split conformal prediction together
with its coverage guarantee. Like conformal prediction, the conformal risk
control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also
introduce extensions of the idea to distribution shift, quantile risk control,
multiple and adversarial risk control, and expectations of U-statistics. Worked
examples from computer vision and natural language processing demonstrate the
usage of our algorithm to bound the false negative rate, graph distance, and
token-level F1-score.",http://arxiv.org/pdf/2208.02814v3,stat.ME
2022-07-30 18:29:11+00:00,Simplex Clustering via sBeta with Applications to Online Adjustment of Black-Box Predictions,"['Florent Chiaroni', 'Malik Boudiaf', 'Amar Mitiche', 'Ismail Ben Ayed']","We explore clustering the softmax predictions of deep neural networks and
introduce a novel probabilistic clustering method, referred to as k-sBetas. In
the general context of clustering discrete distributions, the existing methods
focused on exploring distortion measures tailored to simplex data, such as the
KL divergence, as alternatives to the standard Euclidean distance. We provide a
general maximum a posteriori (MAP) perspective of clustering distributions,
which emphasizes that the statistical models underlying the existing
distortion-based methods may not be descriptive enough. Instead, we optimize a
mixed-variable objective measuring the conformity of data within each cluster
to the introduced sBeta density function, whose parameters are constrained and
estimated jointly with binary assignment variables. Our versatile formulation
approximates a variety of parametric densities for modeling simplex data, and
enables to control the cluster-balance bias. This yields highly competitive
performances for unsupervised adjustments of black-box model predictions in a
variety of scenarios. Our code and comparisons with the existing
simplex-clustering approaches along with our introduced softmax-prediction
benchmarks are publicly available:
https://github.com/fchiaroni/Clustering_Softmax_Predictions.",http://arxiv.org/pdf/2208.00287v3,cs.CV
2022-07-28 16:40:26+00:00,A general framework for multi-step ahead adaptive conformal heteroscedastic time series forecasting,"['Martim Sousa', 'Ana Maria Tomé', 'José Moreira']","This paper introduces a novel model-agnostic algorithm called adaptive
ensemble batch multi-input multi-output conformalized quantile regression
(AEnbMIMOCQR} that enables forecasters to generate multi-step ahead prediction
intervals for a fixed pre-specified miscoverage rate in a distribution-free
manner. Our method is grounded on conformal prediction principles, however, it
does not require data splitting and provides close to exact coverage even when
the data is not exchangeable. Moreover, the resulting prediction intervals,
besides being empirically valid along the forecast horizon, do not neglect
heteroscedasticity. AEnbMIMOCQR is designed to be robust to distribution
shifts, which means that its prediction intervals remain reliable over an
unlimited period of time, without entailing retraining or imposing unrealistic
strict assumptions on the data-generating process. Through methodically
experimentation, we demonstrate that our approach outperforms other competitive
methods on both real-world and synthetic datasets. The code used in the
experimental part and a tutorial on how to use AEnbMIMOCQR can be found at the
following GitHub repository: https://github.com/Quilograma/AEnbMIMOCQR.",http://arxiv.org/pdf/2207.14219v9,stat.ML
2022-07-27 17:23:14+00:00,Conformal Prediction Bands for Two-Dimensional Functional Time Series,"['Niccolò Ajroldi', 'Jacopo Diquigiovanni', 'Matteo Fontana', 'Simone Vantini']","Time evolving surfaces can be modeled as two-dimensional Functional time
series, exploiting the tools of Functional data analysis. Leveraging this
approach, a forecasting framework for such complex data is developed. The main
focus revolves around Conformal Prediction, a versatile nonparametric paradigm
used to quantify uncertainty in prediction problems. Building upon recent
variations of Conformal Prediction for Functional time series, a probabilistic
forecasting scheme for two-dimensional functional time series is presented,
while providing an extension of Functional Autoregressive Processes of order
one to this setting. Estimation techniques for the latter process are
introduced and their performance are compared in terms of the resulting
prediction regions. Finally, the proposed forecasting procedure and the
uncertainty quantification technique are applied to a real dataset, collecting
daily observations of Sea Level Anomalies of the Black Sea",http://arxiv.org/pdf/2207.13656v2,stat.ME
2022-07-27 02:26:55+00:00,Spatio-Temporal Wildfire Prediction using Multi-Modal Data,"['Chen Xu', 'Yao Xie', 'Daniel A. Zuniga Vazquez', 'Rui Yao', 'Feng Qiu']","Due to severe societal and environmental impacts, wildfire prediction using
multi-modal sensing data has become a highly sought-after data-analytical tool
by various stakeholders (such as state governments and power utility companies)
to achieve a more informed understanding of wildfire activities and plan
preventive measures. A desirable algorithm should precisely predict fire risk
and magnitude for a location in real time. In this paper, we develop a flexible
spatio-temporal wildfire prediction framework using multi-modal time series
data. We first predict the wildfire risk (the chance of a wildfire event) in
real-time, considering the historical events using discrete mutually exciting
point process models. Then we further develop a wildfire magnitude prediction
set method based on the flexible distribution-free time-series conformal
prediction (CP) approach. Theoretically, we prove a risk model parameter
recovery guarantee, as well as coverage and set size guarantees for the CP
sets. Through extensive real-data experiments with wildfire data in California,
we demonstrate the effectiveness of our methods, as well as their flexibility
and scalability in large regions.",http://arxiv.org/pdf/2207.13250v5,stat.AP
2022-07-26 15:40:16+00:00,Efficient One Pass Self-distillation with Zipf's Label Smoothing,"['Jiajun Liang', 'Linze Li', 'Zhaodong Bing', 'Borui Zhao', 'Yao Tang', 'Bo Lin', 'Haoqiang Fan']","Self-distillation exploits non-uniform soft supervision from itself during
training and improves performance without any runtime cost. However, the
overhead during training is often overlooked, and yet reducing time and memory
overhead during training is increasingly important in the giant models' era.
This paper proposes an efficient self-distillation method named Zipf's Label
Smoothing (Zipf's LS), which uses the on-the-fly prediction of a network to
generate soft supervision that conforms to Zipf distribution without using any
contrastive samples or auxiliary parameters. Our idea comes from an empirical
observation that when the network is duly trained the output values of a
network's final softmax layer, after sorting by the magnitude and averaged
across samples, should follow a distribution reminiscent to Zipf's Law in the
word frequency statistics of natural languages. By enforcing this property on
the sample level and throughout the whole training period, we find that the
prediction accuracy can be greatly improved. Using ResNet50 on the INAT21
fine-grained classification dataset, our technique achieves +3.61% accuracy
gain compared to the vanilla baseline, and 0.88% more gain against the previous
label smoothing or self-distillation strategies. The implementation is publicly
available at https://github.com/megvii-research/zipfls.",http://arxiv.org/pdf/2207.12980v1,cs.CV
2022-07-25 17:46:09+00:00,A novel Deep Learning approach for one-step Conformal Prediction approximation,"['Julia A. Meister', 'Khuong An Nguyen', 'Stelios Kapetanakis', 'Zhiyuan Luo']","Deep Learning predictions with measurable confidence are increasingly
desirable for real-world problems, especially in high-risk settings. The
Conformal Prediction (CP) framework is a versatile solution that guarantees a
maximum error rate given minimal constraints. In this paper, we propose a novel
conformal loss function that approximates the traditionally two-step CP
approach in a single step. By evaluating and penalising deviations from the
stringent expected CP output distribution, a Deep Learning model may learn the
direct relationship between the input data and the conformal p-values. We carry
out a comprehensive empirical evaluation to show our novel loss function's
competitiveness for seven binary and multi-class prediction tasks on five
benchmark datasets. On the same datasets, our approach achieves significant
training time reductions up to 86% compared to Aggregated Conformal Prediction
(ACP), while maintaining comparable approximate validity and predictive
efficiency.",http://arxiv.org/pdf/2207.12377v4,cs.LG
2022-07-25 15:44:19+00:00,MAPIE: an open-source library for distribution-free uncertainty quantification,"['Vianney Taquet', 'Vincent Blot', 'Thomas Morzadec', 'Louis Lacombe', 'Nicolas Brunel']","Estimating uncertainties associated with the predictions of Machine Learning
(ML) models is of crucial importance to assess their robustness and predictive
power. In this submission, we introduce MAPIE (Model Agnostic Prediction
Interval Estimator), an open-source Python library that quantifies the
uncertainties of ML models for single-output regression and multi-class
classification tasks. MAPIE implements conformal prediction methods, allowing
the user to easily compute uncertainties with strong theoretical guarantees on
the marginal coverages and with mild assumptions on the model or on the
underlying data distribution. MAPIE is hosted on scikit-learn-contrib and is
fully ""scikit-learn-compatible"". As such, it accepts any type of regressor or
classifier coming with a scikit-learn API. The library is available at:
https://github.com/scikit-learn-contrib/MAPIE/.",http://arxiv.org/pdf/2207.12274v1,stat.ML
2022-07-24 16:41:14+00:00,CODiT: Conformal Out-of-Distribution Detection in Time-Series Data,"['Ramneet Kaur', 'Kaustubh Sridhar', 'Sangdon Park', 'Susmit Jha', 'Anirban Roy', 'Oleg Sokolsky', 'Insup Lee']","Machine learning models are prone to making incorrect predictions on inputs
that are far from the training distribution. This hinders their deployment in
safety-critical applications such as autonomous vehicles and healthcare. The
detection of a shift from the training distribution of individual datapoints
has gained attention. A number of techniques have been proposed for such
out-of-distribution (OOD) detection. But in many applications, the inputs to a
machine learning model form a temporal sequence. Existing techniques for OOD
detection in time-series data either do not exploit temporal relationships in
the sequence or do not provide any guarantees on detection. We propose using
deviation from the in-distribution temporal equivariance as the non-conformity
measure in conformal anomaly detection framework for OOD detection in
time-series data.Computing independent predictions from multiple conformal
detectors based on the proposed measure and combining these predictions by
Fisher's method leads to the proposed detector CODiT with guarantees on false
detection in time-series data. We illustrate the efficacy of CODiT by achieving
state-of-the-art results on computer vision datasets in autonomous driving. We
also show that CODiT can be used for OOD detection in non-vision datasets by
performing experiments on the physiological GAIT sensory dataset. Code, data,
and trained models are available at
https://github.com/kaustubhsridhar/time-series-OOD.",http://arxiv.org/pdf/2207.11769v1,cs.LG
2022-07-22 10:41:02+00:00,POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion,"['Christian Joppi', 'Geri Skenderi', 'Marco Cristani']","We propose a data-centric pipeline able to generate exogenous observation
data for the New Fashion Product Performance Forecasting (NFPPF) problem, i.e.,
predicting the performance of a brand-new clothing probe with no available past
observations. Our pipeline manufactures the missing past starting from a
single, available image of the clothing probe. It starts by expanding textual
tags associated with the image, querying related fashionable or unfashionable
images uploaded on the web at a specific time in the past. A binary classifier
is robustly trained on these web images by confident learning, to learn what
was fashionable in the past and how much the probe image conforms to this
notion of fashionability. This compliance produces the POtential Performance
(POP) time series, indicating how performing the probe could have been if it
were available earlier. POP proves to be highly predictive for the probe's
future performance, ameliorating the sales forecasts of all state-of-the-art
models on the recent VISUELLE fast-fashion dataset. We also show that POP
reflects the ground-truth popularity of new styles (ensembles of clothing
items) on the Fashion Forward benchmark, demonstrating that our webly-learned
signal is a truthful expression of popularity, accessible by everyone and
generalizable to any time of analysis. Forecasting code, data and the POP time
series are available at:
https://github.com/HumaticsLAB/POP-Mining-POtential-Performance",http://arxiv.org/pdf/2207.11001v1,cs.CV
2022-07-14 11:36:56+00:00,Unified 2D and 3D Pre-Training of Molecular Representations,"['Jinhua Zhu', 'Yingce Xia', 'Lijun Wu', 'Shufang Xie', 'Tao Qin', 'Wengang Zhou', 'Houqiang Li', 'Tie-Yan Liu']","Molecular representation learning has attracted much attention recently. A
molecule can be viewed as a 2D graph with nodes/atoms connected by edges/bonds,
and can also be represented by a 3D conformation with 3-dimensional coordinates
of all atoms. We note that most previous work handles 2D and 3D information
separately, while jointly leveraging these two sources may foster a more
informative representation. In this work, we explore this appealing idea and
propose a new representation learning method based on a unified 2D and 3D
pre-training. Atom coordinates and interatomic distances are encoded and then
fused with atomic representations through graph neural networks. The model is
pre-trained on three tasks: reconstruction of masked atoms and coordinates, 3D
conformation generation conditioned on 2D graph, and 2D graph generation
conditioned on 3D conformation. We evaluate our method on 11 downstream
molecular property prediction tasks: 7 with 2D information only and 4 with both
2D and 3D information. Our method achieves state-of-the-art results on 10
tasks, and the average improvement on 2D-only tasks is 8.3%. Our method also
achieves significant improvement on two 3D conformation generation tasks.",http://arxiv.org/pdf/2207.08806v1,cs.LG
2022-07-12 19:25:21+00:00,Estimating Test Performance for AI Medical Devices under Distribution Shift with Conformal Prediction,"['Charles Lu', 'Syed Rakin Ahmed', 'Praveer Singh', 'Jayashree Kalpathy-Cramer']","Estimating the test performance of software AI-based medical devices under
distribution shifts is crucial for evaluating the safety, efficiency, and
usability prior to clinical deployment. Due to the nature of regulated medical
device software and the difficulty in acquiring large amounts of labeled
medical datasets, we consider the task of predicting the test accuracy of an
arbitrary black-box model on an unlabeled target domain without modification to
the original training process or any distributional assumptions of the original
source data (i.e. we treat the model as a ""black-box"" and only use the
predicted output responses). We propose a ""black-box"" test estimation technique
based on conformal prediction and evaluate it against other methods on three
medical imaging datasets (mammography, dermatology, and histopathology) under
several clinically relevant types of distribution shift (institution, hardware
scanner, atlas, hospital). We hope that by promoting practical and effective
estimation techniques for black-box models, manufacturers of medical devices
will develop more standardized and realistic evaluation procedures to improve
the robustness and trustworthiness of clinical AI tools.",http://arxiv.org/pdf/2207.05796v1,cs.LG
2022-07-07 05:55:47+00:00,Predicting Opinion Dynamics via Sociologically-Informed Neural Networks,"['Maya Okawa', 'Tomoharu Iwata']","Opinion formation and propagation are crucial phenomena in social networks
and have been extensively studied across several disciplines. Traditionally,
theoretical models of opinion dynamics have been proposed to describe the
interactions between individuals (i.e., social interaction) and their impact on
the evolution of collective opinions. Although these models can incorporate
sociological and psychological knowledge on the mechanisms of social
interaction, they demand extensive calibration with real data to make reliable
predictions, requiring much time and effort. Recently, the widespread use of
social media platforms provides new paradigms to learn deep learning models
from a large volume of social media data. However, these methods ignore any
scientific knowledge about the mechanism of social interaction. In this work,
we present the first hybrid method called Sociologically-Informed Neural
Network (SINN), which integrates theoretical models and social media data by
transporting the concepts of physics-informed neural networks (PINNs) from
natural science (i.e., physics) into social science (i.e., sociology and social
psychology). In particular, we recast theoretical models as ordinary
differential equations (ODEs). Then we train a neural network that
simultaneously approximates the data and conforms to the ODEs that represent
the social scientific knowledge. In addition, we extend PINNs by integrating
matrix factorization and a language model to incorporate rich side information
(e.g., user profiles) and structural knowledge (e.g., cluster structure of the
social interaction network). Moreover, we develop an end-to-end training
procedure for SINN, which involves Gumbel-Softmax approximation to include
stochastic mechanisms of social interaction. Extensive experiments on
real-world and synthetic datasets show SINN outperforms six baseline methods in
predicting opinion dynamics.",http://arxiv.org/pdf/2207.03990v1,cs.SI
2022-07-06 16:54:36+00:00,Improved conformalized quantile regression,"['Martim Sousa', 'Ana Maria Tomé', 'José Moreira']","Conformalized quantile regression is a procedure that inherits the advantages
of conformal prediction and quantile regression. That is, we use quantile
regression to estimate the true conditional quantile and then apply a conformal
step on a calibration set to ensure marginal coverage. In this way, we get
adaptive prediction intervals that account for heteroscedasticity. However, the
aforementioned conformal step lacks adaptiveness as described in (Romano et
al., 2019). To overcome this limitation, instead of applying a single conformal
step after estimating conditional quantiles with quantile regression, we
propose to cluster the explanatory variables weighted by their permutation
importance with an optimized k-means and apply k conformal steps. To show that
this improved version outperforms the classic version of conformalized quantile
regression and is more adaptive to heteroscedasticity, we extensively compare
the prediction intervals of both in open datasets.",http://arxiv.org/pdf/2207.02808v8,stat.ML
2022-07-05 18:01:20+00:00,Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets,"['Charles Lu', 'Anastasios N. Angelopoulos', 'Stuart Pomerantz']","The regulatory approval and broad clinical deployment of medical AI have been
hampered by the perception that deep learning models fail in unpredictable and
possibly catastrophic ways. A lack of statistically rigorous uncertainty
quantification is a significant factor undermining trust in AI results. Recent
developments in distribution-free uncertainty quantification present practical
solutions for these issues by providing reliability guarantees for black-box
models on arbitrary data distributions as formally valid finite-sample
prediction intervals. Our work applies these new uncertainty quantification
methods -- specifically conformal prediction -- to a deep-learning model for
grading the severity of spinal stenosis in lumbar spine MRI. We demonstrate a
technique for forming ordinal prediction sets that are guaranteed to contain
the correct stenosis severity within a user-defined probability (confidence
interval). On a dataset of 409 MRI exams processed by the deep-learning model,
the conformal method provides tight coverage with small prediction set sizes.
Furthermore, we explore the potential clinical applicability of flagging cases
with high uncertainty predictions (large prediction sets) by quantifying an
increase in the prevalence of significant imaging abnormalities (e.g. motion
artifacts, metallic artifacts, and tumors) that could degrade confidence in
predictive performance when compared to a random sample of cases.",http://arxiv.org/pdf/2207.02238v1,cs.LG
2022-07-05 08:53:08+00:00,"""Even if ..."" -- Diverse Semifactual Explanations of Reject","['André Artelt', 'Barbara Hammer']","Machine learning based decision making systems applied in safety critical
areas require reliable high certainty predictions. For this purpose, the system
can be extended by an reject option which allows the system to reject inputs
where only a prediction with an unacceptably low certainty would be possible.
While being able to reject uncertain samples is important, it is also of
importance to be able to explain why a particular sample was rejected. With the
ongoing rise of eXplainable AI (XAI), a lot of explanation methodologies for
machine learning based systems have been developed -- explaining reject
options, however, is still a novel field where only very little prior work
exists.
  In this work, we propose to explain rejects by semifactual explanations, an
instance of example-based explanation methods, which them self have not been
widely considered in the XAI community yet. We propose a conceptual modeling of
semifactual explanations for arbitrary reject options and empirically evaluate
a specific implementation on a conformal prediction based reject option.",http://arxiv.org/pdf/2207.01898v1,cs.LG
2022-07-04 17:53:21+00:00,A Framework for Auditing Multilevel Models using Explainability Methods,"['Debarati Bhaumik', 'Diptish Dey', 'Subhradeep Kayal']","Applications of multilevel models usually result in binary classification
within groups or hierarchies based on a set of input features. For transparent
and ethical applications of such models, sound audit frameworks need to be
developed. In this paper, an audit framework for technical assessment of
regression MLMs is proposed. The focus is on three aspects, model,
discrimination, and transparency and explainability. These aspects are
subsequently divided into sub aspects. Contributors, such as inter MLM group
fairness, feature contribution order, and aggregated feature contribution, are
identified for each of these sub aspects. To measure the performance of the
contributors, the framework proposes a shortlist of KPIs. A traffic light risk
assessment method is furthermore coupled to these KPIs. For assessing
transparency and explainability, different explainability methods (SHAP and
LIME) are used, which are compared with a model intrinsic method using
quantitative methods and machine learning modelling. Using an open source
dataset, a model is trained and tested and the KPIs are computed. It is
demonstrated that popular explainability methods, such as SHAP and LIME,
underperform in accuracy when interpreting these models. They fail to predict
the order of feature importance, the magnitudes, and occasionally even the
nature of the feature contribution. For other contributors, such as group
fairness and their associated KPIs, similar analysis and calculations have been
performed with the aim of adding profundity to the proposed audit framework.
The framework is expected to assist regulatory bodies in performing conformity
assessments of AI systems using multilevel binomial classification models at
businesses. It will also benefit businesses deploying MLMs to be future proof
and aligned with the European Commission proposed Regulation on Artificial
Intelligence.",http://arxiv.org/pdf/2207.01611v2,cs.CY
2022-06-29 14:08:22+00:00,conformalInference.multi and conformalInference.fd: Twin Packages for Conformal Prediction,"['Paolo Vergottini', 'Matteo Fontana', 'Jacopo Diquigiovanni', 'Aldo Solari', 'Simone Vantini']","Building on top of a regression model, Conformal Prediction methods produce
distribution free prediction sets, requiring only i.i.d. data. While R packages
implementing such methods for the univariate response framework have been
developed, this is not the case with multivariate and functional responses.
conformalInference.multi and conformalInference.fd address this void, by
extending classical and more advanced conformal prediction methods like full
conformal, split conformal, jackknife+ and multi split conformal to deal with
the multivariate and functional case. The extreme flexibility of conformal
prediction, fully embraced by the structure of the package, which does not
require any specific regression model, enables users to pass in any regression
function as input while using basic regression models as reference. Finally,
the issue of visualisation is addressed by providing embedded plotting
functions to visualize prediction regions.",http://arxiv.org/pdf/2206.14663v1,stat.ME
2022-06-27 07:53:38+00:00,Split Localized Conformal Prediction,"['Xing Han', 'Ziyang Tang', 'Joydeep Ghosh', 'Qiang Liu']","Conformal prediction is a simple and powerful tool that can quantify
uncertainty without any distributional assumptions. Many existing methods only
address the average coverage guarantee, which is not ideal compared to the
stronger conditional coverage guarantee. Existing methods of approximating
conditional coverage require additional models or time effort, which makes them
not easy to scale. In this paper, we propose a modified non-conformity score by
leveraging the local approximation of the conditional distribution using kernel
density estimation. The modified score inherits the spirit of split conformal
methods, which is simple and efficient and can scale to high dimensional
settings. We also proposed a unified framework that brings together our method
and several state-of-the-art. We perform extensive empirical evaluations:
results measured by both average and conditional coverage confirm the advantage
of our method.",http://arxiv.org/pdf/2206.13092v2,stat.ML
2022-06-25 01:54:53+00:00,Predicting Stock Price Movement after Disclosure of Corporate Annual Reports: A Case Study of 2021 China CSI 300 Stocks,"['Fengyu Han', 'Yue Wang']","In the current stock market, computer science and technology are more and
more widely used to analyse stocks. Not same as most related machine learning
stock price prediction work, this work study the predicting the tendency of the
stock price on the second day right after the disclosure of the companies'
annual reports. We use a variety of different models, including decision tree,
logistic regression, random forest, neural network, prototypical networks. We
use two sets of financial indicators (key and expanded) to conduct experiments,
these financial indicators are obtained from the EastMoney website disclosed by
companies, and finally we find that these models are not well behaved to
predict the tendency. In addition, we also filter stocks with ROE greater than
0.15 and net cash ratio greater than 0.9. We conclude that according to the
financial indicators based on the just-released annual report of the company,
the predictability of the stock price movement on the second day after
disclosure is weak, with maximum accuracy about 59.6% and maximum precision
about 0.56 on our test set by the random forest classifier, and the stock
filtering does not improve the performance. And random forests perform best in
general among all these models which conforms to some work's findings.",http://arxiv.org/pdf/2206.12528v2,q-fin.ST
2022-06-24 21:42:16+00:00,Multitask vocal burst modeling with ResNets and pre-trained paralinguistic Conformers,"['Josh Belanich', 'Krishna Somandepalli', 'Brian Eoff', 'Brendan Jou']","This technical report presents the modeling approaches used in our submission
to the ICML Expressive Vocalizations Workshop & Competition multitask track
(ExVo-MultiTask). We first applied image classification models of various sizes
on mel-spectrogram representations of the vocal bursts, as is standard in sound
event detection literature. Results from these models show an increase of
21.24% over the baseline system with respect to the harmonic mean of the task
metrics, and comprise our team's main submission to the MultiTask track. We
then sought to characterize the headroom in the MultiTask track by applying a
large pre-trained Conformer model that previously achieved state-of-the-art
results on paralinguistic tasks like speech emotion recognition and mask
detection. We additionally investigated the relationship between the sub-tasks
of emotional expression, country of origin, and age prediction, and discovered
that the best performing models are trained as single-task models, questioning
whether the problem truly benefits from a multitask setting.",http://arxiv.org/pdf/2206.12494v1,cs.SD
2022-06-23 16:35:43+00:00,Inductive Conformal Prediction: A Straightforward Introduction with Examples in Python,['Martim Sousa'],"Inductive Conformal Prediction (ICP) is a set of distribution-free and model
agnostic algorithms devised to predict with a user-defined confidence with
coverage guarantee. Instead of having point predictions, i.e., a real number in
the case of regression or a single class in multi class classification, models
calibrated using ICP output an interval or a set of classes, respectively. ICP
takes special importance in high-risk settings where we want the true output to
belong to the prediction set with high probability. As an example, a
classification model might output that given a magnetic resonance image a
patient has no latent diseases to report. However, this model output was based
on the most likely class, the second most likely class might tell that the
patient has a 15% chance of brain tumor or other severe disease and therefore
further exams should be conducted. Using ICP is therefore way more informative
and we believe that should be the standard way of producing forecasts. This
paper is a hands-on introduction, this means that we will provide examples as
we introduce the theory.",http://arxiv.org/pdf/2206.11810v4,stat.ML
2022-06-23 03:25:23+00:00,Modular Conformal Calibration,"['Charles Marx', 'Shengjia Zhao', 'Willie Neiswanger', 'Stefano Ermon']","Uncertainty estimates must be calibrated (i.e., accurate) and sharp (i.e.,
informative) in order to be useful. This has motivated a variety of methods for
recalibration, which use held-out data to turn an uncalibrated model into a
calibrated model. However, the applicability of existing methods is limited due
to their assumption that the original model is also a probabilistic model. We
introduce a versatile class of algorithms for recalibration in regression that
we call Modular Conformal Calibration (MCC). This framework allows one to
transform any regression model into a calibrated probabilistic model. The
modular design of MCC allows us to make simple adjustments to existing
algorithms that enable well-behaved distribution predictions. We also provide
finite-sample calibration guarantees for MCC algorithms. Our framework recovers
isotonic recalibration, conformal calibration, and conformal interval
prediction, implying that our theoretical results apply to those methods as
well. Finally, we conduct an empirical study of MCC on 17 regression datasets.
Our results show that new algorithms designed in our framework achieve
near-perfect calibration and improve sharpness relative to existing methods.",http://arxiv.org/pdf/2206.11468v2,cs.LG
2022-06-16 10:48:26+00:00,TransDrift: Modeling Word-Embedding Drift using Transformer,"['Nishtha Madaan', 'Prateek Chaudhury', 'Nishant Kumar', 'Srikanta Bedathur']","In modern NLP applications, word embeddings are a crucial backbone that can
be readily shared across a number of tasks. However as the text distributions
change and word semantics evolve over time, the downstream applications using
the embeddings can suffer if the word representations do not conform to the
data drift. Thus, maintaining word embeddings to be consistent with the
underlying data distribution is a key problem. In this work, we tackle this
problem and propose TransDrift, a transformer-based prediction model for word
embeddings. Leveraging the flexibility of transformer, our model accurately
learns the dynamics of the embedding drift and predicts the future embedding.
In experiments, we compare with existing methods and show that our model makes
significantly more accurate predictions of the word embedding than the
baselines. Crucially, by applying the predicted embeddings as a backbone for
downstream classification tasks, we show that our embeddings lead to superior
performance compared to the previous methods.",http://arxiv.org/pdf/2206.08081v1,cs.CL
2022-06-15 23:48:53+00:00,Conformal prediction set for time-series,"['Chen Xu', 'Yao Xie']","When building either prediction intervals for regression (with real-valued
response) or prediction sets for classification (with categorical responses),
uncertainty quantification is essential to studying complex machine learning
methods. In this paper, we develop Ensemble Regularized Adaptive Prediction Set
(ERAPS) to construct prediction sets for time-series (with categorical
responses), based on the prior work of [Xu and Xie, 2021]. In particular, we
allow unknown dependencies to exist within features and responses that arrive
in sequence. Method-wise, ERAPS is a distribution-free and ensemble-based
framework that is applicable for arbitrary classifiers. Theoretically, we bound
the coverage gap without assuming data exchangeability and show asymptotic set
convergence. Empirically, we demonstrate valid marginal and conditional
coverage by ERAPS, which also tends to yield smaller prediction sets than
competing methods.",http://arxiv.org/pdf/2206.07851v1,stat.ML
2022-06-14 09:31:18+00:00,Conformal Off-policy Prediction,"['Yingying Zhang', 'Chengchun Shi', 'Shikai Luo']","Off-policy evaluation is critical in a number of applications where new
policies need to be evaluated offline before online deployment. Most existing
methods focus on the expected return, define the target parameter through
averaging and provide a point estimator only. In this paper, we develop a novel
procedure to produce reliable interval estimators for a target policy's return
starting from any initial state. Our proposal accounts for the variability of
the return around its expectation, focuses on the individual effect and offers
valid uncertainty quantification. Our main idea lies in designing a pseudo
policy that generates subsamples as if they were sampled from the target policy
so that existing conformal prediction algorithms are applicable to prediction
interval construction. Our methods are justified by theories, synthetic data
and real data from short-video platforms.",http://arxiv.org/pdf/2206.06711v2,stat.ML
2022-06-14 03:58:03+00:00,Probabilistic Conformal Prediction Using Conditional Random Samples,"['Zhendong Wang', 'Ruijiang Gao', 'Mingzhang Yin', 'Mingyuan Zhou', 'David M. Blei']","This paper proposes probabilistic conformal prediction (PCP), a predictive
inference algorithm that estimates a target variable by a discontinuous
predictive set. Given inputs, PCP construct the predictive set based on random
samples from an estimated generative model. It is efficient and compatible with
either explicit or implicit conditional generative models. Theoretically, we
show that PCP guarantees correct marginal coverage with finite samples.
Empirically, we study PCP on a variety of simulated and real datasets. Compared
to existing methods for conformal inference, PCP provides sharper predictive
sets.",http://arxiv.org/pdf/2206.06584v2,stat.ML
2022-06-10 07:42:51+00:00,Fisher SAM: Information Geometry and Sharpness Aware Minimisation,"['Minyoung Kim', 'Da Li', 'Shell Xu Hu', 'Timothy M. Hospedales']","Recent sharpness-aware minimisation (SAM) is known to find flat minima which
is beneficial for better generalisation with improved robustness. SAM
essentially modifies the loss function by reporting the maximum loss value
within the small neighborhood around the current iterate. However, it uses the
Euclidean ball to define the neighborhood, which can be inaccurate since loss
functions for neural networks are typically defined over probability
distributions (e.g., class predictive probabilities), rendering the parameter
space non Euclidean. In this paper we consider the information geometry of the
model parameter space when defining the neighborhood, namely replacing SAM's
Euclidean balls with ellipsoids induced by the Fisher information. Our
approach, dubbed Fisher SAM, defines more accurate neighborhood structures that
conform to the intrinsic metric of the underlying statistical manifold. For
instance, SAM may probe the worst-case loss value at either a too nearby or
inappropriately distant point due to the ignorance of the parameter space
geometry, which is avoided by our Fisher SAM. Another recent Adaptive SAM
approach stretches/shrinks the Euclidean ball in accordance with the scale of
the parameter magnitudes. This might be dangerous, potentially destroying the
neighborhood structure. We demonstrate improved performance of the proposed
Fisher SAM on several benchmark datasets/tasks.",http://arxiv.org/pdf/2206.04920v1,cs.LG
2022-06-10 03:43:53+00:00,Conformal Prediction Intervals for Markov Decision Process Trajectories,"['Thomas G. Dietterich', 'Jesse Hostetler']","Before delegating a task to an autonomous system, a human operator may want a
guarantee about the behavior of the system. This paper extends previous work on
conformal prediction for functional data and conformalized quantile regression
to provide conformal prediction intervals over the future behavior of an
autonomous system executing a fixed control policy on a Markov Decision Process
(MDP). The prediction intervals are constructed by applying conformal
corrections to prediction intervals computed by quantile regression. The
resulting intervals guarantee that with probability $1-\delta$ the observed
trajectory will lie inside the prediction interval, where the probability is
computed with respect to the starting state distribution and the stochasticity
of the MDP. The method is illustrated on MDPs for invasive species management
and StarCraft2 battles.",http://arxiv.org/pdf/2206.04860v2,cs.LG
2022-06-09 10:39:33+00:00,Conformal Off-Policy Prediction in Contextual Bandits,"['Muhammad Faaiz Taufiq', 'Jean-Francois Ton', 'Rob Cornish', 'Yee Whye Teh', 'Arnaud Doucet']","Most off-policy evaluation methods for contextual bandits have focused on the
expected outcome of a policy, which is estimated via methods that at best
provide only asymptotic guarantees. However, in many applications, the
expectation may not be the best measure of performance as it does not capture
the variability of the outcome. In addition, particularly in safety-critical
settings, stronger guarantees than asymptotic correctness may be required. To
address these limitations, we consider a novel application of conformal
prediction to contextual bandits. Given data collected under a behavioral
policy, we propose \emph{conformal off-policy prediction} (COPP), which can
output reliable predictive intervals for the outcome under a new target policy.
We provide theoretical finite-sample guarantees without making any additional
assumptions beyond the standard contextual bandit setup, and empirically
demonstrate the utility of COPP compared with existing methods on synthetic and
real-world data.",http://arxiv.org/pdf/2206.04405v2,stat.ML
2022-06-02 14:33:00+00:00,Practical Adversarial Multivalid Conformal Prediction,"['Osbert Bastani', 'Varun Gupta', 'Christopher Jung', 'Georgy Noarov', 'Ramya Ramalingam', 'Aaron Roth']","We give a simple, generic conformal prediction method for sequential
prediction that achieves target empirical coverage guarantees against
adversarially chosen data. It is computationally lightweight -- comparable to
split conformal prediction -- but does not require having a held-out validation
set, and so all data can be used for training models from which to derive a
conformal score. It gives stronger than marginal coverage guarantees in two
ways. First, it gives threshold calibrated prediction sets that have correct
empirical coverage even conditional on the threshold used to form the
prediction set from the conformal score. Second, the user can specify an
arbitrary collection of subsets of the feature space -- possibly intersecting
-- and the coverage guarantees also hold conditional on membership in each of
these subsets. We call our algorithm MVP, short for MultiValid Prediction. We
give both theory and an extensive set of empirical evaluations.",http://arxiv.org/pdf/2206.01067v1,cs.LG
2022-05-30 16:53:16+00:00,Conformal Credal Self-Supervised Learning,"['Julian Lienen', 'Caglar Demir', 'Eyke Hüllermeier']","In semi-supervised learning, the paradigm of self-training refers to the idea
of learning from pseudo-labels suggested by the learner itself. Across various
domains, corresponding methods have proven effective and achieve
state-of-the-art performance. However, pseudo-labels typically stem from ad-hoc
heuristics, relying on the quality of the predictions though without
guaranteeing their validity. One such method, so-called credal self-supervised
learning, maintains pseudo-supervision in the form of sets of (instead of
single) probability distributions over labels, thereby allowing for a flexible
yet uncertainty-aware labeling. Again, however, there is no justification
beyond empirical effectiveness. To address this deficiency, we make use of
conformal prediction, an approach that comes with guarantees on the validity of
set-valued predictions. As a result, the construction of credal sets of labels
is supported by a rigorous theoretical foundation, leading to better calibrated
and less error-prone supervision for unlabeled data. Along with this, we
present effective algorithms for learning from credal self-supervision. An
empirical study demonstrates excellent calibration properties of the
pseudo-supervision, as well as the competitiveness of our method on several
benchmark datasets.",http://arxiv.org/pdf/2205.15239v2,stat.ML
2022-05-28 03:23:56+00:00,A Confidence Machine for Sparse High-Order Interaction Model,"['Diptesh Das', 'Eugene Ndiaye', 'Ichiro Takeuchi']","In predictive modeling for high-stake decision-making, predictors must be not
only accurate but also reliable. Conformal prediction (CP) is a promising
approach for obtaining the confidence of prediction results with fewer
theoretical assumptions. To obtain the confidence set by so-called full-CP, we
need to refit the predictor for all possible values of prediction results,
which is only possible for simple predictors. For complex predictors such as
random forests (RFs) or neural networks (NNs), split-CP is often employed where
the data is split into two parts: one part for fitting and another to compute
the confidence set. Unfortunately, because of the reduced sample size, split-CP
is inferior to full-CP both in fitting as well as confidence set computation.
In this paper, we develop a full-CP of sparse high-order interaction model
(SHIM), which is sufficiently flexible as it can take into account high-order
interactions among variables. We resolve the computational challenge for
full-CP of SHIM by introducing a novel approach called homotopy mining. Through
numerical experiments, we demonstrate that SHIM is as accurate as complex
predictors such as RF and NN and enjoys the superior statistical power of
full-CP.",http://arxiv.org/pdf/2205.14317v2,stat.ML
2022-05-28 02:59:05+00:00,Approximate Conditional Coverage & Calibration via Neural Model Approximations,"['Allen Schmaltz', 'Danielle Rasooly']","A typical desideratum for quantifying the uncertainty from a classification
model as a prediction set is class-conditional singleton set calibration. That
is, such sets should map to the output of well-calibrated selective
classifiers, matching the observed frequencies of similar instances. Recent
works proposing adaptive and localized conformal p-values for deep networks do
not guarantee this behavior, nor do they achieve it empirically. Instead, we
use the strong signals for prediction reliability from KNN-based approximations
of Transformer networks to construct data-driven partitions for Mondrian
Conformal Predictors, which are treated as weak selective classifiers that are
then calibrated via a new Inductive Venn Predictor, the Venn-ADMIT Predictor.
The resulting selective classifiers are well-calibrated, in a conservative but
practically useful sense for a given threshold. They are inherently robust to
changes in the proportions of the data partitions, and straightforward
conservative heuristics provide additional robustness to covariate shifts. We
compare and contrast to the quantities produced by recent Conformal Predictors
on several representative and challenging natural language processing
classification tasks, including class-imbalanced and distribution-shifted
settings.",http://arxiv.org/pdf/2205.14310v3,cs.LG
2022-05-25 17:45:04+00:00,Conformal Prediction Intervals with Temporal Dependence,"['Zhen Lin', 'Shubhendu Trivedi', 'Jimeng Sun']","Cross-sectional prediction is common in many domains such as healthcare,
including forecasting tasks using electronic health records, where different
patients form a cross-section. We focus on the task of constructing valid
prediction intervals (PIs) in time series regression with a cross-section. A
prediction interval is considered valid if it covers the true response with (a
pre-specified) high probability. We first distinguish between two notions of
validity in such a setting: cross-sectional and longitudinal. Cross-sectional
validity is concerned with validity across the cross-section of the time series
data, while longitudinal validity accounts for the temporal dimension. Coverage
guarantees along both these dimensions are ideally desirable; however, we show
that distribution-free longitudinal validity is theoretically impossible.
Despite this limitation, we propose Conformal Prediction with Temporal
Dependence (CPTD), a procedure that is able to maintain strict cross-sectional
validity while improving longitudinal coverage. CPTD is post-hoc and
light-weight, and can easily be used in conjunction with any prediction model
as long as a calibration set is available. We focus on neural networks due to
their ability to model complicated data such as diagnosis codes for time series
regression, and perform extensive experimental validation to verify the
efficacy of our approach. We find that CPTD outperforms baselines on a variety
of datasets by improving longitudinal coverage and often providing more
efficient (narrower) PIs.",http://arxiv.org/pdf/2205.12940v3,stat.ML
2022-05-23 14:51:23+00:00,Graph-Based Methods for Discrete Choice,"['Kiran Tomlinson', 'Austin R. Benson']","Choices made by individuals have widespread impacts--for instance, people
choose between political candidates to vote for, between social media posts to
share, and between brands to purchase--moreover, data on these choices are
increasingly abundant. Discrete choice models are a key tool for learning
individual preferences from such data. Additionally, social factors like
conformity and contagion influence individual choice. Existing methods for
incorporating these factors into choice models do not account for the entire
social network and require hand-crafted features. To overcome these
limitations, we use graph learning to study choice in networked contexts. We
identify three ways in which graph learning techniques can be used for discrete
choice: learning chooser representations, regularizing choice model parameters,
and directly constructing predictions from a network. We design methods in each
category and test them on real-world choice datasets, including county-level
2016 US election results and Android app installation and usage data. We show
that incorporating social network structure can improve the predictions of the
standard econometric choice model, the multinomial logit. We provide evidence
that app installations are influenced by social context, but we find no such
effect on app usage among the same participants, which instead is habit-driven.
In the election data, we highlight the additional insights a discrete choice
framework provides over classification or regression, the typical approaches.
On synthetic data, we demonstrate the sample complexity benefit of using social
information in choice models.",http://arxiv.org/pdf/2205.11365v1,cs.LG
2022-05-22 04:17:30+00:00,Robust Flow-based Conformal Inference (FCI) with Statistical Guarantee,"['Youhui Ye', 'Meimei Liu', 'Xin Xing']","Conformal prediction aims to determine precise levels of confidence in
predictions for new objects using past experience. However, the commonly used
exchangeable assumptions between the training data and testing data limit its
usage in dealing with contaminated testing sets. In this paper, we develop a
novel flow-based conformal inference (FCI) method to build predictive sets and
infer outliers for complex and high-dimensional data. We leverage ideas from
adversarial flow to transfer the input data to a random vector with known
distributions. Our roundtrip transformation can map the input data to a
low-dimensional space, meanwhile reserving the conditional distribution of
input data given each class label, which enables us to construct a
non-conformity score for uncertainty quantification. Our approach is applicable
and robust when the testing data is contaminated. We evaluate our method,
robust flow-based conformal inference, on benchmark datasets. We find that it
produces effective predictive sets and accurate outlier detection and is more
powerful relative to competing approaches.",http://arxiv.org/pdf/2205.10732v2,stat.ML
2022-05-21 13:09:01+00:00,Calibration of Natural Language Understanding Models with Venn--ABERS Predictors,['Patrizio Giovannotti'],"Transformers, currently the state-of-the-art in natural language
understanding (NLU) tasks, are prone to generate uncalibrated predictions or
extreme probabilities, making the process of taking different decisions based
on their output relatively difficult. In this paper we propose to build several
inductive Venn--ABERS predictors (IVAP), which are guaranteed to be well
calibrated under minimal assumptions, based on a selection of pre-trained
transformers. We test their performance over a set of diverse NLU tasks and
show that they are capable of producing well-calibrated probabilistic
predictions that are uniformly spread over the [0,1] interval -- all while
retaining the original model's predictive accuracy.",http://arxiv.org/pdf/2205.10586v2,cs.CL
2022-05-20 03:31:03+00:00,Conformal Prediction with Temporal Quantile Adjustments,"['Zhen Lin', 'Shubhendu Trivedi', 'Jimeng Sun']","We develop Temporal Quantile Adjustment (TQA), a general method to construct
efficient and valid prediction intervals (PIs) for regression on
cross-sectional time series data. Such data is common in many domains,
including econometrics and healthcare. A canonical example in healthcare is
predicting patient outcomes using physiological time-series data, where a
population of patients composes a cross-section. Reliable PI estimators in this
setting must address two distinct notions of coverage: cross-sectional coverage
across a cross-sectional slice, and longitudinal coverage along the temporal
dimension for each time series. Recent works have explored adapting Conformal
Prediction (CP) to obtain PIs in the time series context. However, none handles
both notions of coverage simultaneously. CP methods typically query a
pre-specified quantile from the distribution of nonconformity scores on a
calibration set. TQA adjusts the quantile to query in CP at each time $t$,
accounting for both cross-sectional and longitudinal coverage in a
theoretically-grounded manner. The post-hoc nature of TQA facilitates its use
as a general wrapper around any time series regression model. We validate TQA's
performance through extensive experimentation: TQA generally obtains efficient
PIs and improves longitudinal coverage while preserving cross-sectional
coverage.",http://arxiv.org/pdf/2205.09940v2,stat.ML
2022-05-19 18:21:12+00:00,Causal Discovery and Knowledge Injection for Contestable Neural Networks (with Appendices),"['Fabrizio Russo', 'Francesca Toni']","Neural networks have proven to be effective at solving machine learning tasks
but it is unclear whether they learn any relevant causal relationships, while
their black-box nature makes it difficult for modellers to understand and debug
them. We propose a novel method overcoming these issues by allowing a two-way
interaction whereby neural-network-empowered machines can expose the
underpinning learnt causal graphs and humans can contest the machines by
modifying the causal graphs before re-injecting them into the machines. The
learnt models are guaranteed to conform to the graphs and adhere to expert
knowledge, some of which can also be given up-front. By building a window into
the model behaviour and enabling knowledge injection, our method allows
practitioners to debug networks based on the causal structure discovered from
the data and underpinning the predictions. Experiments with real and synthetic
tabular data show that our method improves predictive performance up to 2.4x
while producing parsimonious networks, up to 7x smaller in the input layer,
compared to SOTA regularised networks.",http://arxiv.org/pdf/2205.09787v4,cs.LG
2022-05-18 17:41:37+00:00,Achieving Risk Control in Online Learning Settings,"['Shai Feldman', 'Liran Ringel', 'Stephen Bates', 'Yaniv Romano']","To provide rigorous uncertainty quantification for online learning models, we
develop a framework for constructing uncertainty sets that provably control
risk -- such as coverage of confidence intervals, false negative rate, or F1
score -- in the online setting. This extends conformal prediction to apply to a
larger class of online learning problems. Our method guarantees risk control at
any user-specified level even when the underlying data distribution shifts
drastically, even adversarially, over time in an unknown fashion. The technique
we propose is highly flexible as it can be applied with any base online
learning algorithm (e.g., a deep neural network trained online), requiring
minimal implementation effort and essentially zero additional computational
cost. We further extend our approach to control multiple risks simultaneously,
so the prediction sets we generate are valid for all given risks. To
demonstrate the utility of our method, we conduct experiments on real-world
tabular time-series data sets showing that the proposed method rigorously
controls various natural risks. Furthermore, we show how to construct valid
intervals for an online image-depth estimation problem that previous sequential
calibration schemes cannot handle.",http://arxiv.org/pdf/2205.09095v7,cs.LG
2022-05-12 05:08:10+00:00,Training Uncertainty-Aware Classifiers with Conformalized Deep Learning,"['Bat-Sheva Einbinder', 'Yaniv Romano', 'Matteo Sesia', 'Yanfei Zhou']","Deep neural networks are powerful tools to detect hidden patterns in data and
leverage them to make predictions, but they are not designed to understand
uncertainty and estimate reliable probabilities. In particular, they tend to be
overconfident. We begin to address this problem in the context of multi-class
classification by developing a novel training algorithm producing models with
more dependable uncertainty estimates, without sacrificing predictive power.
The idea is to mitigate overconfidence by minimizing a loss function, inspired
by advances in conformal inference, that quantifies model uncertainty by
carefully leveraging hold-out data. Experiments with synthetic and real data
demonstrate this method can lead to smaller conformal prediction sets with
higher conditional coverage, after exact calibration with hold-out data,
compared to state-of-the-art alternatives.",http://arxiv.org/pdf/2205.05878v2,stat.ML
2022-05-10 07:25:32+00:00,Knowledge Augmented Machine Learning with Applications in Autonomous Driving: A Survey,"['Julian Wörmann', 'Daniel Bogdoll', 'Etienne Bührle', 'Han Chen', 'Evaristus Fuh Chuo', 'Kostadin Cvejoski', 'Ludger van Elst', 'Philip Gottschall', 'Stefan Griesche', 'Christian Hellert', 'Christian Hesels', 'Sebastian Houben', 'Tim Joseph', 'Niklas Keil', 'Johann Kelsch', 'Hendrik Königshof', 'Erwin Kraft', 'Leonie Kreuser', 'Kevin Krone', 'Tobias Latka', 'Denny Mattern', 'Stefan Matthes', 'Mohsin Munir', 'Moritz Nekolla', 'Adrian Paschke', 'Maximilian Alexander Pintz', 'Tianming Qiu', 'Faraz Qureishi', 'Syed Tahseen Raza Rizvi', 'Jörg Reichardt', 'Laura von Rueden', 'Stefan Rudolph', 'Alexander Sagel', 'Tobias Scholl', 'Gerhard Schunk', 'Hao Shen', 'Hendrik Stapelbroek', 'Vera Stehr', 'Gurucharan Srinivas', 'Anh Tuan Tran', 'Abhishek Vivekanandan', 'Ya Wang', 'Florian Wasserrab', 'Tino Werner', 'Christian Wirth', 'Stefan Zwicklbauer']","The existence of representative datasets is a prerequisite of many successful
artificial intelligence and machine learning models. However, the subsequent
application of these models often involves scenarios that are inadequately
represented in the data used for training. The reasons for this are manifold
and range from time and cost constraints to ethical considerations. As a
consequence, the reliable use of these models, especially in safety-critical
applications, is a huge challenge. Leveraging additional, already existing
sources of knowledge is key to overcome the limitations of purely data-driven
approaches, and eventually to increase the generalization capability of these
models. Furthermore, predictions that conform with knowledge are crucial for
making trustworthy and safe decisions even in underrepresented scenarios. This
work provides an overview of existing techniques and methods in the literature
that combine data-based models with existing knowledge. The identified
approaches are structured according to the categories integration, extraction
and conformity. Special attention is given to applications in the field of
autonomous driving.",http://arxiv.org/pdf/2205.04712v2,cs.LG
2022-05-06 13:48:14+00:00,Goal-Oriented Next Best Activity Recommendation using Reinforcement Learning,"['Prerna Agarwal', 'Avani Gupta', 'Renuka Sindhgatta', 'Sampath Dechu']","Recommending a sequence of activities for an ongoing case requires that the
recommendations conform to the underlying business process and meet the
performance goal of either completion time or process outcome. Existing work on
next activity prediction can predict the future activity but cannot provide
guarantees of the prediction being conformant or meeting the goal. Hence, we
propose a goal-oriented next best activity recommendation. Our proposed
framework uses a deep learning model to predict the next best activity and an
estimated value of a goal given the activity. A reinforcement learning method
explores the sequence of activities based on the estimates likely to meet one
or more goals. We further address a real-world problem of multiple goals by
introducing an additional reward function to balance the outcome of a
recommended activity and satisfy the goal. We demonstrate the effectiveness of
the proposed method on four real-world datasets with different characteristics.
The results show that the recommendations from our proposed approach outperform
in goal satisfaction and conformance compared to the existing state-of-the-art
next best activity recommendation techniques.",http://arxiv.org/pdf/2205.03219v1,cs.AI
2022-05-03 10:53:40+00:00,On the Utility of Prediction Sets in Human-AI Teams,"['Varun Babbar', 'Umang Bhatt', 'Adrian Weller']","Research on human-AI teams usually provides experts with a single label,
which ignores the uncertainty in a model's recommendation. Conformal prediction
(CP) is a well established line of research that focuses on building a
theoretically grounded, calibrated prediction set, which may contain multiple
labels. We explore how such prediction sets impact expert decision-making in
human-AI teams. Our evaluation on human subjects finds that set valued
predictions positively impact experts. However, we notice that the predictive
sets provided by CP can be very large, which leads to unhelpful AI assistants.
To mitigate this, we introduce D-CP, a method to perform CP on some examples
and defer to experts. We prove that D-CP can reduce the prediction set size of
non-deferred examples. We show how D-CP performs in quantitative and in human
subject experiments ($n=120$). Our results suggest that CP prediction sets
improve human-AI team performance over showing the top-1 prediction alone, and
that experts find D-CP prediction sets are more useful than CP prediction sets.",http://arxiv.org/pdf/2205.01411v2,cs.AI
2022-04-22 15:13:12+00:00,E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR,"['W. Ronny Huang', 'Shuo-yiin Chang', 'David Rybach', 'Rohit Prabhavalkar', 'Tara N. Sainath', 'Cyril Allauzen', 'Cal Peyser', 'Zhiyun Lu']","Improving the performance of end-to-end ASR models on long utterances ranging
from minutes to hours in length is an ongoing challenge in speech recognition.
A common solution is to segment the audio in advance using a separate voice
activity detector (VAD) that decides segment boundary locations based purely on
acoustic speech/non-speech information. VAD segmenters, however, may be
sub-optimal for real-world speech where, e.g., a complete sentence that should
be taken as a whole may contain hesitations in the middle (""set an alarm for...
5 o'clock"").
  We propose to replace the VAD with an end-to-end ASR model capable of
predicting segment boundaries in a streaming fashion, allowing the segmentation
decision to be conditioned not only on better acoustic features but also on
semantic features from the decoded text with negligible extra computation. In
experiments on real world long-form audio (YouTube) with lengths of up to 30
minutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in
median end-of-segment latency compared to the VAD segmenter baseline on a
state-of-the-art Conformer RNN-T model.",http://arxiv.org/pdf/2204.10749v2,cs.SD
2022-04-19 06:08:08+00:00,Audio-Visual Wake Word Spotting System For MISP Challenge 2021,"['Yanguang Xu', 'Jianwei Sun', 'Yang Han', 'Shuaijiang Zhao', 'Chaoyang Mei', 'Tingwei Guo', 'Shuran Zhou', 'Chuandong Xie', 'Wei Zou', 'Xiangang Li', 'Shuran Zhou', 'Chuandong Xie', 'Wei Zou', 'Xiangang Li']","This paper presents the details of our system designed for the Task 1 of
Multimodal Information Based Speech Processing (MISP) Challenge 2021. The
purpose of Task 1 is to leverage both audio and video information to improve
the environmental robustness of far-field wake word spotting. In the proposed
system, firstly, we take advantage of speech enhancement algorithms such as
beamforming and weighted prediction error (WPE) to address the multi-microphone
conversational audio. Secondly, several data augmentation techniques are
applied to simulate a more realistic far-field scenario. For the video
information, the provided region of interest (ROI) is used to obtain visual
representation. Then the multi-layer CNN is proposed to learn audio and visual
representations, and these representations are fed into our two-branch
attention-based network which can be employed for fusion, such as transformer
and conformed. The focal loss is used to fine-tune the model and improve the
performance significantly. Finally, multiple trained models are integrated by
casting vote to achieve our final 0.091 score.",http://arxiv.org/pdf/2204.08686v2,cs.SD
2022-04-19 04:55:43+00:00,Pre-training of Equivariant Graph Matching Networks with Conformation Flexibility for Drug Binding,"['Fang Wu', 'Shuting Jin', 'Yinghui Jiang', 'Xurui Jin', 'Bowen Tang', 'Zhangming Niu', 'Xiangrong Liu', 'Qiang Zhang', 'Xiangxiang Zeng', 'Stan Z. Li']","The latest biological findings observe that the traditional motionless
'lock-and-key' theory is not generally applicable because the receptor and
ligand are constantly moving. Nonetheless, remarkable changes in associated
atomic sites and binding pose can provide vital information in understanding
the process of drug binding. Based on this mechanism, molecular dynamics (MD)
simulations were invented as a useful tool for investigating the dynamic
properties of a molecular system. However, the computational expenditure limits
the growth and application of protein trajectory-related studies, thus
hindering the possibility of supervised learning. To tackle this obstacle, we
present a novel spatial-temporal pre-training method based on the modified
Equivariant Graph Matching Networks (EGMN), dubbed ProtMD, which has two
specially designed self-supervised learning tasks: an atom-level prompt-based
denoising generative task and a conformation-level snapshot ordering task to
seize the flexibility information inside MD trajectories with very fine
temporal resolutions. The ProtMD can grant the encoder network the capacity to
capture the time-dependent geometric mobility of conformations along MD
trajectories. Two downstream tasks are chosen, i.e., the binding affinity
prediction and the ligand efficacy prediction, to verify the effectiveness of
ProtMD through linear detection and task-specific fine-tuning. We observe a
huge improvement from current state-of-the-art methods, with a decrease of 4.3%
in RMSE for the binding affinity problem and an average increase of 13.8% in
AUROC and AUPRC for the ligand efficacy problem. The results demonstrate
valuable insight into a strong correlation between the magnitude of
conformation's motion in the 3D space (i.e., flexibility) and the strength with
which the ligand binds with its receptor.",http://arxiv.org/pdf/2204.08663v5,cs.CE
2022-04-18 01:38:17+00:00,Optimal Conformal Prediction for Small Areas,"['Elizabeth Bersson', 'Peter D. Hoff']","Existing inferential methods for small area data involve a trade-off between
maintaining area-level frequentist coverage rates and improving inferential
precision via the incorporation of indirect information. In this article, we
propose a method to obtain an area-level prediction region for a future
observation which mitigates this trade-off. The proposed method takes a
conformal prediction approach in which the conformity measure is the posterior
predictive density of a working model that incorporates indirect information.
The resulting prediction region has guaranteed frequentist coverage regardless
of the working model, and, if the working model assumptions are accurate, the
region has minimum expected volume compared to other regions with the same
coverage rate. When constructed under a normal working model, we prove such a
prediction region is an interval and construct an efficient algorithm to obtain
the exact interval. We illustrate the performance of our method through
simulation studies and an application to EPA radon survey data.",http://arxiv.org/pdf/2204.08122v1,stat.ME
2022-04-15 14:33:42+00:00,Towards PAC Multi-Object Detection and Tracking,"['Shuo Li', 'Sangdon Park', 'Xiayan Ji', 'Insup Lee', 'Osbert Bastani']","Accurately detecting and tracking multi-objects is important for
safety-critical applications such as autonomous navigation. However, it remains
challenging to provide guarantees on the performance of state-of-the-art
techniques based on deep learning. We consider a strategy known as conformal
prediction, which predicts sets of labels instead of a single label; in the
classification and regression settings, these algorithms can guarantee that the
true label lies within the prediction set with high probability. Building on
these ideas, we propose multi-object detection and tracking algorithms that
come with probably approximately correct (PAC) guarantees. They do so by
constructing both a prediction set around each object detection as well as
around the set of edge transitions; given an object, the detection prediction
set contains its true bounding box with high probability, and the edge
prediction set contains its true transition across frames with high
probability. We empirically demonstrate that our method can detect and track
objects with PAC guarantees on the COCO and MOT-17 datasets.",http://arxiv.org/pdf/2204.07482v1,cs.CV
2022-04-14 20:55:11+00:00,Causal Disentanglement with Network Information for Debiased Recommendations,"['Paras Sheth', 'Ruocheng Guo', 'Lu Cheng', 'Huan Liu', 'K. Selçuk Candan']","Recommender systems aim to recommend new items to users by learning user and
item representations. In practice, these representations are highly entangled
as they consist of information about multiple factors, including user's
interests, item attributes along with confounding factors such as user
conformity, and item popularity. Considering these entangled representations
for inferring user preference may lead to biased recommendations (e.g., when
the recommender model recommends popular items even if they do not align with
the user's interests).
  Recent research proposes to debias by modeling a recommender system from a
causal perspective. The exposure and the ratings are analogous to the treatment
and the outcome in the causal inference framework, respectively. The critical
challenge in this setting is accounting for the hidden confounders. These
confounders are unobserved, making it hard to measure them. On the other hand,
since these confounders affect both the exposure and the ratings, it is
essential to account for them in generating debiased recommendations. To better
approximate hidden confounders, we propose to leverage network information
(i.e., user-social and user-item networks), which are shown to influence how
users discover and interact with an item. Aside from the user conformity,
aspects of confounding such as item popularity present in the network
information is also captured in our method with the aid of \textit{causal
disentanglement} which unravels the learned representations into independent
factors that are responsible for (a) modeling the exposure of an item to the
user, (b) predicting the ratings, and (c) controlling the hidden confounders.
Experiments on real-world datasets validate the effectiveness of the proposed
model for debiasing recommender systems.",http://arxiv.org/pdf/2204.07221v1,cs.IR
2022-04-14 05:44:59+00:00,Supplementation of deep neural networks with simplified physics-based features to increase model prediction accuracy,"['Nicholus R. Clinkinbeard', 'Prof. Nicole N. Hashemi']","To improve predictive models for STEM applications, supplemental
physics-based features computed from input parameters are introduced into
single and multiple layers of a deep neural network (DNN). While many studies
focus on informing DNNs with physics through differential equations or
numerical simulation, much may be gained through integration of simplified
relationships. To evaluate this hypothesis, a number of thin rectangular plates
simply-supported on all edges are simulated for five materials. With plate
dimensions and material properties as input features and fundamental natural
frequency as the sole output, predictive performance of a purely data-driven
DNN-based model is compared with models using additional inputs computed from
simplified physical relationships among baseline parameters, namely plate
weight, modulus of rigidity, and shear modulus. To better understand the
benefit to model accuracy, these additional features are injected into various
single and multiple DNN layers, and trained with four different dataset sizes.
When these physics-enhanced models are evaluated against independent data of
the same materials and similar dimensions to the training sets, supplementation
with simplified physics-based parameters provides little reduction in
prediction error over the baseline for models trained with dataset sizes of 60
and greater, although small improvement from 19.3% to 16.1% occurs when trained
with a sparse size of 30. Conversely, notable accuracy gains occur when the
independent test data is of material and dimensions not conforming to the
training set. Specifically, when physics-enhanced data is injected into
multiple DNN layers, reductions in error from 33.2% to 19.6%, 34.9% to 19.9%,
35.8% to 22.4%, and 43.0% to 28.4% are achieved for training dataset sizes of
261, 117, 60, and 30, respectively, demonstrating attainment of a degree of
generalizability.",http://arxiv.org/pdf/2204.06764v1,cs.ET
2022-04-10 06:18:23+00:00,Confidence Estimation Transformer for Long-term Renewable Energy Forecasting in Reinforcement Learning-based Power Grid Dispatching,"['Xinhang Li', 'Zihao Li', 'Nan Yang', 'Zheng Yuan', 'Qinwen Wang', 'Yiying Yang', 'Yupeng Huang', 'Xuri Song', 'Lei Li', 'Lin Zhang']","The expansion of renewable energy could help realizing the goals of peaking
carbon dioxide emissions and carbon neutralization. Some existing grid
dispatching methods integrating short-term renewable energy prediction and
reinforcement learning (RL) have been proved to alleviate the adverse impact of
energy fluctuations risk. However, these methods omit the long-term output
prediction, which leads to stability and security problems on the optimal power
flow. This paper proposes a confidence estimation Transformer for long-term
renewable energy forecasting in reinforcement learning-based power grid
dispatching (Conformer-RLpatching). Conformer-RLpatching predicts long-term
active output of each renewable energy generator with an enhanced Transformer
to boost the performance of hybrid energy grid dispatching. Furthermore, a
confidence estimation method is proposed to reduce the prediction error of
renewable energy. Meanwhile, a dispatching necessity evaluation mechanism is
put forward to decide whether the active output of a generator needs to be
adjusted. Experiments carried out on the SG-126 power grid simulator show that
Conformer-RLpatching achieves great improvement over the second best algorithm
DDPG in security score by 25.8% and achieves a better total reward compared
with the golden medal team in the power grid dispatching competition sponsored
by State Grid Corporation of China under the same simulation environment. Codes
are outsourced in https://github.com/buptlxh/Conformer-RLpatching.",http://arxiv.org/pdf/2204.04612v1,cs.LG
2022-04-09 13:01:33+00:00,Multichannel Speech Separation with Narrow-band Conformer,"['Changsheng Quan', 'Xiaofei Li']","This work proposes a multichannel speech separation method with narrow-band
Conformer (named NBC). The network is trained to learn to automatically exploit
narrow-band speech separation information, such as spatial vector clustering of
multiple speakers. Specifically, in the short-time Fourier transform (STFT)
domain, the network processes each frequency independently, and is shared by
all frequencies. For one frequency, the network inputs the STFT coefficients of
multichannel mixture signals, and predicts the STFT coefficients of separated
speech signals. Clustering of spatial vectors shares a similar principle with
the self-attention mechanism in the sense of computing the similarity of
vectors and then aggregating similar vectors. Therefore, Conformer would be
especially suitable for the present problem. Experiments show that the proposed
narrow-band Conformer achieves better speech separation performance than other
state-of-the-art methods by a large margin.",http://arxiv.org/pdf/2204.04464v2,cs.SD
2022-04-05 01:28:06+00:00,Online No-regret Model-Based Meta RL for Personalized Navigation,"['Yuda Song', 'Ye Yuan', 'Wen Sun', 'Kris Kitani']","The interaction between a vehicle navigation system and the driver of the
vehicle can be formulated as a model-based reinforcement learning problem,
where the navigation systems (agent) must quickly adapt to the characteristics
of the driver (environmental dynamics) to provide the best sequence of
turn-by-turn driving instructions. Most modern day navigation systems (e.g,
Google maps, Waze, Garmin) are not designed to personalize their low-level
interactions for individual users across a wide range of driving styles (e.g.,
vehicle type, reaction time, level of expertise). Towards the development of
personalized navigation systems that adapt to a variety of driving styles, we
propose an online no-regret model-based RL method that quickly conforms to the
dynamics of the current user. As the user interacts with it, the navigation
system quickly builds a user-specific model, from which navigation commands are
optimized using model predictive control. By personalizing the policy in this
way, our method is able to give well-timed driving instructions that match the
user's dynamics. Our theoretical analysis shows that our method is a no-regret
algorithm and we provide the convergence rate in the agnostic setting. Our
empirical analysis with 60+ hours of real-world user data using a driving
simulator shows that our method can reduce the number of collisions by more
than 60%.",http://arxiv.org/pdf/2204.01925v1,cs.LG
2022-04-05 00:18:39+00:00,Prediction Intervals for Simulation Metamodeling,"['Henry Lam', 'Haofeng Zhang']","Simulation metamodeling refers to the construction of lower-fidelity models
to represent input-output relations using few simulation runs. Stochastic
kriging, which is based on Gaussian process, is a versatile and common
technique for such a task. However, this approach relies on specific model
assumptions and could encounter scalability challenges. In this paper, we study
an alternative metamodeling approach using prediction intervals to capture the
uncertainty of simulation outputs. We cast the metamodeling task as an
empirical constrained optimization framework to train prediction intervals that
attain accurate prediction coverage and narrow width. We specifically use
neural network to represent these intervals and discuss procedures to
approximately solve this optimization problem. We also present an adaptation of
conformal prediction tools as another approach to construct prediction
intervals for metamodeling. Lastly, we present a validation machinery and show
how our method can enjoy a distribution-free finite-sample guarantee on the
prediction performance. We demonstrate and compare our proposed approaches with
existing methods including stochastic kriging through numerical examples.",http://arxiv.org/pdf/2204.01904v1,stat.ME
2022-03-29 05:48:24+00:00,MFA-Conformer: Multi-scale Feature Aggregation Conformer for Automatic Speaker Verification,"['Yang Zhang', 'Zhiqiang Lv', 'Haibin Wu', 'Shanshan Zhang', 'Pengfei Hu', 'Zhiyong Wu', 'Hung-yi Lee', 'Helen Meng']","In this paper, we present Multi-scale Feature Aggregation Conformer
(MFA-Conformer), an easy-to-implement, simple but effective backbone for
automatic speaker verification based on the Convolution-augmented Transformer
(Conformer). The architecture of the MFA-Conformer is inspired by recent
stateof-the-art models in speech recognition and speaker verification. Firstly,
we introduce a convolution subsampling layer to decrease the computational cost
of the model. Secondly, we adopt Conformer blocks which combine Transformers
and convolution neural networks (CNNs) to capture global and local features
effectively. Finally, the output feature maps from all Conformer blocks are
concatenated to aggregate multi-scale representations before final pooling. We
evaluate the MFA-Conformer on the widely used benchmarks. The best system
obtains 0.64%, 1.29% and 1.63% EER on VoxCeleb1-O, SITW.Dev, and SITW.Eval set,
respectively. MFA-Conformer significantly outperforms the popular ECAPA-TDNN
systems in both recognition performance and inference speed. Last but not the
least, the ablation studies clearly demonstrate that the combination of global
and local feature learning can lead to robust and accurate speaker embedding
extraction. We have also released the code for future comparison.",http://arxiv.org/pdf/2203.15249v2,cs.SD
2022-03-23 22:42:29+00:00,Functional mimicry of Ruffini receptors with Fiber Bragg Gratings and Deep Neural Networks enables a bio-inspired large-area tactile sensitive skin,"['Luca Massari', 'Giulia Fransvea', ""Jessica D'Abbraccio"", 'Mariangela Filosa', 'Giuseppe Terruso', 'Andrea Aliperta', ""Giacomo D'Alesio"", 'Martina Zaltieri', 'Emiliano Schena', 'Eduardo Palermo', 'Edoardo Sinibaldi', 'Calogero Maria Oddo']","Collaborative robots are expected to physically interact with humans in daily
living and workplace, including industrial and healthcare settings. A related
key enabling technology is tactile sensing, which currently requires addressing
the outstanding scientific challenge to simultaneously detect contact location
and intensity by means of soft conformable artificial skins adapting over large
areas to the complex curved geometries of robot embodiments. In this work, the
development of a large-area sensitive soft skin with a curved geometry is
presented, allowing for robot total-body coverage through modular patches. The
biomimetic skin consists of a soft polymeric matrix, resembling a human
forearm, embedded with photonic Fiber Bragg Grating (FBG) transducers, which
partially mimics Ruffini mechanoreceptor functionality with diffuse,
overlapping receptive fields. A Convolutional Neural Network deep learning
algorithm and a multigrid Neuron Integration Process were implemented to decode
the FBG sensor outputs for inferring contact force magnitude and localization
through the skin surface. Results achieved 35 mN (IQR = 56 mN) and 3.2 mm (IQR
= 2.3 mm) median errors, for force and localization predictions, respectively.
Demonstrations with an anthropomorphic arm pave the way towards AI-based
integrated skins enabling safe human-robot cooperation via machine
intelligence.",http://arxiv.org/pdf/2203.12752v1,cs.RO
2022-03-21 13:38:41+00:00,What is a randomization test?,"['Yao Zhang', 'Qingyuan Zhao']","The meaning of randomization tests has become obscure in statistics education
and practice over the last century. This article makes a fresh attempt at
rectifying this core concept of statistics. A new term -- ""quasi-randomization
test"" -- is introduced to define significance tests based on theoretical models
and distinguish these tests from the ""randomization tests"" based on the
physical act of randomization. The practical importance of this distinction is
illustrated through a real stepped-wedge cluster-randomized trial. Building on
the recent literature of randomization inference, a general framework of
conditional randomization tests is developed and some practical methods to
construct conditioning events are given. The proposed terminology and framework
are then applied to understand several widely used (quasi-)randomization tests,
including Fisher's exact test, permutation tests for treatment effect,
quasi-randomization tests for independence and conditional independence,
adaptive randomization, and conformal prediction.",http://arxiv.org/pdf/2203.10980v2,stat.ME
2022-03-15 17:58:03+00:00,CryoAI: Amortized Inference of Poses for Ab Initio Reconstruction of 3D Molecular Volumes from Real Cryo-EM Images,"['Axel Levy', 'Frédéric Poitevin', 'Julien Martel', 'Youssef Nashed', 'Ariana Peck', 'Nina Miolane', 'Daniel Ratner', 'Mike Dunne', 'Gordon Wetzstein']","Cryo-electron microscopy (cryo-EM) has become a tool of fundamental
importance in structural biology, helping us understand the basic building
blocks of life. The algorithmic challenge of cryo-EM is to jointly estimate the
unknown 3D poses and the 3D electron scattering potential of a biomolecule from
millions of extremely noisy 2D images. Existing reconstruction algorithms,
however, cannot easily keep pace with the rapidly growing size of cryo-EM
datasets due to their high computational and memory cost. We introduce cryoAI,
an ab initio reconstruction algorithm for homogeneous conformations that uses
direct gradient-based optimization of particle poses and the electron
scattering potential from single-particle cryo-EM data. CryoAI combines a
learned encoder that predicts the poses of each particle image with a
physics-based decoder to aggregate each particle image into an implicit
representation of the scattering potential volume. This volume is stored in the
Fourier domain for computational efficiency and leverages a modern coordinate
network architecture for memory efficiency. Combined with a symmetrized loss
function, this framework achieves results of a quality on par with
state-of-the-art cryo-EM solvers for both simulated and experimental data, one
order of magnitude faster for large datasets and with significantly lower
memory requirements than existing methods.",http://arxiv.org/pdf/2203.08138v4,cs.CV
2022-03-07 06:00:07+00:00,On the Construction of Distribution-Free Prediction Intervals for an Image Regression Problem in Semiconductor Manufacturing,"['Inimfon I. Akpabio', 'Serap A. Savari']","The high-volume manufacturing of the next generation of semiconductor devices
requires advances in measurement signal analysis. Many in the semiconductor
manufacturing community have reservations about the adoption of deep learning;
they instead prefer other model-based approaches for some image regression
problems, and according to the 2021 IEEE International Roadmap for Devices and
Systems (IRDS) report on Metrology a SEMI standardization committee may endorse
this philosophy. The semiconductor manufacturing community does, however,
communicate a need for state-of-the-art statistical analyses to reduce
measurement uncertainty. Prediction intervals which characterize the
reliability of the predictive performance of regression models can impact
decisions, build trust in machine learning, and be applied to other regression
models. However, we are not aware of effective and sufficiently simple
distribution-free approaches that offer valid coverage for important classes of
image data, so we consider the distribution-free conformal prediction and
conformalized quantile regression framework.The image regression problem that
is the focus of this paper pertains to line edge roughness (LER) estimation
from noisy scanning electron microscopy images. LER affects semiconductor
device performance and reliability as well as the yield of the manufacturing
process; the 2021 IRDS emphasizes the crucial importance of LER by devoting a
white paper to it in addition to mentioning or discussing it in the reports of
multiple international focus teams. It is not immediately apparent how to
effectively use normalized conformal prediction and quantile regression for LER
estimation. The modeling techniques we apply appear to be novel for finding
distribution-free prediction intervals for image data and will be presented at
the 2022 SEMI Advanced Semiconductor Manufacturing Conference.",http://arxiv.org/pdf/2203.03150v1,cs.CV
2022-03-06 09:47:01+00:00,GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation,"['Minkai Xu', 'Lantao Yu', 'Yang Song', 'Chence Shi', 'Stefano Ermon', 'Jian Tang']","Predicting molecular conformations from molecular graphs is a fundamental
problem in cheminformatics and drug discovery. Recently, significant progress
has been achieved with machine learning approaches, especially with deep
generative models. Inspired by the diffusion process in classical
non-equilibrium thermodynamics where heated particles will diffuse from
original states to a noise distribution, in this paper, we propose a novel
generative model named GeoDiff for molecular conformation prediction. GeoDiff
treats each atom as a particle and learns to directly reverse the diffusion
process (i.e., transforming from a noise distribution to stable conformations)
as a Markov chain. Modeling such a generation process is however very
challenging as the likelihood of conformations should be roto-translational
invariant. We theoretically show that Markov chains evolving with equivariant
Markov kernels can induce an invariant distribution by design, and further
propose building blocks for the Markov kernels to preserve the desirable
equivariance property. The whole framework can be efficiently trained in an
end-to-end fashion by optimizing a weighted variational lower bound to the
(conditional) likelihood. Experiments on multiple benchmarks show that GeoDiff
is superior or comparable to existing state-of-the-art approaches, especially
on large molecules.",http://arxiv.org/pdf/2203.02923v1,cs.LG
2022-03-03 15:12:12+00:00,Doubly Robust Calibration of Prediction Sets under Covariate Shift,"['Yachong Yang', 'Arun Kumar Kuchibhotla', 'Eric Tchetgen Tchetgen']","Conformal prediction has received tremendous attention in recent years and
has offered new solutions to problems in missing data and causal inference; yet
these advances have not leveraged modern semiparametric efficiency theory for
more robust and efficient uncertainty quantification. In this paper, we
consider the problem of obtaining distribution-free prediction regions
accounting for a shift in the distribution of the covariates between the
training and test data. Under an explainable covariate shift assumption
analogous to the standard missing at random assumption, we propose three
variants of a general framework to construct well-calibrated prediction regions
for the unobserved outcome in the test sample. Our approach is based on the
efficient influence function for the quantile of the unobserved outcome in the
test population combined with an arbitrary machine learning prediction
algorithm, without compromising asymptotic coverage. Next, we extend our
approach to account for departure from the explainable covariate shift
assumption in a semiparametric sensitivity analysis for potential latent
covariate shift. In all cases, we establish that the resulting prediction sets
eventually attain nominal average coverage in large samples. This guarantee is
a consequence of the product bias form of our proposal which implies correct
coverage if either the propensity score or the conditional distribution of the
response is estimated sufficiently well. Our results also provide a framework
for construction of doubly robust prediction sets of individual treatment
effects, under both unconfoundedness and allowing for some degree of unmeasured
confounding. Finally, we discuss aggregation of prediction sets from different
machine learning algorithms for optimal prediction and illustrate the
performance of our methods in both synthetic and real data.",http://arxiv.org/pdf/2203.01761v3,stat.ME
2022-02-27 18:18:43+00:00,Conformal prediction beyond exchangeability,"['Rina Foygel Barber', 'Emmanuel J. Candes', 'Aaditya Ramdas', 'Ryan J. Tibshirani']","Conformal prediction is a popular, modern technique for providing valid
predictive inference for arbitrary machine learning models. Its validity relies
on the assumptions of exchangeability of the data, and symmetry of the given
model fitting algorithm as a function of the data. However, exchangeability is
often violated when predictive models are deployed in practice. For example, if
the data distribution drifts over time, then the data points are no longer
exchangeable; moreover, in such settings, we might want to use a nonsymmetric
algorithm that treats recent observations as more relevant. This paper
generalizes conformal prediction to deal with both aspects: we employ weighted
quantiles to introduce robustness against distribution drift, and design a new
randomization technique to allow for algorithms that do not treat data points
symmetrically. Our new methods are provably robust, with substantially less
loss of coverage when exchangeability is violated due to distribution drift or
other challenging features of real data, while also achieving the same coverage
guarantees as existing conformal prediction methods if the data points are in
fact exchangeable. We demonstrate the practical utility of these new tools with
simulations and real-data experiments on electricity and election forecasting.",http://arxiv.org/pdf/2202.13415v5,stat.ME
2022-02-25 08:32:27+00:00,Raman Spectrum Matching with Contrastive Representation Learning,"['Bo Li', 'Mikkel N. Schmidt', 'Tommy S. Alstrøm']","Raman spectroscopy is an effective, low-cost, non-intrusive technique often
used for chemical identification. Typical approaches are based on matching
observations to a reference database, which requires careful preprocessing, or
supervised machine learning, which requires a fairly large number of training
observations from each class. We propose a new machine learning technique for
Raman spectrum matching, based on contrastive representation learning, that
requires no preprocessing and works with as little as a single reference
spectrum from each class. On three datasets we demonstrate that our approach
significantly improves or is on par with the state of the art in prediction
accuracy, and we show how to compute conformal prediction sets with specified
frequentist coverage. Based on our findings, we believe contrastive
representation learning is a promising alternative to existing methods for
Raman spectrum matching.",http://arxiv.org/pdf/2202.12549v1,cs.LG
2022-02-24 19:02:14+00:00,AutoIP: A United Framework to Integrate Physics into Gaussian Processes,"['Da Long', 'Zheng Wang', 'Aditi Krishnapriyan', 'Robert Kirby', 'Shandian Zhe', 'Michael Mahoney']","Physical modeling is critical for many modern science and engineering
applications. From a data science or machine learning perspective, where more
domain-agnostic, data-driven models are pervasive, physical knowledge -- often
expressed as differential equations -- is valuable in that it is complementary
to data, and it can potentially help overcome issues such as data sparsity,
noise, and inaccuracy. In this work, we propose a simple, yet powerful and
general framework -- AutoIP, for Automatically Incorporating Physics -- that
can integrate all kinds of differential equations into Gaussian Processes (GPs)
to enhance prediction accuracy and uncertainty quantification. These equations
can be linear or nonlinear, spatial, temporal, or spatio-temporal, complete or
incomplete with unknown source terms, and so on. Based on kernel
differentiation, we construct a GP prior to sample the values of the target
function, equation-related derivatives, and latent source functions, which are
all jointly from a multivariate Gaussian distribution. The sampled values are
fed to two likelihoods: one to fit the observations, and the other to conform
to the equation. We use the whitening method to evade the strong dependency
between the sampled function values and kernel parameters, and we develop a
stochastic variational learning algorithm. AutoIP shows improvement upon
vanilla GPs in both simulation and several real-world applications, even using
rough, incomplete equations.",http://arxiv.org/pdf/2202.12316v2,cs.LG
2022-02-23 11:14:24+00:00,Open5x: Accessible 5-axis 3D printing and conformal slicing,"['Freddie Hong', 'Steve Hodges', 'Connor Myant', 'David Boyle']","The common layer-by-layer deposition of regular, 3-axis 3D printing
simplifies both the fabrication process and the 3D printer's mechanical design.
However, the resulting 3D printed objects have some unfavourable
characteristics including visible layers, uneven structural strength and
support material. To overcome these, researchers have employed robotic arms and
multi-axis CNCs to deposit materials in conformal layers. Conformal deposition
improves the quality of the 3D printed parts through support-less printing and
curved layer deposition. However, such multi-axis 3D printing is inaccessible
to many individuals due to high costs and technical complexities. Furthermore,
the limited GUI support for conformal slicers creates an additional barrier for
users. To open multi-axis 3D printing up to more makers and researchers, we
present a cheap and accessible way to upgrade a regular 3D printer to 5 axes.
We have also developed a GUI-based conformal slicer, integrated within a
popular CAD package. Together, these deliver an accessible workflow for
designing, simulating and creating conformally-printed 3D models.",http://arxiv.org/pdf/2202.11426v2,cs.HC
2022-02-22 18:37:23+00:00,Efficient and Differentiable Conformal Prediction with General Function Classes,"['Yu Bai', 'Song Mei', 'Huan Wang', 'Yingbo Zhou', 'Caiming Xiong']","Quantifying the data uncertainty in learning tasks is often done by learning
a prediction interval or prediction set of the label given the input. Two
commonly desired properties for learned prediction sets are \emph{valid
coverage} and \emph{good efficiency} (such as low length or low cardinality).
Conformal prediction is a powerful technique for learning prediction sets with
valid coverage, yet by default its conformalization step only learns a single
parameter, and does not optimize the efficiency over more expressive function
classes.
  In this paper, we propose a generalization of conformal prediction to
multiple learnable parameters, by considering the constrained empirical risk
minimization (ERM) problem of finding the most efficient prediction set subject
to valid empirical coverage. This meta-algorithm generalizes existing conformal
prediction algorithms, and we show that it achieves approximate valid
population coverage and near-optimal efficiency within class, whenever the
function class in the conformalization step is low-capacity in a certain sense.
Next, this ERM problem is challenging to optimize as it involves a
non-differentiable coverage constraint. We develop a gradient-based algorithm
for it by approximating the original constrained ERM using differentiable
surrogate losses and Lagrangians. Experiments show that our algorithm is able
to learn valid prediction sets and improve the efficiency significantly over
existing approaches in several applications such as prediction intervals with
improved length, minimum-volume prediction sets for multi-output regression,
and label prediction sets for image classification.",http://arxiv.org/pdf/2202.11091v2,cs.LG
2022-02-21 17:58:07+00:00,Investigations of Performance and Bias in Human-AI Teamwork in Hiring,"['Andi Peng', 'Besmira Nushi', 'Emre Kiciman', 'Kori Inkpen', 'Ece Kamar']","In AI-assisted decision-making, effective hybrid (human-AI) teamwork is not
solely dependent on AI performance alone, but also on its impact on human
decision-making. While prior work studies the effects of model accuracy on
humans, we endeavour here to investigate the complex dynamics of how both a
model's predictive performance and bias may transfer to humans in a
recommendation-aided decision task. We consider the domain of ML-assisted
hiring, where humans -- operating in a constrained selection setting -- can
choose whether they wish to utilize a trained model's inferences to help select
candidates from written biographies. We conduct a large-scale user study
leveraging a re-created dataset of real bios from prior work, where humans
predict the ground truth occupation of given candidates with and without the
help of three different NLP classifiers (random, bag-of-words, and deep neural
network). Our results demonstrate that while high-performance models
significantly improve human performance in a hybrid setting, some models
mitigate hybrid bias while others accentuate it. We examine these findings
through the lens of decision conformity and observe that our model architecture
choices have an impact on human-AI conformity and bias, motivating the explicit
need to assess these complex dynamics prior to deployment.",http://arxiv.org/pdf/2202.11812v1,cs.HC
2022-02-17 18:58:31+00:00,Data-SUITE: Data-centric identification of in-distribution incongruous examples,"['Nabeel Seedat', 'Jonathan Crabbé', 'Mihaela van der Schaar']","Systematic quantification of data quality is critical for consistent model
performance. Prior works have focused on out-of-distribution data. Instead, we
tackle an understudied yet equally important problem of characterizing
incongruous regions of in-distribution (ID) data, which may arise from feature
space heterogeneity. To this end, we propose a paradigm shift with Data-SUITE:
a data-centric AI framework to identify these regions, independent of a
task-specific model. Data-SUITE leverages copula modeling, representation
learning, and conformal prediction to build feature-wise confidence interval
estimators based on a set of training instances. These estimators can be used
to evaluate the congruence of test instances with respect to the training set,
to answer two practically useful questions: (1) which test instances will be
reliably predicted by a model trained with the training instances? and (2) can
we identify incongruous regions of the feature space so that data owners
understand the data's limitations or guide future data collection? We
empirically validate Data-SUITE's performance and coverage guarantees and
demonstrate on cross-site medical data, biased data, and data with concept
drift, that Data-SUITE best identifies ID regions where a downstream model may
be reliable (independent of said model). We also illustrate how these
identified regions can provide insights into datasets and highlight their
limitations.",http://arxiv.org/pdf/2202.08836v3,cs.LG
2022-02-17 16:54:20+00:00,Ensemble Conformalized Quantile Regression for Probabilistic Time Series Forecasting,"['Vilde Jensen', 'Filippo Maria Bianchi', 'Stian Norman Anfinsen']","This paper presents a novel probabilistic forecasting method called ensemble
conformalized quantile regression (EnCQR). EnCQR constructs distribution-free
and approximately marginally valid prediction intervals (PIs), which are
suitable for nonstationary and heteroscedastic time series data. EnCQR can be
applied on top of a generic forecasting model, including deep learning
architectures. EnCQR exploits a bootstrap ensemble estimator, which enables the
use of conformal predictors for time series by removing the requirement of data
exchangeability. The ensemble learners are implemented as generic machine
learning algorithms performing quantile regression, which allow the length of
the PIs to adapt to local variability in the data. In the experiments, we
predict time series characterized by a different amount of heteroscedasticity.
The results demonstrate that EnCQR outperforms models based only on quantile
regression or conformal prediction, and it provides sharper, more informative,
and valid PIs.",http://arxiv.org/pdf/2202.08756v2,cs.LG
2022-02-16 04:33:05+00:00,Conversational Speech Recognition By Learning Conversation-level Characteristics,"['Kun Wei', 'Yike Zhang', 'Sining Sun', 'Lei Xie', 'Long Ma']","Conversational automatic speech recognition (ASR) is a task to recognize
conversational speech including multiple speakers. Unlike sentence-level ASR,
conversational ASR can naturally take advantages from specific characteristics
of conversation, such as role preference and topical coherence. This paper
proposes a conversational ASR model which explicitly learns conversation-level
characteristics under the prevalent end-to-end neural framework. The highlights
of the proposed model are twofold. First, a latent variational module (LVM) is
attached to a conformer-based encoder-decoder ASR backbone to learn role
preference and topical coherence. Second, a topic model is specifically adopted
to bias the outputs of the decoder to words in the predicted topics.
Experiments on two Mandarin conversational ASR tasks show that the proposed
model achieves a maximum 12% relative character error rate (CER) reduction.",http://arxiv.org/pdf/2202.07855v2,cs.SD
2022-02-15 18:52:33+00:00,Conformal Prediction Sets with Limited False Positives,"['Adam Fisch', 'Tal Schuster', 'Tommi Jaakkola', 'Regina Barzilay']","We develop a new approach to multi-label conformal prediction in which we aim
to output a precise set of promising prediction candidates with a bounded
number of incorrect answers. Standard conformal prediction provides the ability
to adapt to model uncertainty by constructing a calibrated candidate set in
place of a single prediction, with guarantees that the set contains the correct
answer with high probability. In order to obey this coverage property, however,
conformal sets can become inundated with noisy candidates -- which can render
them unhelpful in practice. This is particularly relevant to practical
applications where there is a limited budget, and the cost (monetary or
otherwise) associated with false positives is non-negligible. We propose to
trade coverage for a notion of precision by enforcing that the presence of
incorrect candidates in the predicted conformal sets (i.e., the total number of
false positives) is bounded according to a user-specified tolerance. Subject to
this constraint, our algorithm then optimizes for a generalized notion of set
coverage (i.e., the true positive rate) that allows for any number of true
answers for a given query (including zero). We demonstrate the effectiveness of
this approach across a number of classification tasks in natural language
processing, computer vision, and computational chemistry.",http://arxiv.org/pdf/2202.07650v1,cs.LG
2022-02-15 09:57:01+00:00,Adaptive Conformal Predictions for Time Series,"['Margaux Zaffran', 'Aymeric Dieuleveut', 'Olivier Féron', 'Yannig Goude', 'Julie Josse']","Uncertainty quantification of predictive models is crucial in decision-making
problems. Conformal prediction is a general and theoretically sound answer.
However, it requires exchangeable data, excluding time series. While recent
works tackled this issue, we argue that Adaptive Conformal Inference (ACI,
Gibbs and Cand{\`e}s, 2021), developed for distribution-shift time series, is a
good procedure for time series with general dependency. We theoretically
analyse the impact of the learning rate on its efficiency in the exchangeable
and auto-regressive case. We propose a parameter-free method, AgACI, that
adaptively builds upon ACI based on online expert aggregation. We lead
extensive fair simulations against competing methods that advocate for ACI's
use in time series. We conduct a real case study: electricity price
forecasting. The proposed aggregation algorithm provides efficient prediction
intervals for day-ahead forecasting. All the code and data to reproduce the
experiments is made available.",http://arxiv.org/pdf/2202.07282v1,stat.ML
2022-02-09 01:14:26+00:00,"Stein Particle Filter for Nonlinear, Non-Gaussian State Estimation","['Fahira Afzal Maken', 'Fabio Ramos', 'Lionel Ott']","Estimation of a dynamical system's latent state subject to sensor noise and
model inaccuracies remains a critical yet difficult problem in robotics. While
Kalman filters provide the optimal solution in the least squared sense for
linear and Gaussian noise problems, the general nonlinear and non-Gaussian
noise case is significantly more complicated, typically relying on sampling
strategies that are limited to low-dimensional state spaces. In this paper we
devise a general inference procedure for filtering of nonlinear, non-Gaussian
dynamical systems that exploits the differentiability of both the update and
prediction models to scale to higher dimensional spaces. Our method, Stein
particle filter, can be seen as a deterministic flow of particles, embedded in
a reproducing kernel Hilbert space, from an initial state to the desirable
posterior. The particles evolve jointly to conform to a posterior approximation
while interacting with each other through a repulsive force. We evaluate the
method in simulation and in complex localization tasks while comparing it to
sequential Monte Carlo solutions.",http://arxiv.org/pdf/2202.04213v1,cs.RO
2022-02-08 02:59:12+00:00,Conformal prediction for the design problem,"['Clara Fannjiang', 'Stephen Bates', 'Anastasios N. Angelopoulos', 'Jennifer Listgarten', 'Michael I. Jordan']","Many applications of machine learning methods involve an iterative protocol
in which data are collected, a model is trained, and then outputs of that model
are used to choose what data to consider next. For example, one data-driven
approach for designing proteins is to train a regression model to predict the
fitness of protein sequences, then use it to propose new sequences believed to
exhibit greater fitness than observed in the training data. Since validating
designed sequences in the wet lab is typically costly, it is important to
quantify the uncertainty in the model's predictions. This is challenging
because of a characteristic type of distribution shift between the training and
test data in the design setting -- one in which the training and test data are
statistically dependent, as the latter is chosen based on the former.
Consequently, the model's error on the test data -- that is, the designed
sequences -- has an unknown and possibly complex relationship with its error on
the training data. We introduce a method to quantify predictive uncertainty in
such settings. We do so by constructing confidence sets for predictions that
account for the dependence between the training and test data. The confidence
sets we construct have finite-sample guarantees that hold for any prediction
algorithm, even when a trained model chooses the test-time input distribution.
As a motivating use case, we demonstrate with several real data sets how our
method quantifies uncertainty for the predicted fitness of designed proteins,
and can therefore be used to select design algorithms that achieve acceptable
trade-offs between high predicted fitness and low predictive uncertainty.",http://arxiv.org/pdf/2202.03613v4,cs.LG
2022-02-07 05:07:44+00:00,Prompt-Guided Injection of Conformation to Pre-trained Protein Model,"['Qiang Zhang', 'Zeyuan Wang', 'Yuqiang Han', 'Haoran Yu', 'Xurui Jin', 'Huajun Chen']","Pre-trained protein models (PTPMs) represent a protein with one fixed
embedding and thus are not capable for diverse tasks. For example, protein
structures can shift, namely protein folding, between several conformations in
various biological processes. To enable PTPMs to produce task-aware
representations, we propose to learn interpretable, pluggable and extensible
protein prompts as a way of injecting task-related knowledge into PTPMs. In
this regard, prior PTPM optimization with the masked language modeling task can
be interpreted as learning a sequence prompt (Seq prompt) that enables PTPMs to
capture the sequential dependency between amino acids. To incorporate
conformational knowledge to PTPMs, we propose an interaction-conformation
prompt (IC prompt) that is learned through back-propagation with the
protein-protein interaction task. As an instantiation, we present a
conformation-aware pre-trained protein model that learns both sequence and
interaction-conformation prompts in a multi-task setting. We conduct
comprehensive experiments on nine protein datasets. Results confirm our
expectation that using the sequence prompt does not hurt PTPMs' performance on
sequence-related tasks while incorporating the interaction-conformation prompt
significantly improves PTPMs' performance on tasks where conformational
knowledge counts. We also show the learned prompts can be combined and extended
to deal with new complex tasks.",http://arxiv.org/pdf/2202.02944v1,cs.AI
2022-02-06 04:03:01+00:00,Soundiation: A multi-functional GUI-based software in evaluation of the acoustophoresis by the acoustic radiation force and torque on arbitrary axisymmetric objects,"['Tianquan Tang', 'Lixi Huang']","Acoustic radiation force and torque arising from wave scattering are commonly
used to manipulate micro-objects without contact. We applied the partial wave
expansion series and the conformal transformation approach to estimate the
radiation force and torque exerted on an axisymmetric particle. Meanwhile,
translational and rotational transformations are required to keep the
coordinate system consistent [1]. Although these theoretical derivations have
been well established, coding the required systems, including generation of the
wave function, implementation of the transformations, calculations between
modules, etc., is non-trivial and time-consuming. Here, a new open-source,
MATLAB-based software, called Soundiation, is provided to address the radiation
force and torque while supporting the dynamic prediction of non-spherical
particles. The implementation is basically generic, and its applicability is
demonstrated through the validation of numerical methods. Furthermore, a
graphical user interface is provided so that it can be used and extended
easily.",http://arxiv.org/pdf/2202.04526v1,cs.CE
2022-02-05 12:53:40+00:00,TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials,"['Philipp Thölke', 'Gianni De Fabritiis']","The prediction of quantum mechanical properties is historically plagued by a
trade-off between accuracy and speed. Machine learning potentials have
previously shown great success in this domain, reaching increasingly better
accuracy while maintaining computational efficiency comparable with classical
force fields. In this work we propose TorchMD-NET, a novel equivariant
transformer (ET) architecture, outperforming state-of-the-art on MD17, ANI-1,
and many QM9 targets in both accuracy and computational efficiency. Through an
extensive attention weight analysis, we gain valuable insights into the black
box predictor and show differences in the learned representation of conformers
versus conformations sampled from molecular dynamics or normal modes.
Furthermore, we highlight the importance of datasets including off-equilibrium
conformations for the evaluation of molecular potentials.",http://arxiv.org/pdf/2202.02541v2,cs.LG
2022-02-03 14:38:26+00:00,The RoyalFlush System of Speech Recognition for M2MeT Challenge,"['Shuaishuai Ye', 'Peiyao Wang', 'Shunfei Chen', 'Xinhui Hu', 'Xinkang Xu']","This paper describes our RoyalFlush system for the track of multi-speaker
automatic speech recognition (ASR) in the M2MeT challenge. We adopted the
serialized output training (SOT) based multi-speakers ASR system with
large-scale simulation data. Firstly, we investigated a set of front-end
methods, including multi-channel weighted predicted error (WPE), beamforming,
speech separation, speech enhancement and so on, to process training,
validation and test sets. But we only selected WPE and beamforming as our
frontend methods according to their experimental results. Secondly, we made
great efforts in the data augmentation for multi-speaker ASR, mainly including
adding noise and reverberation, overlapped speech simulation, multi-channel
speech simulation, speed perturbation, front-end processing, and so on, which
brought us a great performance improvement. Finally, in order to make full use
of the performance complementary of different model architecture, we trained
the standard conformer based joint CTC/Attention (Conformer) and U2++ ASR model
with a bidirectional attention decoder, a modification of Conformer, to fuse
their results. Comparing with the official baseline system, our system got a
12.22% absolute Character Error Rate (CER) reduction on the validation set and
12.11% on the test set.",http://arxiv.org/pdf/2202.01614v2,cs.SD
2022-02-03 01:01:58+00:00,Direct Molecular Conformation Generation,"['Jinhua Zhu', 'Yingce Xia', 'Chang Liu', 'Lijun Wu', 'Shufang Xie', 'Yusong Wang', 'Tong Wang', 'Tao Qin', 'Wengang Zhou', 'Houqiang Li', 'Haiguang Liu', 'Tie-Yan Liu']","Molecular conformation generation aims to generate three-dimensional
coordinates of all the atoms in a molecule and is an important task in
bioinformatics and pharmacology. Previous methods usually first predict the
interatomic distances, the gradients of interatomic distances or the local
structures (e.g., torsion angles) of a molecule, and then reconstruct its 3D
conformation. How to directly generate the conformation without the above
intermediate values is not fully explored. In this work, we propose a method
that directly predicts the coordinates of atoms: (1) the loss function is
invariant to roto-translation of coordinates and permutation of symmetric
atoms; (2) the newly proposed model adaptively aggregates the bond and atom
information and iteratively refines the coordinates of the generated
conformation. Our method achieves the best results on GEOM-QM9 and GEOM-Drugs
datasets. Further analysis shows that our generated conformations have closer
properties (e.g., HOMO-LUMO gap) with the groundtruth conformations. In
addition, our method improves molecular docking by providing better initial
conformations. All the results demonstrate the effectiveness of our method and
the great potential of the direct approach. The code is released at
https://github.com/DirectMolecularConfGen/DMCG",http://arxiv.org/pdf/2202.01356v2,cs.AI
2022-02-02 22:38:40+00:00,Approximating Full Conformal Prediction at Scale via Influence Functions,"['Javier Abad', 'Umang Bhatt', 'Adrian Weller', 'Giovanni Cherubin']","Conformal prediction (CP) is a wrapper around traditional machine learning
models, giving coverage guarantees under the sole assumption of
exchangeability; in classification problems, for a chosen significance level
$\varepsilon$, CP guarantees that the error rate is at most $\varepsilon$,
irrespective of whether the underlying model is misspecified. However, the
prohibitive computational costs of ""full"" CP led researchers to design scalable
alternatives, which alas do not attain the same guarantees or statistical power
of full CP. In this paper, we use influence functions to efficiently
approximate full CP. We prove that our method is a consistent approximation of
full CP, and empirically show that the approximation error becomes smaller as
the training set increases; e.g., for $10^{3}$ training points the two methods
output p-values that are $<10^{-3}$ apart: a negligible error for any practical
application. Our methods enable scaling full CP to large real-world datasets.
We compare our full CP approximation (ACP) to mainstream CP alternatives, and
observe that our method is computationally competitive whilst enjoying the
statistical predictive power of full CP.",http://arxiv.org/pdf/2202.01315v3,cs.LG
2022-01-28 09:35:37+00:00,Improving Expert Predictions with Conformal Prediction,"['Eleni Straitouri', 'Lequn Wang', 'Nastaran Okati', 'Manuel Gomez Rodriguez']","Automated decision support systems promise to help human experts solve
multiclass classification tasks more efficiently and accurately. However,
existing systems typically require experts to understand when to cede agency to
the system or when to exercise their own agency. Otherwise, the experts may be
better off solving the classification tasks on their own. In this work, we
develop an automated decision support system that, by design, does not require
experts to understand when to trust the system to improve performance. Rather
than providing (single) label predictions and letting experts decide when to
trust these predictions, our system provides sets of label predictions
constructed using conformal prediction$\unicode{x2014}$prediction
sets$\unicode{x2014}$and forcefully asks experts to predict labels from these
sets. By using conformal prediction, our system can precisely trade-off the
probability that the true label is not in the prediction set, which determines
how frequently our system will mislead the experts, and the size of the
prediction set, which determines the difficulty of the classification task the
experts need to solve using our system. In addition, we develop an efficient
and near-optimal search method to find the conformal predictor under which the
experts benefit the most from using our system. Simulation experiments using
synthetic and real expert predictions demonstrate that our system may help
experts make more accurate predictions and is robust to the accuracy of the
classifier the conformal predictor relies on.",http://arxiv.org/pdf/2201.12006v5,cs.LG
2022-01-21 18:05:00+00:00,Deconfounding to Explanation Evaluation in Graph Neural Networks,"['Ying-Xin Wu', 'Xiang Wang', 'An Zhang', 'Xia Hu', 'Fuli Feng', 'Xiangnan He', 'Tat-Seng Chua']","Explainability of graph neural networks (GNNs) aims to answer ""Why the GNN
made a certain prediction?"", which is crucial to interpret the model
prediction. The feature attribution framework distributes a GNN's prediction to
its input features (e.g., edges), identifying an influential subgraph as the
explanation. When evaluating the explanation (i.e., subgraph importance), a
standard way is to audit the model prediction based on the subgraph solely.
However, we argue that a distribution shift exists between the full graph and
the subgraph, causing the out-of-distribution problem. Furthermore, with an
in-depth causal analysis, we find the OOD effect acts as the confounder, which
brings spurious associations between the subgraph importance and model
prediction, making the evaluation less reliable. In this work, we propose
Deconfounded Subgraph Evaluation (DSE) which assesses the causal effect of an
explanatory subgraph on the model prediction. While the distribution shift is
generally intractable, we employ the front-door adjustment and introduce a
surrogate variable of the subgraphs. Specifically, we devise a generative model
to generate the plausible surrogates that conform to the data distribution,
thus approaching the unbiased estimation of subgraph importance. Empirical
results demonstrate the effectiveness of DSE in terms of explanation fidelity.",http://arxiv.org/pdf/2201.08802v3,cs.LG
2022-01-20 17:26:52+00:00,Predictive Inference with Weak Supervision,"['Maxime Cauchois', 'Suyash Gupta', 'Alnur Ali', 'John Duchi']","The expense of acquiring labels in large-scale statistical machine learning
makes partially and weakly-labeled data attractive, though it is not always
apparent how to leverage such data for model fitting or validation. We present
a methodology to bridge the gap between partial supervision and validation,
developing a conformal prediction framework to provide valid predictive
confidence sets -- sets that cover a true label with a prescribed probability,
independent of the underlying distribution -- using weakly labeled data. To do
so, we introduce a (necessary) new notion of coverage and predictive validity,
then develop several application scenarios, providing efficient algorithms for
classification and several large-scale structured prediction problems. We
corroborate the hypothesis that the new coverage definition allows for tighter
and more informative (but valid) confidence sets through several experiments.",http://arxiv.org/pdf/2201.08315v2,stat.ML
2022-01-19 18:33:01+00:00,Monte Carlo sampling of flexible protein structures: an application to the SARS-CoV-2 omicron variant,['Samuel W. K. Wong'],"Proteins can exhibit dynamic structural flexibility as they carry out their
functions, especially in binding regions that interact with other molecules.
For the key SARS-CoV-2 spike protein that facilitates COVID-19 infection,
studies have previously identified several such highly flexible regions with
therapeutic importance. However, protein structures available from the Protein
Data Bank are presented as static snapshots that may not adequately depict this
flexibility, and furthermore these cannot keep pace with new mutations and
variants. In this paper we present a sequential Monte Carlo method for broadly
sampling the 3-D conformational space of protein structure, according to the
Boltzmann distribution of a given energy function. Our approach is distinct
from previous sampling methods that focus on finding the lowest-energy
conformation for predicting a single stable structure. We exemplify our method
on the SARS-CoV-2 omicron variant as an application of timely interest. Our
results identify sequence positions 495-508 as a key region where omicron
mutations have the most impact on the space of possible conformations, which
coincides with the findings of other preliminary studies on the binding
properties of the omicron variant.",http://arxiv.org/pdf/2201.07775v2,stat.AP
2022-01-13 16:44:51+00:00,Feature-rich multiplex lexical networks reveal mental strategies of early language learning,"['Salvatore Citraro', 'Michael S. Vitevitch', 'Massimo Stella', 'Giulio Rossetti']","Knowledge in the human mind exhibits a dualistic vector/network nature.
Modelling words as vectors is key to natural language processing, whereas
networks of word associations can map the nature of semantic memory. We
reconcile these paradigms - fragmented across linguistics, psychology and
computer science - by introducing FEature-Rich MUltiplex LEXical (FERMULEX)
networks. This novel framework merges structural similarities in networks and
vector features of words, which can be combined or explored independently.
Similarities model heterogenous word associations across
semantic/syntactic/phonological aspects of knowledge. Words are enriched with
multi-dimensional feature embeddings including frequency, age of acquisition,
length and polysemy. These aspects enable unprecedented explorations of
cognitive knowledge. Through CHILDES data, we use FERMULEX networks to model
normative language acquisition by 1000 toddlers between 18 and 30 months.
Similarities and embeddings capture word homophily via conformity, which
measures assortative mixing via distance and features. Conformity unearths a
language kernel of frequent/polysemous/short nouns and verbs key for basic
sentence production, supporting recent evidence of children's syntactic
constructs emerging at 30 months. This kernel is invisible to network
core-detection and feature-only clustering: It emerges from the dual
vector/network nature of words. Our quantitative analysis reveals two key
strategies in early word learning. Modelling word acquisition as random walks
on FERMULEX topology, we highlight non-uniform filling of communicative
developmental inventories (CDIs). Conformity-based walkers lead to accurate
(75%), precise (55%) and partially well-recalled (34%) predictions of early
word learning in CDIs, providing quantitative support to previous empirical
findings and developmental theories.",http://arxiv.org/pdf/2201.05061v1,cs.CL
2022-01-07 05:21:40+00:00,iDECODe: In-distribution Equivariance for Conformal Out-of-distribution Detection,"['Ramneet Kaur', 'Susmit Jha', 'Anirban Roy', 'Sangdon Park', 'Edgar Dobriban', 'Oleg Sokolsky', 'Insup Lee']","Machine learning methods such as deep neural networks (DNNs), despite their
success across different domains, are known to often generate incorrect
predictions with high confidence on inputs outside their training distribution.
The deployment of DNNs in safety-critical domains requires detection of
out-of-distribution (OOD) data so that DNNs can abstain from making predictions
on those. A number of methods have been recently developed for OOD detection,
but there is still room for improvement. We propose the new method iDECODe,
leveraging in-distribution equivariance for conformal OOD detection. It relies
on a novel base non-conformity measure and a new aggregation method, used in
the inductive conformal anomaly detection framework, thereby guaranteeing a
bounded false detection rate. We demonstrate the efficacy of iDECODe by
experiments on image and audio datasets, obtaining state-of-the-art results. We
also show that iDECODe can detect adversarial examples.",http://arxiv.org/pdf/2201.02331v1,cs.LG
2021-12-19 18:53:32+00:00,Stable Conformal Prediction Sets,['Eugene Ndiaye'],"When one observes a sequence of variables $(x_1, y_1), \ldots, (x_n, y_n)$,
Conformal Prediction (CP) is a methodology that allows to estimate a confidence
set for $y_{n+1}$ given $x_{n+1}$ by merely assuming that the distribution of
the data is exchangeable. CP sets have guaranteed coverage for any finite
population size $n$. While appealing, the computation of such a set turns out
to be infeasible in general, e.g. when the unknown variable $y_{n+1}$ is
continuous. The bottleneck is that it is based on a procedure that readjusts a
prediction model on data where we replace the unknown target by all its
possible values in order to select the most probable one. This requires
computing an infinite number of models, which often makes it intractable. In
this paper, we combine CP techniques with classical algorithmic stability
bounds to derive a prediction set computable with a single model fit. We
demonstrate that our proposed confidence set does not lose any coverage
guarantees while avoiding the need for data splitting as currently done in the
literature. We provide some numerical experiments to illustrate the tightness
of our estimation when the sample size is sufficiently large, on both synthetic
and real datasets.",http://arxiv.org/pdf/2112.10224v2,stat.ML
2021-12-13 18:16:18+00:00,Why Are You Weird? Infusing Interpretability in Isolation Forest for Anomaly Detection,"['Nirmal Sobha Kartha', 'Clément Gautrais', 'Vincent Vercruyssen']","Anomaly detection is concerned with identifying examples in a dataset that do
not conform to the expected behaviour. While a vast amount of anomaly detection
algorithms exist, little attention has been paid to explaining why these
algorithms flag certain examples as anomalies. However, such an explanation
could be extremely useful to anyone interpreting the algorithms' output. This
paper develops a method to explain the anomaly predictions of the
state-of-the-art Isolation Forest anomaly detection algorithm. The method
outputs an explanation vector that captures how important each attribute of an
example is to identifying it as anomalous. A thorough experimental evaluation
on both synthetic and real-world datasets shows that our method is more
accurate and more efficient than most contemporary state-of-the-art
explainability methods.",http://arxiv.org/pdf/2112.06858v1,cs.LG
2021-12-07 04:47:43+00:00,Conformal Sensitivity Analysis for Individual Treatment Effects,"['Mingzhang Yin', 'Claudia Shi', 'Yixin Wang', 'David M. Blei']","Estimating an individual treatment effect (ITE) is essential to personalized
decision making. However, existing methods for estimating the ITE often rely on
unconfoundedness, an assumption that is fundamentally untestable with observed
data. To assess the robustness of individual-level causal conclusion with
unconfoundedness, this paper proposes a method for sensitivity analysis of the
ITE, a way to estimate a range of the ITE under unobserved confounding. The
method we develop quantifies unmeasured confounding through a marginal
sensitivity model [Ros2002, Tan2006], and adapts the framework of conformal
inference to estimate an ITE interval at a given confounding strength. In
particular, we formulate this sensitivity analysis problem as a conformal
inference problem under distribution shift, and we extend existing methods of
covariate-shifted conformal inference to this more general setting. The result
is a predictive interval that has guaranteed nominal coverage of the ITE, a
method that provides coverage with distribution-free and nonasymptotic
guarantees. We evaluate the method on synthetic data and illustrate its
application in an observational study.",http://arxiv.org/pdf/2112.03493v4,stat.ME
2021-12-06 18:23:29+00:00,Text2Mesh: Text-Driven Neural Stylization for Meshes,"['Oscar Michel', 'Roi Bar-On', 'Richard Liu', 'Sagie Benaim', 'Rana Hanocka']","In this work, we develop intuitive controls for editing the style of 3D
objects. Our framework, Text2Mesh, stylizes a 3D mesh by predicting color and
local geometric details which conform to a target text prompt. We consider a
disentangled representation of a 3D object using a fixed mesh input (content)
coupled with a learned neural network, which we term neural style field
network. In order to modify style, we obtain a similarity score between a text
prompt (describing style) and a stylized mesh by harnessing the
representational power of CLIP. Text2Mesh requires neither a pre-trained
generative model nor a specialized 3D mesh dataset. It can handle low-quality
meshes (non-manifold, boundaries, etc.) with arbitrary genus, and does not
require UV parameterization. We demonstrate the ability of our technique to
synthesize a myriad of styles over a wide variety of 3D meshes.",http://arxiv.org/pdf/2112.03221v1,cs.CV
2021-12-02 17:11:33+00:00,Training Efficiency and Robustness in Deep Learning,['Fartash Faghri'],"Deep Learning has revolutionized machine learning and artificial
intelligence, achieving superhuman performance in several standard benchmarks.
It is well-known that deep learning models are inefficient to train; they learn
by processing millions of training data multiple times and require powerful
computational resources to process large batches of data in parallel at the
same time rather than sequentially. Deep learning models also have unexpected
failure modes; they can be fooled into misbehaviour, producing unexpectedly
incorrect predictions.
  In this thesis, we study approaches to improve the training efficiency and
robustness of deep learning models. In the context of learning visual-semantic
embeddings, we find that prioritizing learning on more informative training
data increases convergence speed and improves generalization performance on
test data. We formalize a simple trick called hard negative mining as a
modification to the learning objective function with no computational overhead.
Next, we seek improvements to optimization speed in general-purpose
optimization methods in deep learning. We show that a redundancy-aware
modification to the sampling of training data improves the training speed and
develops an efficient method for detecting the diversity of training signal,
namely, gradient clustering. Finally, we study adversarial robustness in deep
learning and approaches to achieve maximal adversarial robustness without
training with additional data. For linear models, we prove guaranteed maximal
robustness achieved only by appropriate choice of the optimizer,
regularization, or architecture.",http://arxiv.org/pdf/2112.01423v1,cs.LG
2021-11-23 21:40:39+00:00,Sensitivity Analysis of Individual Treatment Effects: A Robust Conformal Inference Approach,"['Ying Jin', 'Zhimei Ren', 'Emmanuel J. Candès']","We propose a model-free framework for sensitivity analysis of individual
treatment effects (ITEs), building upon ideas from conformal inference. For any
unit, our procedure reports the $\Gamma$-value, a number which quantifies the
minimum strength of confounding needed to explain away the evidence for ITE.
Our approach rests on the reliable predictive inference of counterfactuals and
ITEs in situations where the training data is confounded. Under the marginal
sensitivity model of Tan (2006), we characterize the shift between the
distribution of the observations and that of the counterfactuals. We first
develop a general method for predictive inference of test samples from a
shifted distribution; we then leverage this to construct covariate-dependent
prediction sets for counterfactuals. No matter the value of the shift, these
prediction sets (resp. approximately) achieve marginal coverage if the
propensity score is known exactly (resp. estimated). We describe a distinct
procedure also attaining coverage, however, conditional on the training data.
In the latter case, we prove a sharpness result showing that for certain
classes of prediction problems, the prediction intervals cannot possibly be
tightened. We verify the validity and performance of the new methods via
simulation studies and apply them to analyze real datasets.",http://arxiv.org/pdf/2111.12161v2,stat.ME
2021-11-17 16:05:58+00:00,Improving Fairness in Criminal Justice Algorithmic Risk Assessments Using Optimal Transport and Conformal Prediction Sets,"['Richard A. Berk', 'Arun Kumar Kuchibhotla', 'Eric Tchetgen Tchetgen']","In the United States and elsewhere, risk assessment algorithms are being used
to help inform criminal justice decision-makers. A common intent is to forecast
an offender's ``future dangerousness.'' Such algorithms have been correctly
criticized for potential unfairness, and there is an active cottage industry
trying to make repairs. In this paper, we use counterfactual reasoning to
consider the prospects for improved fairness when members of a less privileged
group are treated by a risk algorithm as if they are members of a more
privileged group. We combine a machine learning classifier trained in a novel
manner with an optimal transport adjustment for the relevant joint probability
distributions, which together provide a constructive response to claims of
bias-in-bias-out. A key distinction is between fairness claims that are
empirically testable and fairness claims that are not. We then use confusion
tables and conformal prediction sets to evaluate achieved fairness for
projected risk. Our data are a random sample of 300,000 offenders at their
arraignments for a large metropolitan area in the United States during which
decisions to release or detain are made. We show that substantial improvement
in fairness can be achieved consistent with a Pareto improvement for protected
groups.",http://arxiv.org/pdf/2111.09211v2,stat.AP
2021-11-15 18:46:37+00:00,Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking,"['Octavian-Eugen Ganea', 'Xinyuan Huang', 'Charlotte Bunne', 'Yatao Bian', 'Regina Barzilay', 'Tommi Jaakkola', 'Andreas Krause']","Protein complex formation is a central problem in biology, being involved in
most of the cell's processes, and essential for applications, e.g. drug design
or protein engineering. We tackle rigid body protein-protein docking, i.e.,
computationally predicting the 3D structure of a protein-protein complex from
the individual unbound structures, assuming no conformational change within the
proteins happens during binding. We design a novel pairwise-independent
SE(3)-equivariant graph matching network to predict the rotation and
translation to place one of the proteins at the right docked position relative
to the second protein. We mathematically guarantee a basic principle: the
predicted complex is always identical regardless of the initial locations and
orientations of the two structures. Our model, named EquiDock, approximates the
binding pockets and predicts the docking poses using keypoint matching and
alignment, achieved through optimal transport and a differentiable Kabsch
algorithm. Empirically, we achieve significant running time improvements and
often outperform existing docking software despite not relying on heavy
candidate sampling, structure refinement, or templates.",http://arxiv.org/pdf/2111.07786v2,cs.AI
2021-11-04 02:23:05+00:00,Conformal prediction for text infilling and part-of-speech prediction,"['Neil Dey', 'Jing Ding', 'Jack Ferrell', 'Carolina Kapper', 'Maxwell Lovig', 'Emiliano Planchon', 'Jonathan P Williams']","Modern machine learning algorithms are capable of providing remarkably
accurate point-predictions; however, questions remain about their statistical
reliability. Unlike conventional machine learning methods, conformal prediction
algorithms return confidence sets (i.e., set-valued predictions) that
correspond to a given significance level. Moreover, these confidence sets are
valid in the sense that they guarantee finite sample control over type 1 error
probabilities, allowing the practitioner to choose an acceptable error rate. In
our paper, we propose inductive conformal prediction (ICP) algorithms for the
tasks of text infilling and part-of-speech (POS) prediction for natural
language data. We construct new conformal prediction-enhanced bidirectional
encoder representations from transformers (BERT) and bidirectional long
short-term memory (BiLSTM) algorithms for POS tagging and a new conformal
prediction-enhanced BERT algorithm for text infilling. We analyze the
performance of the algorithms in simulations using the Brown Corpus, which
contains over 57,000 sentences. Our results demonstrate that the ICP algorithms
are able to produce valid set-valued predictions that are small enough to be
applicable in real-world applications. We also provide a real data example for
how our proposed set-valued predictions can improve machine generated audio
transcriptions.",http://arxiv.org/pdf/2111.02592v1,stat.ML
2021-10-26 14:26:25+00:00,SE(3) Equivariant Graph Neural Networks with Complete Local Frames,"['Weitao Du', 'He Zhang', 'Yuanqi Du', 'Qi Meng', 'Wei Chen', 'Bin Shao', 'Tie-Yan Liu']","Group equivariance (e.g. SE(3) equivariance) is a critical physical symmetry
in science, from classical and quantum physics to computational biology. It
enables robust and accurate prediction under arbitrary reference
transformations. In light of this, great efforts have been put on encoding this
symmetry into deep neural networks, which has been shown to improve the
generalization performance and data efficiency for downstream tasks.
Constructing an equivariant neural network generally brings high computational
costs to ensure expressiveness. Therefore, how to better trade-off the
expressiveness and computational efficiency plays a core role in the design of
the equivariant deep learning models. In this paper, we propose a framework to
construct SE(3) equivariant graph neural networks that can approximate the
geometric quantities efficiently. Inspired by differential geometry and
physics, we introduce equivariant local complete frames to graph neural
networks, such that tensor information at given orders can be projected onto
the frames. The local frame is constructed to form an orthonormal basis that
avoids direction degeneration and ensure completeness. Since the frames are
built only by cross product operations, our method is computationally
efficient. We evaluate our method on two tasks: Newton mechanics modeling and
equilibrium molecule conformation generation. Extensive experimental results
demonstrate that our model achieves the best or competitive performance in two
types of datasets.",http://arxiv.org/pdf/2110.14811v2,cs.CE
2021-10-25 15:11:32+00:00,Applying Regression Conformal Prediction with Nearest Neighbors to time series data,"['Samya Tajmouati', 'Bouazza El Wahbi', 'Mohammed Dakkoun']","In this paper, we apply conformal prediction to time series data. Conformal
prediction isa method that produces predictive regions given a confidence
level. The regions outputs arealways valid under the exchangeability
assumption. However, this assumption does not holdfor the time series data
because there is a link among past, current, and future
observations.Consequently, the challenge of applying conformal predictors to
the problem of time seriesdata lies in the fact that observations of a time
series are dependent and therefore do notmeet the exchangeability assumption.
This paper aims to present a way of constructingreliable prediction intervals
by using conformal predictors in the context of time series. Weuse the nearest
neighbors method based on the fast parameters tuning technique in theweighted
nearest neighbors (FPTO-WNN) approach as the underlying algorithm. Dataanalysis
demonstrates the effectiveness of the proposed approach.",http://arxiv.org/pdf/2110.13031v1,stat.ME
2021-10-25 02:47:59+00:00,DelightfulTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2021,"['Yanqing Liu', 'Zhihang Xu', 'Gang Wang', 'Kuan Chen', 'Bohan Li', 'Xu Tan', 'Jinzhu Li', 'Lei He', 'Sheng Zhao']","This paper describes the Microsoft end-to-end neural text to speech (TTS)
system: DelightfulTTS for Blizzard Challenge 2021. The goal of this challenge
is to synthesize natural and high-quality speech from text, and we approach
this goal in two perspectives: The first is to directly model and generate
waveform in 48 kHz sampling rate, which brings higher perception quality than
previous systems with 16 kHz or 24 kHz sampling rate; The second is to model
the variation information in speech through a systematic design, which improves
the prosody and naturalness. Specifically, for 48 kHz modeling, we predict 16
kHz mel-spectrogram in acoustic model, and propose a vocoder called HiFiNet to
directly generate 48 kHz waveform from predicted 16 kHz mel-spectrogram, which
can better trade off training efficiency, modelling stability and voice
quality. We model variation information systematically from both explicit
(speaker ID, language ID, pitch and duration) and implicit (utterance-level and
phoneme-level prosody) perspectives: 1) For speaker and language ID, we use
lookup embedding in training and inference; 2) For pitch and duration, we
extract the values from paired text-speech data in training and use two
predictors to predict the values in inference; 3) For utterance-level and
phoneme-level prosody, we use two reference encoders to extract the values in
training, and use two separate predictors to predict the values in inference.
Additionally, we introduce an improved Conformer block to better model the
local and global dependency in acoustic model. For task SH1, DelightfulTTS
achieves 4.17 mean score in MOS test and 4.35 in SMOS test, which indicates the
effectiveness of our proposed system",http://arxiv.org/pdf/2110.12612v2,cs.SD
2021-10-19 11:32:17+00:00,Learning Pareto-Efficient Decisions with Confidence,"['Sofia Ek', 'Dave Zachariah', 'Petre Stoica']","The paper considers the problem of multi-objective decision support when
outcomes are uncertain. We extend the concept of Pareto-efficient decisions to
take into account the uncertainty of decision outcomes across varying contexts.
This enables quantifying trade-offs between decisions in terms of tail outcomes
that are relevant in safety-critical applications. We propose a method for
learning efficient decisions with statistical confidence, building on results
from the conformal prediction literature. The method adapts to weak or
nonexistent context covariate overlap and its statistical guarantees are
evaluated using both synthetic and real data.",http://arxiv.org/pdf/2110.09864v1,stat.ML
2021-10-18 11:25:33+00:00,Learning Optimal Conformal Classifiers,"['David Stutz', 'Krishnamurthy', 'Dvijotham', 'Ali Taylan Cemgil', 'Arnaud Doucet']","Modern deep learning based classifiers show very high accuracy on test data
but this does not provide sufficient guarantees for safe deployment, especially
in high-stake AI applications such as medical diagnosis. Usually, predictions
are obtained without a reliable uncertainty estimate or a formal guarantee.
Conformal prediction (CP) addresses these issues by using the classifier's
predictions, e.g., its probability estimates, to predict confidence sets
containing the true class with a user-specified probability. However, using CP
as a separate processing step after training prevents the underlying model from
adapting to the prediction of confidence sets. Thus, this paper explores
strategies to differentiate through CP during training with the goal of
training model with the conformal wrapper end-to-end. In our approach,
conformal training (ConfTr), we specifically ""simulate"" conformalization on
mini-batches during training. Compared to standard training, ConfTr reduces the
average confidence set size (inefficiency) of state-of-the-art CP methods
applied after training. Moreover, it allows to ""shape"" the confidence sets
predicted at test time, which is difficult for standard CP. On experiments with
several datasets, we show ConfTr can influence how inefficiency is distributed
across classes, or guide the composition of confidence sets in terms of the
included classes, while retaining the guarantees offered by CP.",http://arxiv.org/pdf/2110.09192v3,cs.LG
2021-10-14 18:41:17+00:00,Distribution-Free Federated Learning with Conformal Predictions,"['Charles Lu', 'Jayasheree Kalpathy-Cramer']","Federated learning has attracted considerable interest for collaborative
machine learning in healthcare to leverage separate institutional datasets
while maintaining patient privacy. However, additional challenges such as poor
calibration and lack of interpretability may also hamper widespread deployment
of federated models into clinical practice, leading to user distrust or misuse
of ML tools in high-stakes clinical decision-making. In this paper, we propose
to address these challenges by incorporating an adaptive conformal framework
into federated learning to ensure distribution-free prediction sets that
provide coverage guarantees. Importantly, these uncertainty estimates can be
obtained without requiring any additional modifications to the model. Empirical
results on the MedMNIST medical imaging benchmark demonstrate our federated
method provides tighter coverage over local conformal predictions on 6
different medical imaging datasets for 2D and 3D multi-class classification
tasks. Furthermore, we correlate class entropy with prediction set size to
assess task uncertainty.",http://arxiv.org/pdf/2110.07661v2,cs.LG
2021-10-14 13:45:55+00:00,Distribution-Free Bayesian multivariate predictive inference,['Daniel Yekutieli'],"We introduce a comprehensive Bayesian multivariate predictive inference
framework. The basis for our framework is a hierarchical Bayesian model, that
is a mixture of finite Polya trees corresponding to multiple dyadic partitions
of the unit cube. Given a sample of observations from an unknown multivariate
distribution, the posterior predictive distribution is used to model and
generate future observations from the unknown distribution. We illustrate the
implementation of our methodology and study its performance on simulated
examples. We introduce an algorithm for constructing conformal prediction sets,
that provide finite sample probability assurances for future observations, with
our Bayesian model.",http://arxiv.org/pdf/2110.07361v1,stat.ME
2021-10-13 13:46:23+00:00,On the Parameter Combinations That Matter and on Those That do Not,"['Nikolaos Evangelou', 'Noah J. Wichrowski', 'George A. Kevrekidis', 'Felix Dietrich', 'Mahdi Kooshkbaghi', 'Sarah McFann', 'Ioannis G. Kevrekidis']","We present a data-driven approach to characterizing nonidentifiability of a
model's parameters and illustrate it through dynamic as well as steady kinetic
models. By employing Diffusion Maps and their extensions, we discover the
minimal combinations of parameters required to characterize the output behavior
of a chemical system: a set of effective parameters for the model. Furthermore,
we introduce and use a Conformal Autoencoder Neural Network technique, as well
as a kernel-based Jointly Smooth Function technique, to disentangle the
redundant parameter combinations that do not affect the output behavior from
the ones that do. We discuss the interpretability of our data-driven effective
parameters, and demonstrate the utility of the approach both for behavior
prediction and parameter estimation. In the latter task, it becomes important
to describe level sets in parameter space that are consistent with a particular
output behavior. We validate our approach on a model of multisite
phosphorylation, where a reduced set of effective parameters (nonlinear
combinations of the physical ones) has previously been established
analytically.",http://arxiv.org/pdf/2110.06717v2,cs.LG
2021-10-11 07:52:28+00:00,Controllable Recommenders using Deep Generative Models and Disentanglement,"['Samarth Bhargav', 'Evangelos Kanoulas']","In this paper, we consider controllability as a means to satisfy dynamic
preferences of users, enabling them to control recommendations such that their
current preference is met. While deep models have shown improved performance
for collaborative filtering, they are generally not amenable to fine grained
control by a user, leading to the development of methods like deep language
critiquing. We propose an alternate view, where instead of keyphrase based
critiques, a user is provided 'knobs' in a disentangled latent space, with each
knob corresponding to an item aspect. Disentanglement here refers to a latent
space where generative factors (here, a preference towards an item category
like genre) are captured independently in their respective dimensions, thereby
enabling predictable manipulations, otherwise not possible in an entangled
space. We propose using a (semi-)supervised disentanglement objective for this
purpose, as well as multiple metrics to evaluate the controllability and the
degree of personalization of controlled recommendations. We show that by
updating the disentangled latent space based on user feedback, and by
exploiting the generative nature of the recommender, controlled and
personalized recommendations can be produced. Through experiments on two widely
used collaborative filtering datasets, we demonstrate that a controllable
recommender can be trained with a slight reduction in recommender performance,
provided enough supervision is provided. The recommendations produced by these
models appear to both conform to a user's current preference and remain
personalized.",http://arxiv.org/pdf/2110.05056v1,cs.IR
2021-10-08 21:25:47+00:00,Learning 3D Representations of Molecular Chirality with Invariance to Bond Rotations,"['Keir Adams', 'Lagnajit Pattanaik', 'Connor W. Coley']","Molecular chirality, a form of stereochemistry most often describing relative
spatial arrangements of bonded neighbors around tetrahedral carbon centers,
influences the set of 3D conformers accessible to the molecule without changing
its 2D graph connectivity. Chirality can strongly alter (bio)chemical
interactions, particularly protein-drug binding. Most 2D graph neural networks
(GNNs) designed for molecular property prediction at best use atomic labels to
na\""ively treat chirality, while E(3)-invariant 3D GNNs are invariant to
chirality altogether. To enable representation learning on molecules with
defined stereochemistry, we design an SE(3)-invariant model that processes
torsion angles of a 3D molecular conformer. We explicitly model conformational
flexibility by integrating a novel type of invariance to rotations about
internal molecular bonds into the architecture, mitigating the need for
multi-conformer data augmentation. We test our model on four benchmarks:
contrastive learning to distinguish conformers of different stereoisomers in a
learned latent space, classification of chiral centers as R/S, prediction of
how enantiomers rotate circularly polarized light, and ranking enantiomers by
their docking scores in an enantiosensitive protein pocket. We compare our
model, Chiral InterRoto-Invariant Neural Network (ChIRo), with 2D and 3D GNNs
to demonstrate that our model achieves state of the art performance when
learning chiral-sensitive functions from molecular structures.",http://arxiv.org/pdf/2110.04383v1,cs.LG
2021-10-07 03:16:15+00:00,StrengthNet: Deep Learning-based Emotion Strength Assessment for Emotional Speech Synthesis,"['Rui Liu', 'Berrak Sisman', 'Haizhou Li']","Recently, emotional speech synthesis has achieved remarkable performance. The
emotion strength of synthesized speech can be controlled flexibly using a
strength descriptor, which is obtained by an emotion attribute ranking
function. However, a trained ranking function on specific data has poor
generalization, which limits its applicability for more realistic cases. In
this paper, we propose a deep learning based emotion strength assessment
network for strength prediction that is referred to as StrengthNet. Our model
conforms to a multi-task learning framework with a structure that includes an
acoustic encoder, a strength predictor and an auxiliary emotion predictor. A
data augmentation strategy was utilized to improve the model generalization.
Experiments show that the predicted emotion strength of the proposed
StrengthNet are highly correlated with ground truth scores for seen and unseen
speech. Our codes are available at: https://github.com/ttslr/StrengthNet.",http://arxiv.org/pdf/2110.03156v2,cs.SD
2021-10-07 00:40:34+00:00,Improving Prediction Confidence in Learning-Enabled Autonomous Systems,"['Dimitrios Boursinos', 'Xenofon Koutsoukos']","Autonomous systems use extensively learning-enabled components such as deep
neural networks (DNNs) for prediction and decision making. In this paper, we
utilize a feedback loop between learning-enabled components used for
classification and the sensors of an autonomous system in order to improve the
confidence of the predictions. We design a classifier using Inductive Conformal
Prediction (ICP) based on a triplet network architecture in order to learn
representations that can be used to quantify the similarity between test and
training examples. The method allows computing confident set predictions with
an error rate predefined using a selected significance level. A feedback loop
that queries the sensors for a new input is used to further refine the
predictions and increase the classification accuracy. The method is
computationally efficient, scalable to high-dimensional inputs, and can be
executed in a feedback loop with the system in real-time. The approach is
evaluated using a traffic sign recognition dataset and the results show that
the error rate is reduced.",http://arxiv.org/pdf/2110.03123v1,cs.LG
2021-10-07 00:21:45+00:00,Assurance Monitoring of Learning Enabled Cyber-Physical Systems Using Inductive Conformal Prediction based on Distance Learning,"['Dimitrios Boursinos', 'Xenofon Koutsoukos']","Machine learning components such as deep neural networks are used extensively
in Cyber-Physical Systems (CPS). However, such components may introduce new
types of hazards that can have disastrous consequences and need to be addressed
for engineering trustworthy systems. Although deep neural networks offer
advanced capabilities, they must be complemented by engineering methods and
practices that allow effective integration in CPS. In this paper, we proposed
an approach for assurance monitoring of learning-enabled CPS based on the
conformal prediction framework. In order to allow real-time assurance
monitoring, the approach employs distance learning to transform
high-dimensional inputs into lower size embedding representations. By
leveraging conformal prediction, the approach provides well-calibrated
confidence and ensures a bounded small error rate while limiting the number of
inputs for which an accurate prediction cannot be made. We demonstrate the
approach using three data sets of mobile robot following a wall, speaker
recognition, and traffic sign recognition. The experimental results demonstrate
that the error rates are well-calibrated while the number of alarms is very
small. Further, the method is computationally efficient and allows real-time
assurance monitoring of CPS.",http://arxiv.org/pdf/2110.03120v1,cs.LG
2021-10-02 14:50:15+00:00,Calibrated Multiple-Output Quantile Regression with Representation Learning,"['Shai Feldman', 'Stephen Bates', 'Yaniv Romano']","We develop a method to generate predictive regions that cover a multivariate
response variable with a user-specified probability. Our work is composed of
two components. First, we use a deep generative model to learn a representation
of the response that has a unimodal distribution. Existing multiple-output
quantile regression approaches are effective in such cases, so we apply them on
the learned representation, and then transform the solution to the original
space of the response. This process results in a flexible and informative
region that can have an arbitrary shape, a property that existing methods lack.
Second, we propose an extension of conformal prediction to the multivariate
response setting that modifies any method to return sets with a pre-specified
coverage level. The desired coverage is theoretically guaranteed in the
finite-sample case for any distribution. Experiments conducted on both real and
synthetic data show that our method constructs regions that are significantly
smaller compared to existing techniques.",http://arxiv.org/pdf/2110.00816v2,cs.LG
2021-09-28 23:00:30+00:00,Sample-Efficient Safety Assurances using Conformal Prediction,"['Rachel Luo', 'Shengjia Zhao', 'Jonathan Kuck', 'Boris Ivanovic', 'Silvio Savarese', 'Edward Schmerling', 'Marco Pavone']","When deploying machine learning models in high-stakes robotics applications,
the ability to detect unsafe situations is crucial. Early warning systems can
provide alerts when an unsafe situation is imminent (in the absence of
corrective action). To reliably improve safety, these warning systems should
have a provable false negative rate; i.e. of the situations that are unsafe,
fewer than $\epsilon$ will occur without an alert. In this work, we present a
framework that combines a statistical inference technique known as conformal
prediction with a simulator of robot/environment dynamics, in order to tune
warning systems to provably achieve an $\epsilon$ false negative rate using as
few as $1/\epsilon$ data points. We apply our framework to a driver warning
system and a robotic grasping application, and empirically demonstrate
guaranteed false negative rate while also observing low false detection
(positive) rate.",http://arxiv.org/pdf/2109.14082v4,cs.RO
2021-09-27 00:02:52+00:00,Anomalous Edge Detection in Edge Exchangeable Social Network Models,"['Rui Luo', 'Buddhika Nettasinghe', 'Vikram Krishnamurthy']","This paper studies detecting anomalous edges in directed graphs that model
social networks. We exploit edge exchangeability as a criterion for
distinguishing anomalous edges from normal edges. Then we present an anomaly
detector based on conformal prediction theory; this detector has a guaranteed
upper bound for false positive rate. In numerical experiments, we show that the
proposed algorithm achieves superior performance to baseline methods.",http://arxiv.org/pdf/2109.12727v2,cs.SI
2021-09-26 20:52:20+00:00,Dynamic Risk Assessment for Geologic CO2 Sequestration,"['Bailian Chen', 'Dylan R. Harp', 'Yingqi Zhang', 'Curtis M. Oldenburg', 'Rajesh J. Pawar']","At a geologic CO2 sequestration (GCS) site, geologic uncertainty usually
leads to large uncertainty in the predictions of properties that influence
metrics for leakage risk assessment, such as CO2 saturations and pressures in
potentially leaky wellbores, CO2/brine leakage rates, and leakage consequences
such as changes in drinking water quality in groundwater aquifers. The large
uncertainty in these risk-related system properties and risk metrics can lead
to over-conservative risk management decisions to ensure safe operations of GCS
sites. The objective of this work is to develop a novel approach based on
dynamic risk assessment to effectively reduce the uncertainty in the predicted
risk-related system properties and risk metrics. We demonstrate our framework
for dynamic risk assessment on two case studies: a 3D synthetic example and a
synthetic field example based on the Rock Springs Uplift (RSU) storage site in
Wyoming, USA. Results show that the NRAP-Open-IAM risk assessment tool coupled
with a conformance evaluation can be used to effectively quantify and reduce
the uncertainty in the predictions of risk-related system properties and risk
metrics in GCS.",http://arxiv.org/pdf/2109.12703v1,cs.IT
2021-09-06 18:37:53+00:00,Towards Reusable Surrogate Models: Graph-Based Transfer Learning on Trusses,"['Eamon Whalen', 'Caitlin Mueller']","Surrogate models have several uses in engineering design, including speeding
up design optimization, noise reduction, test measurement interpolation,
gradient estimation, portability, and protection of intellectual property.
Traditionally, surrogate models require that all training data conform to the
same parametrization (e.g. design variables), limiting design freedom and
prohibiting the reuse of historical data. In response, this paper proposes
Graph-based Surrogate Models (GSMs) for trusses. The GSM can accurately predict
displacement fields from static loads given the structure's geometry as input,
enabling training across multiple parametrizations. GSMs build upon recent
advancements in geometric deep learning which have led to the ability to learn
on undirected graphs: a natural representation for trusses. To further promote
flexible surrogate models, the paper explores transfer learning within the
context of engineering design, and demonstrates positive knowledge transfer
across data sets of different topologies, complexities, loads and applications,
resulting in more flexible and data-efficient surrogate models for trusses.",http://arxiv.org/pdf/2109.02689v1,cs.CE
2021-08-25 10:43:59+00:00,Hybrid Planning with Receding Horizon: A Case for Meta-self-awareness,"['Sona Ghahremani', 'Holger Giese']","The trade-off between the quality and timeliness of adaptation is a
multi-faceted challenge in engineering self-adaptive systems. Obtaining
adaptation plans that fulfill system objectives with high utility and in a
timely manner is the holy grail, however, as recent research revealed, it is
not trivial. Hybrid planning is concerned with resolving the time and quality
trade-off via dynamically combining multiple planners that individually aim to
perform either timely or with high quality. The choice of the most fitting
planner is steered based on assessments of runtime information. A hybrid
planner for a self-adaptive system requires (i) a decision-making mechanism
that utilizes (ii) system-level as well as (iii) feedback control-level
information at runtime.
  In this paper, we present HYPEZON, a hybrid planner for self-adaptive
systems. Inspired by model predictive control, HYPEZON leverages receding
horizon control to utilize runtime information during its decision-making.
Moreover, we propose to engineer HYPEZON for self-adaptive systems via two
alternative designs that conform to meta-self-aware architectures.
Meta-self-awareness allows for obtaining knowledge and reasoning about own
awareness via adding a higher-level reasoning entity. HYPEZON aims to address
the problem of hybrid planning by considering it as a case for
meta-self-awareness.",http://arxiv.org/pdf/2108.11170v1,cs.SE
2021-08-25 05:54:56+00:00,Social Norm Bias: Residual Harms of Fairness-Aware Algorithms,"['Myra Cheng', 'Maria De-Arteaga', 'Lester Mackey', 'Adam Tauman Kalai']","Many modern machine learning algorithms mitigate bias by enforcing fairness
constraints across coarsely-defined groups related to a sensitive attribute
like gender or race. However, these algorithms seldom account for within-group
heterogeneity and biases that may disproportionately affect some members of a
group. In this work, we characterize Social Norm Bias (SNoB), a subtle but
consequential type of algorithmic discrimination that may be exhibited by
machine learning models, even when these systems achieve group fairness
objectives. We study this issue through the lens of gender bias in occupation
classification. We quantify SNoB by measuring how an algorithm's predictions
are associated with conformity to inferred gender norms. When predicting if an
individual belongs to a male-dominated occupation, this framework reveals that
""fair"" classifiers still favor biographies written in ways that align with
inferred masculine norms. We compare SNoB across algorithmic fairness methods
and show that it is frequently a residual bias, and post-processing approaches
do not mitigate this type of bias at all.",http://arxiv.org/pdf/2108.11056v3,cs.LG
2021-08-22 09:32:15+00:00,Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer,['Krishna D N'],"Transformers have recently become very popular for sequence-to-sequence
applications such as machine translation and speech recognition. In this work,
we propose a multi-task learning-based transformer model for low-resource
multilingual speech recognition for Indian languages. Our proposed model
consists of a conformer [1] encoder and two parallel transformer decoders. We
use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme
decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme
recognition task as an auxiliary task for our multi-task learning framework. We
jointly optimize the network for both phoneme and grapheme recognition tasks
using Joint CTC-Attention [2] training. We use a conditional decoding scheme to
inject the language information into the model before predicting the grapheme
sequence. Our experiments show that our proposed approach can obtain
significant improvement over previous approaches [4]. We also show that our
conformer-based dual-decoder approach outperforms both the transformer-based
dual-decoder approach and single decoder approach. Finally, We compare
monolingual ASR models with our proposed multilingual ASR approach.",http://arxiv.org/pdf/2109.03969v2,cs.CL
2021-08-22 09:22:28+00:00,A Dual-Decoder Conformer for Multilingual Speech Recognition,['Krishna D N'],"Transformer-based models have recently become very popular for
sequence-to-sequence applications such as machine translation and speech
recognition. This work proposes a dual-decoder transformer model for
low-resource multilingual speech recognition for Indian languages. Our proposed
model consists of a Conformer [1] encoder, two parallel transformer decoders,
and a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme
recognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence
along with language information. We consider phoneme recognition and language
identification as auxiliary tasks in the multi-task learning framework. We
jointly optimize the network for phoneme recognition, grapheme recognition, and
language identification tasks with Joint CTC-Attention [2] training. Our
experiments show that we can obtain a significant reduction in WER over the
baseline approaches. We also show that our dual-decoder approach obtains
significant improvement over the single decoder approach.",http://arxiv.org/pdf/2109.03277v1,cs.CL
2021-08-16 15:08:20+00:00,Neural Predictive Monitoring under Partial Observability,"['Francesca Cairoli', 'Luca Bortolussi', 'Nicola Paoletti']","We consider the problem of predictive monitoring (PM), i.e., predicting at
runtime future violations of a system from the current state. We work under the
most realistic settings where only partial and noisy observations of the state
are available at runtime. Such settings directly affect the accuracy and
reliability of the reachability predictions, jeopardizing the safety of the
system. In this work, we present a learning-based method for PM that produces
accurate and reliable reachability predictions despite partial observability
(PO). We build on Neural Predictive Monitoring (NPM), a PM method that uses
deep neural networks for approximating hybrid systems reachability, and extend
it to the PO case. We propose and compare two solutions, an end-to-end
approach, which directly operates on the rough observations, and a two-step
approach, which introduces an intermediate state estimation step. Both
solutions rely on conformal prediction to provide 1) probabilistic guarantees
in the form of prediction regions and 2) sound estimates of predictive
uncertainty. We use the latter to identify unreliable (and likely erroneous)
predictions and to retrain and improve the monitors on these uncertain inputs
(i.e., active learning). Our method results in highly accurate reachability
predictions and error detection, as well as tight prediction regions with
guaranteed coverage.",http://arxiv.org/pdf/2108.07134v2,cs.LG
2021-08-12 11:50:12+00:00,How Nonconformity Functions and Difficulty of Datasets Impact the Efficiency of Conformal Classifiers,"['Marharyta Aleksandrova', 'Oleg Chertov']","The property of conformal predictors to guarantee the required accuracy rate
makes this framework attractive in various practical applications. However,
this property is achieved at a price of reduction in precision. In the case of
conformal classification, the systems can output multiple class labels instead
of one. It is also known from the literature, that the choice of nonconformity
function has a major impact on the efficiency of conformal classifiers.
Recently, it was shown that different model-agnostic nonconformity functions
result in conformal classifiers with different characteristics. For a Neural
Network-based conformal classifier, the inverse probability (or hinge loss)
allows minimizing the average number of predicted labels, and margin results in
a larger fraction of singleton predictions. In this work, we aim to further
extend this study. We perform an experimental evaluation using 8 different
classification algorithms and discuss when the previously observed relationship
holds or not. Additionally, we propose a successful method to combine the
properties of these two nonconformity functions. The experimental evaluation is
done using 11 real and 5 synthetic datasets.",http://arxiv.org/pdf/2108.05677v1,cs.LG
2021-08-08 15:27:14+00:00,TDLS: A Top-Down Layer Searching Algorithm for Generating Counterfactual Visual Explanation,"['Cong Wang', 'Haocheng Han', 'Caleb Chen Cao']","Explanation of AI, as well as fairness of algorithms' decisions and the
transparency of the decision model, are becoming more and more important. And
it is crucial to design effective and human-friendly techniques when opening
the black-box model. Counterfactual conforms to the human way of thinking and
provides a human-friendly explanation, and its corresponding explanation
algorithm refers to a strategic alternation of a given data point so that its
model output is ""counter-facted"", i.e. the prediction is reverted. In this
paper, we adapt counterfactual explanation over fine-grained image
classification problem. We demonstrated an adaptive method that could give a
counterfactual explanation by showing the composed counterfactual feature map
using top-down layer searching algorithm (TDLS). We have proved that our TDLS
algorithm could provide more flexible counterfactual visual explanation in an
efficient way using VGG-16 model on Caltech-UCSD Birds 200 dataset. At the end,
we discussed several applicable scenarios of counterfactual visual
explanations.",http://arxiv.org/pdf/2108.04238v2,cs.CV
2021-08-07 06:29:36+00:00,W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training,"['Yu-An Chung', 'Yu Zhang', 'Wei Han', 'Chung-Cheng Chiu', 'James Qin', 'Ruoming Pang', 'Yonghui Wu']","Motivated by the success of masked language modeling~(MLM) in pre-training
natural language processing models, we propose w2v-BERT that explores MLM for
self-supervised speech representation learning. w2v-BERT is a framework that
combines contrastive learning and MLM, where the former trains the model to
discretize input continuous speech signals into a finite set of discriminative
speech tokens, and the latter trains the model to learn contextualized speech
representations via solving a masked prediction task consuming the discretized
tokens. In contrast to existing MLM-based speech pre-training frameworks such
as HuBERT, which relies on an iterative re-clustering and re-training process,
or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can
be optimized in an end-to-end fashion by solving the two self-supervised
tasks~(the contrastive task and MLM) simultaneously. Our experiments show that
w2v-BERT achieves competitive results compared to current state-of-the-art
pre-trained models on the LibriSpeech benchmarks when using the Libri-Light~60k
corpus as the unsupervised data. In particular, when compared to published
models such as conformer-based wav2vec~2.0 and HuBERT, our model shows~5\%
to~10\% relative WER reduction on the test-clean and test-other subsets. When
applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our
internal conformer-based wav2vec~2.0 by more than~30\% relatively.",http://arxiv.org/pdf/2108.06209v2,cs.LG
2021-07-25 15:07:24+00:00,Graph-free Multi-hop Reading Comprehension: A Select-to-Guide Strategy,"['Bohong Wu', 'Zhuosheng Zhang', 'Hai Zhao']","Multi-hop reading comprehension (MHRC) requires not only to predict the
correct answer span in the given passage, but also to provide a chain of
supporting evidences for reasoning interpretability. It is natural to model
such a process into graph structure by understanding multi-hop reasoning as
jumping over entity nodes, which has made graph modelling dominant on this
task. Recently, there have been dissenting voices about whether graph modelling
is indispensable due to the inconvenience of the graph building, however
existing state-of-the-art graph-free attempts suffer from huge performance gap
compared to graph-based ones. This work presents a novel graph-free alternative
which firstly outperform all graph models on MHRC. In detail, we exploit a
select-to-guide (S2G) strategy to accurately retrieve evidence paragraphs in a
coarse-to-fine manner, incorporated with two novel attention mechanisms, which
surprisingly shows conforming to the nature of multi-hop reasoning. Our
graph-free model achieves significant and consistent performance gain over
strong baselines and the current new state-of-the-art on the MHRC benchmark,
HotpotQA, among all the published works.",http://arxiv.org/pdf/2107.11823v1,cs.CL
2021-07-15 17:59:50+00:00,A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification,"['Anastasios N. Angelopoulos', 'Stephen Bates']","Black-box machine learning models are now routinely used in high-risk
settings, like medical diagnostics, which demand uncertainty quantification to
avoid consequential model failures. Conformal prediction is a user-friendly
paradigm for creating statistically rigorous uncertainty sets/intervals for the
predictions of such models. Critically, the sets are valid in a
distribution-free sense: they possess explicit, non-asymptotic guarantees even
without distributional assumptions or model assumptions. One can use conformal
prediction with any pre-trained model, such as a neural network, to produce
sets that are guaranteed to contain the ground truth with a user-specified
probability, such as 90%. It is easy-to-understand, easy-to-use, and general,
applying naturally to problems arising in the fields of computer vision,
natural language processing, deep reinforcement learning, and so on.
  This hands-on introduction is aimed to provide the reader a working
understanding of conformal prediction and related distribution-free uncertainty
quantification techniques with one self-contained document. We lead the reader
through practical theory for and examples of conformal prediction and describe
its extensions to complex machine learning tasks involving structured outputs,
distribution shift, time-series, outliers, models that abstain, and more.
Throughout, there are many explanatory illustrations, examples, and code
samples in Python. With each code sample comes a Jupyter notebook implementing
the method on a real-data example; the notebooks can be accessed and easily run
using our codebase.",http://arxiv.org/pdf/2107.07511v6,cs.LG
2021-07-14 09:26:36+00:00,A Note on Learning Rare Events in Molecular Dynamics using LSTM and Transformer,"['Wenqi Zeng', 'Siqin Cao', 'Xuhui Huang', 'Yuan Yao']","Recurrent neural networks for language models like long short-term memory
(LSTM) have been utilized as a tool for modeling and predicting long term
dynamics of complex stochastic molecular systems. Recently successful examples
on learning slow dynamics by LSTM are given with simulation data of low
dimensional reaction coordinate. However, in this report we show that the
following three key factors significantly affect the performance of language
model learning, namely dimensionality of reaction coordinates, temporal
resolution and state partition. When applying recurrent neural networks to
molecular dynamics simulation trajectories of high dimensionality, we find that
rare events corresponding to the slow dynamics might be obscured by other
faster dynamics of the system, and cannot be efficiently learned. Under such
conditions, we find that coarse graining the conformational space into
metastable states and removing recrossing events when estimating transition
probabilities between states could greatly help improve the accuracy of slow
dynamics learning in molecular dynamics. Moreover, we also explore other models
like Transformer, which do not show superior performance than LSTM in
overcoming these issues. Therefore, to learn rare events of slow molecular
dynamics by LSTM and Transformer, it is critical to choose proper temporal
resolution (i.e., saving intervals of MD simulation trajectories) and state
partition in high resolution data, since deep neural network models might not
automatically disentangle slow dynamics from fast dynamics when both are
present in data influencing each other.",http://arxiv.org/pdf/2107.06573v1,cs.AI
2021-07-13 07:53:01+00:00,Speech Representation Learning Combining Conformer CPC with Deep Cluster for the ZeroSpeech Challenge 2021,"['Takashi Maekaku', 'Xuankai Chang', 'Yuya Fujita', 'Li-Wei Chen', 'Shinji Watanabe', 'Alexander Rudnicky']","We present a system for the Zero Resource Speech Challenge 2021, which
combines a Contrastive Predictive Coding (CPC) with deep cluster. In deep
cluster, we first prepare pseudo-labels obtained by clustering the outputs of a
CPC network with k-means. Then, we train an additional autoregressive model to
classify the previously obtained pseudo-labels in a supervised manner. Phoneme
discriminative representation is achieved by executing the second-round
clustering with the outputs of the final layer of the autoregressive model. We
show that replacing a Transformer layer with a Conformer layer leads to a
further gain in a lexical metric. Experimental results show that a relative
improvement of 35% in a phonetic metric, 1.5% in the lexical metric, and 2.3%
in a syntactic metric are achieved compared to a baseline method of CPC-small
which is trained on LibriSpeech 460h data. We achieve top results in this
challenge with the syntactic metric.",http://arxiv.org/pdf/2107.05899v2,cs.SD
2021-07-09 03:48:14+00:00,Probabilistic Trajectory Prediction with Structural Constraints,"['Weiming Zhi', 'Lionel Ott', 'Fabio Ramos']","This work addresses the problem of predicting the motion trajectories of
dynamic objects in the environment. Recent advances in predicting motion
patterns often rely on machine learning techniques to extrapolate motion
patterns from observed trajectories, with no mechanism to directly incorporate
known rules. We propose a novel framework, which combines probabilistic
learning and constrained trajectory optimisation. The learning component of our
framework provides a distribution over future motion trajectories conditioned
on observed past coordinates. This distribution is then used as a prior to a
constrained optimisation problem which enforces chance constraints on the
trajectory distribution. This results in constraint-compliant trajectory
distributions which closely resemble the prior. In particular, we focus our
investigation on collision constraints, such that extrapolated future
trajectory distributions conform to the environment structure. We empirically
demonstrate on real-world and simulated datasets the ability of our framework
to learn complex probabilistic motion trajectories for motion data, while
directly enforcing constraints to improve generalisability, producing more
robust and higher quality trajectory distributions.",http://arxiv.org/pdf/2107.04193v1,cs.RO
2021-07-08 03:27:51+00:00,"Moore's law, Wright's law and the Countdown to Exponential Space","['Daniel Berleant', 'Venkat Kodali', 'Richard Segall', 'Hyacinthe Aboudja', 'Michael Howell']","Technologies have often been observed to improve exponentially over time. In
practice this often means identifying a constant known as the doubling time,
describing the time period over which the technology roughly doubles in some
measure of performance or of performance per dollar. Moore's law is,
classically, the empirical observation that the number of electronic components
that can be put on a chip doubles every 18 months to 2 years. Today it is
frequently stated as the number of computations available per unit of cost.
Generalized to the appropriate doubling time, it describes the rate of
advancement in many technologies. A frequently noted competitor to Moore's law
is known as Wright's law, which has aeronautical roots. Wright's law (also
called power law, experience curve and Henderson's law) relates some quality of
a manufactured unit (for Wright, airplanes) to the volume of units
manufactured. The Wright's law equation expresses the idea that performance -
price or a quality metric - improves according to a power of the number
produced, or alternatively stated, improves by a constant percentage for every
doubling of the total number produced.
  Does exploration of outer space conform to Moore's law or Wright's law-like
behavior? Our results below are broadly consistent with these laws. This is
true for many technologies. Although the two laws can make somewhat different
predictions, Sahal found that they converge to the same predictions when
manufacturing volume increases exponentially over time. When space exploration
transitions into an independent commercial sector, as many people hope and
expect, spacecraft technology will then likely enter an era of unambiguously
exponential advancement.",http://arxiv.org/pdf/2107.09637v1,q-fin.GN
2021-07-07 15:19:16+00:00,MD-split+: Practical Local Conformal Inference in High Dimensions,"['Benjamin LeRoy', 'David Zhao']","Quantifying uncertainty in model predictions is a common goal for
practitioners seeking more than just point predictions. One tool for
uncertainty quantification that requires minimal assumptions is conformal
inference, which can help create probabilistically valid prediction regions for
black box models. Classical conformal prediction only provides marginal
validity, whereas in many situations locally valid prediction regions are
desirable. Deciding how best to partition the feature space X when applying
localized conformal prediction is still an open question. We present MD-split+,
a practical local conformal approach that creates X partitions based on
localized model performance of conditional density estimation models. Our
method handles complex real-world data settings where such models may be
misspecified, and scales to high-dimensional inputs. We discuss how our local
partitions philosophically align with expected behavior from an unattainable
conditional conformal inference approach. We also empirically compare our
method against other local conformal approaches.",http://arxiv.org/pdf/2107.03280v1,stat.ML
2021-07-06 16:36:57+00:00,Finding Significant Features for Few-Shot Learning using Dimensionality Reduction,"['Mauricio Mendez-Ruiz', 'Ivan Garcia Jorge Gonzalez-Zapata', 'Gilberto Ochoa-Ruiz', 'Andres Mendez-Vazquez']","Few-shot learning is a relatively new technique that specializes in problems
where we have little amounts of data. The goal of these methods is to classify
categories that have not been seen before with just a handful of samples.
Recent approaches, such as metric learning, adopt the meta-learning strategy in
which we have episodic tasks conformed by support (training) data and query
(test) data. Metric learning methods have demonstrated that simple models can
achieve good performance by learning a similarity function to compare the
support and the query data. However, the feature space learned by a given
metric learning approach may not exploit the information given by a specific
few-shot task. In this work, we explore the use of dimension reduction
techniques as a way to find task-significant features helping to make better
predictions. We measure the performance of the reduced features by assigning a
score based on the intra-class and inter-class distance, and selecting a
feature reduction method in which instances of different classes are far away
and instances of the same class are close. This module helps to improve the
accuracy performance by allowing the similarity function, given by the metric
learning method, to have more discriminative features for the classification.
Our method outperforms the metric learning baselines in the miniImageNet
dataset by around 2% in accuracy performance.",http://arxiv.org/pdf/2107.06992v1,cs.LG
2021-07-06 13:20:14+00:00,Stateless actor-critic for instance segmentation with high-level priors,"['Paul Hilt', 'Maedeh Zarvandi', 'Edgar Kaziakhmedov', 'Sourabh Bhide', 'Maria Leptin', 'Constantin Pape', 'Anna Kreshuk']","Instance segmentation is an important computer vision problem which remains
challenging despite impressive recent advances due to deep learning-based
methods. Given sufficient training data, fully supervised methods can yield
excellent performance, but annotation of ground-truth data remains a major
bottleneck, especially for biomedical applications where it has to be performed
by domain experts. The amount of labels required can be drastically reduced by
using rules derived from prior knowledge to guide the segmentation. However,
these rules are in general not differentiable and thus cannot be used with
existing methods. Here, we relax this requirement by using stateless actor
critic reinforcement learning, which enables non-differentiable rewards. We
formulate the instance segmentation problem as graph partitioning and the actor
critic predicts the edge weights driven by the rewards, which are based on the
conformity of segmented instances to high-level priors on object shape,
position or size. The experiments on toy and real datasets demonstrate that we
can achieve excellent performance without any direct supervision based only on
a rich set of priors.",http://arxiv.org/pdf/2107.02600v2,cs.CV
2021-07-06 07:45:23+00:00,The NiuTrans End-to-End Speech Translation System for IWSLT 2021 Offline Task,"['Chen Xu', 'Xiaoqian Liu', 'Xiaowen Liu', 'Laohu Wang', 'Canan Huang', 'Tong Xiao', 'Jingbo Zhu']","This paper describes the submission of the NiuTrans end-to-end speech
translation system for the IWSLT 2021 offline task, which translates from the
English audio to German text directly without intermediate transcription. We
use the Transformer-based model architecture and enhance it by Conformer,
relative position encoding, and stacked acoustic and textual encoding. To
augment the training data, the English transcriptions are translated to German
translations. Finally, we employ ensemble decoding to integrate the predictions
from several models trained with the different datasets. Combining these
techniques, we achieve 33.84 BLEU points on the MuST-C En-De test set, which
shows the enormous potential of the end-to-end model.",http://arxiv.org/pdf/2107.02444v2,cs.CL
2021-07-04 20:32:52+00:00,Protected probabilistic classification,"['Vladimir Vovk', 'Ivan Petej', 'Alex Gammerman']","This paper proposes a way of protecting probabilistic prediction models
against changes in the data distribution, concentrating on the case of
classification and paying particular attention to binary classification. This
is important in applications of machine learning, where the quality of a
trained prediction algorithm may drop significantly in the process of its
exploitation. Our techniques are based on recent work on conformal test
martingales and older work on prediction with expert advice, namely tracking
the best expert.",http://arxiv.org/pdf/2107.01726v2,cs.LG
2021-07-02 19:52:31+00:00,Uncertainty in Lung Cancer Stage for Outcome Estimation via Set-Valued Classification,"['Savannah Bergquist', 'Gabriel Brooks', 'Mary Beth Landrum', 'Nancy Keating', 'Sherri Rose']","Difficulty in identifying cancer stage in health care claims data has limited
oncology quality of care and health outcomes research. We fit prediction
algorithms for classifying lung cancer stage into three classes (stages I/II,
stage III, and stage IV) using claims data, and then demonstrate a method for
incorporating the classification uncertainty in outcomes estimation. Leveraging
set-valued classification and split conformal inference, we show how a fixed
algorithm developed in one cohort of data may be deployed in another, while
rigorously accounting for uncertainty from the initial classification step. We
demonstrate this process using SEER cancer registry data linked with Medicare
claims data.",http://arxiv.org/pdf/2107.01251v1,stat.AP
2021-07-01 10:59:36+00:00,Valid prediction intervals for regression problems,"['Nicolas Dewolf', 'Bernard De Baets', 'Willem Waegeman']","Over the last few decades, various methods have been proposed for estimating
prediction intervals in regression settings, including Bayesian methods,
ensemble methods, direct interval estimation methods and conformal prediction
methods. An important issue is the calibration of these methods: the generated
prediction intervals should have a predefined coverage level, without being
overly conservative. In this work, we review the above four classes of methods
from a conceptual and experimental point of view. Results on benchmark data
sets from various domains highlight large fluctuations in performance from one
data set to another. These observations can be attributed to the violation of
certain assumptions that are inherent to some classes of methods. We illustrate
how conformal prediction can be used as a general calibration procedure for
methods that deliver poor results without a calibration step.",http://arxiv.org/pdf/2107.00363v3,stat.ML
2021-06-24 19:01:05+00:00,Geometric learning of the conformational dynamics of molecules using dynamic graph neural networks,"['Michael Hunter Ashby', 'Jenna A. Bilbrey']","We apply a temporal edge prediction model for weighted dynamic graphs to
predict time-dependent changes in molecular structure. Each molecule is
represented as a complete graph in which each atom is a vertex and all vertex
pairs are connected by an edge weighted by the Euclidean distance between atom
pairs. We ingest a sequence of complete molecular graphs into a dynamic graph
neural network (GNN) to predict the graph at the next time step. Our dynamic
GNN predicts atom-to-atom distances with a mean absolute error of 0.017 \r{A},
which is considered ``chemically accurate'' for molecular simulations. We also
explored the transferability of a trained network to new molecular systems and
found that finetuning with less than 10% of the total trajectory provides a
mean absolute error of the same order of magnitude as that when training from
scratch on the full molecular trajectory.",http://arxiv.org/pdf/2106.13277v1,cs.LG
2021-06-16 05:07:28+00:00,Fast Quantum Property Prediction via Deeper 2D and 3D Graph Networks,"['Meng Liu', 'Cong Fu', 'Xuan Zhang', 'Limei Wang', 'Yaochen Xie', 'Hao Yuan', 'Youzhi Luo', 'Zhao Xu', 'Shenglong Xu', 'Shuiwang Ji']","Molecular property prediction is gaining increasing attention due to its
diverse applications. One task of particular interests and importance is to
predict quantum chemical properties without 3D equilibrium structures. This is
practically favorable since obtaining 3D equilibrium structures requires
extremely expensive calculations. In this work, we design a deep graph neural
network to predict quantum properties by directly learning from 2D molecular
graphs. In addition, we propose a 3D graph neural network to learn from
low-cost conformer sets, which can be obtained with open-source tools using an
affordable budget. We employ our methods to participate in the 2021 KDD Cup on
OGB Large-Scale Challenge (OGB-LSC), which aims to predict the HOMO-LUMO energy
gap of molecules. Final evaluation results reveal that we are one of the
winners with a mean absolute error of 0.1235 on the holdout test set. Our
implementation is available as part of the MoleculeX package
(https://github.com/divelab/MoleculeX).",http://arxiv.org/pdf/2106.08551v1,cs.LG
2021-06-15 13:48:26+00:00,Graphical Gaussian Process Regression Model for Aqueous Solvation Free Energy Prediction of Organic Molecules in Redox Flow Battery,"['Peiyuan Gao', 'Xiu Yang', 'Yu-Hang Tang', 'Muqing Zheng', 'Amity Anderson', 'Vijayakumar Murugesan', 'Aaron Hollas', 'Wei Wang']","The solvation free energy of organic molecules is a critical parameter in
determining emergent properties such as solubility, liquid-phase equilibrium
constants, and pKa and redox potentials in an organic redox flow battery. In
this work, we present a machine learning (ML) model that can learn and predict
the aqueous solvation free energy of an organic molecule using Gaussian process
regression method based on a new molecular graph kernel. To investigate the
performance of the ML model on electrostatic interaction, the nonpolar
interaction contribution of solvent and the conformational entropy of solute in
solvation free energy, three data sets with implicit or explicit water solvent
models, and contribution of conformational entropy of solute are tested. We
demonstrate that our ML model can predict the solvation free energy of
molecules at chemical accuracy with a mean absolute error of less than 1
kcal/mol for subsets of the QM9 dataset and the Freesolv database. To solve the
general data scarcity problem for a graph-based ML model, we propose a
dimension reduction algorithm based on the distance between molecular graphs,
which can be used to examine the diversity of the molecular data set. It
provides a promising way to build a minimum training set to improve prediction
for certain test sets where the space of molecular structures is predetermined.",http://arxiv.org/pdf/2106.08146v1,cs.CE
2021-06-14 10:00:39+00:00,Flexible dual-branched message passing neural network for quantum mechanical property prediction with molecular conformation,"['Jeonghee Jo', 'Bumju Kwak', 'Byunghan Lee', 'Sungroh Yoon']","A molecule is a complex of heterogeneous components, and the spatial
arrangements of these components determine the whole molecular properties and
characteristics. With the advent of deep learning in computational chemistry,
several studies have focused on how to predict molecular properties based on
molecular configurations. Message passing neural network provides an effective
framework for capturing molecular geometric features with the perspective of a
molecule as a graph. However, most of these studies assumed that all
heterogeneous molecular features, such as atomic charge, bond length, or other
geometric features always contribute equivalently to the target prediction,
regardless of the task type. In this study, we propose a dual-branched neural
network for molecular property prediction based on message-passing framework.
Our model learns heterogeneous molecular features with different scales, which
are trained flexibly according to each prediction target. In addition, we
introduce a discrete branch to learn single atom features without local
aggregation, apart from message-passing steps. We verify that this novel
structure can improve the model performance with faster convergence in most
targets. The proposed model outperforms other recent models with sparser
representations. Our experimental results indicate that in the chemical
property prediction tasks, the diverse chemical nature of targets should be
carefully considered for both model performance and generalizability.",http://arxiv.org/pdf/2106.07273v1,cs.LG
2021-06-11 02:49:12+00:00,Conformal Bayesian Computation,"['Edwin Fong', 'Chris Holmes']","We develop scalable methods for producing conformal Bayesian predictive
intervals with finite sample calibration guarantees. Bayesian posterior
predictive distributions, $p(y \mid x)$, characterize subjective beliefs on
outcomes of interest, $y$, conditional on predictors, $x$. Bayesian prediction
is well-calibrated when the model is true, but the predictive intervals may
exhibit poor empirical coverage when the model is misspecified, under the so
called ${\cal{M}}$-open perspective. In contrast, conformal inference provides
finite sample frequentist guarantees on predictive confidence intervals without
the requirement of model fidelity. Using 'add-one-in' importance sampling, we
show that conformal Bayesian predictive intervals are efficiently obtained from
re-weighted posterior samples of model parameters. Our approach contrasts with
existing conformal methods that require expensive refitting of models or
data-splitting to achieve computational efficiency. We demonstrate the utility
on a range of examples including extensions to partially exchangeable settings
such as hierarchical models.",http://arxiv.org/pdf/2106.06137v2,stat.ME
2021-06-10 19:11:43+00:00,Do peers share the same criteria for assessing grant applications?,"['Sven E. Hug', 'Michael Ochsner']","This study examines a basic assumption of peer review, namely, the idea that
there is a consensus on evaluation criteria among peers, which is a necessary
condition for the reliability of peer judgements. Empirical evidence indicating
that there is no consensus or more than one consensus would offer an
explanation for the disagreement effect, the low inter-rater reliability
consistently observed in peer review. To investigate this basic assumption, we
have surveyed all humanities scholars in Switzerland on 23 grant review
criteria. We have employed latent class tree modelling to identify subgroups in
which scholars rated criteria similarly (i.e. latent classes) and to explore
covariates predicting class membership. We have identified two consensus
classes, two consensus-close classes, and a consensus-far class. The consensus
classes contain a core consensus (ten criteria related to knowledge gaps,
feasibility, rigour, comprehensibility and argumentation, and academic
relevance, as well as to the competence and experience of the applicant) and a
broad consensus that includes the core consensus plus eight
contribution-related criteria, such as originality. These results provide a
possible explanation for the disagreement effect. Moreover, the results are
consistent with the notion of conservatism, which holds that original research
is undervalued in peer review, while other aspects, such as methodology and
feasibility, are overweighted. The covariate analysis indicated that age and
having tenure increases from the consensus-far to the consensus-close to the
consensus classes. This suggests that the more academic experience scholars
accumulate, the more their understanding of review criteria conforms to the
social norm.",http://arxiv.org/pdf/2106.07386v2,cs.SI
2021-06-08 09:14:39+00:00,Seismic Inverse Modeling Method based on Generative Adversarial Network,"['Pengfei Xie', 'YanShu Yin', 'JiaGen Hou', 'Mei Chen', 'Lixin Wang']","Seismic inverse modeling is a common method in reservoir prediction and it
plays a vital role in the exploration and development of oil and gas.
Conventional seismic inversion method is difficult to combine with complicated
and abstract knowledge on geological mode and its uncertainty is difficult to
be assessed. The paper proposes an inversion modeling method based on GAN
consistent with geology, well logs, seismic data. GAN is a the most promising
generation model algorithm that extracts spatial structure and abstract
features of training images. The trained GAN can reproduce the models with
specific mode. In our test, 1000 models were generated in 1 second. Based on
the trained GAN after assessment, the optimal result of models can be
calculated through Bayesian inversion frame. Results show that inversion models
conform to observation data and have a low uncertainty under the premise of
fast generation. This seismic inverse modeling method increases the efficiency
and quality of inversion iteration. It is worthy of studying and applying in
fusion of seismic data and geological knowledge.",http://arxiv.org/pdf/2106.04197v1,stat.ML
2021-06-07 15:12:47+00:00,Can a single neuron learn predictive uncertainty?,['Edgardo Solano-Carrillo'],"Uncertainty estimation methods using deep learning approaches strive against
separating how uncertain the state of the world manifests to us via measurement
(objective end) from the way this gets scrambled with the model specification
and training procedure used to predict such state (subjective means) -- e.g.,
number of neurons, depth, connections, priors (if the model is bayesian),
weight initialization, etc. This poses the question of the extent to which one
can eliminate the degrees of freedom associated with these specifications and
still being able to capture the objective end. Here, a novel non-parametric
quantile estimation method for continuous random variables is introduced, based
on the simplest neural network architecture with one degree of freedom: a
single neuron. Its advantage is first shown in synthetic experiments comparing
with the quantile estimation achieved from ranking the order statistics
(specifically for small sample size) and with quantile regression. In
real-world applications, the method can be used to quantify predictive
uncertainty under the split conformal prediction setting, whereby prediction
intervals are estimated from the residuals of a pre-trained model on a held-out
validation set and then used to quantify the uncertainty in future predictions
-- the single neuron used here as a structureless ``thermometer'' that measures
how uncertain the pre-trained model is. Benchmarking regression and
classification experiments demonstrate that the method is competitive in
quality and coverage with state-of-the-art solutions, with the added benefit of
being more computationally efficient.",http://arxiv.org/pdf/2106.03702v3,stat.ML
2021-06-04 14:12:12+00:00,Towards Fairness Certification in Artificial Intelligence,"['Tatiana Tommasi', 'Silvia Bucci', 'Barbara Caputo', 'Pietro Asinari']","Thanks to the great progress of machine learning in the last years, several
Artificial Intelligence (AI) techniques have been increasingly moving from the
controlled research laboratory settings to our everyday life. AI is clearly
supportive in many decision-making scenarios, but when it comes to sensitive
areas such as health care, hiring policies, education, banking or justice, with
major impact on individuals and society, it becomes crucial to establish
guidelines on how to design, develop, deploy and monitor this technology.
Indeed the decision rules elaborated by machine learning models are data-driven
and there are multiple ways in which discriminatory biases can seep into data.
Algorithms trained on those data incur the risk of amplifying prejudices and
societal stereotypes by over associating protected attributes such as gender,
ethnicity or disabilities with the prediction task. Starting from the extensive
experience of the National Metrology Institute on measurement standards and
certification roadmaps, and of Politecnico di Torino on machine learning as
well as methods for domain bias evaluation and mastering, we propose a first
joint effort to define the operational steps needed for AI fairness
certification. Specifically we will overview the criteria that should be met by
an AI system before coming into official service and the conformity assessment
procedures useful to monitor its functioning for fair decisions.",http://arxiv.org/pdf/2106.02498v1,cs.AI
2021-06-03 12:25:49+00:00,Conformal Prediction Bands for Multivariate Functional Data,"['Jacopo Diquigiovanni', 'Matteo Fontana', 'Simone Vantini']","Motivated by the pressing request of methods able to create prediction sets
in a general regression framework for a multivariate functional response and
pushed by new methodological advancements in non-parametric prediction for
functional data, we propose a set of conformal predictors that produce
finite-sample either valid or exact multivariate simultaneous prediction bands
under the mild assumption of exchangeable regression pairs. The fact that the
prediction bands can be built around any regression estimator and that can be
easily found in closed form yields a very widely usable method, which is fairly
straightforward to implement. In addition, we first introduce and then describe
a specific conformal predictor that guarantees an asymptotic result in terms of
efficiency and inducing prediction bands able to modulate their width based on
the local behavior and magnitude of the functional data. The method is
investigated and analyzed through a simulation study and a real-world
application in the field of urban mobility.",http://arxiv.org/pdf/2106.01792v1,stat.ME
2021-06-01 04:39:56+00:00,Locally Valid and Discriminative Prediction Intervals for Deep Learning Models,"['Zhen Lin', 'Shubhendu Trivedi', 'Jimeng Sun']","Crucial for building trust in deep learning models for critical real-world
applications is efficient and theoretically sound uncertainty quantification, a
task that continues to be challenging. Useful uncertainty information is
expected to have two key properties: It should be valid (guaranteeing coverage)
and discriminative (more uncertain when the expected risk is high). Moreover,
when combined with deep learning (DL) methods, it should be scalable and affect
the DL model performance minimally. Most existing Bayesian methods lack
frequentist coverage guarantees and usually affect model performance. The few
available frequentist methods are rarely discriminative and/or violate coverage
guarantees due to unrealistic assumptions. Moreover, many methods are expensive
or require substantial modifications to the base neural network. Building upon
recent advances in conformal prediction [13, 33] and leveraging the classical
idea of kernel regression, we propose Locally Valid and Discriminative
prediction intervals (LVD), a simple, efficient, and lightweight method to
construct discriminative prediction intervals (PIs) for almost any DL model.
With no assumptions on the data distribution, such PIs also offer finite-sample
local coverage guarantees (contrasted to the simpler marginal coverage). We
empirically verify, using diverse datasets, that besides being the only locally
valid method for DL, LVD also exceeds or matches the performance (including
coverage rate and prediction accuracy) of existing uncertainty quantification
methods, while offering additional benefits in scalability and flexibility.",http://arxiv.org/pdf/2106.00225v4,cs.LG
2021-06-01 01:37:32+00:00,Adaptive Conformal Inference Under Distribution Shift,"['Isaac Gibbs', 'Emmanuel Candès']","We develop methods for forming prediction sets in an online setting where the
data generating distribution is allowed to vary over time in an unknown
fashion. Our framework builds on ideas from conformal inference to provide a
general wrapper that can be combined with any black box method that produces
point predictions of the unseen label or estimated quantiles of its
distribution. While previous conformal inference methods rely on the assumption
that the data points are exchangeable, our adaptive approach provably achieves
the desired coverage frequency over long-time intervals irrespective of the
true data generating process. We accomplish this by modelling the distribution
shift as a learning problem in a single parameter whose optimal value is
varying over time and must be continuously re-estimated. We test our method,
adaptive conformal inference, on two real world datasets and find that its
predictions are robust to visible and significant distribution shifts.",http://arxiv.org/pdf/2106.00170v3,stat.ME
2021-05-31 13:42:24+00:00,Conformal Uncertainty Sets for Robust Optimization,"['Chancellor Johnstone', 'Bruce Cox']","Decision-making under uncertainty is hugely important for any decisions
sensitive to perturbations in observed data. One method of incorporating
uncertainty into making optimal decisions is through robust optimization, which
minimizes the worst-case scenario over some uncertainty set. We connect
conformal prediction regions to robust optimization, providing finite sample
valid and conservative ellipsoidal uncertainty sets, aptly named conformal
uncertainty sets. In pursuit of this connection we explicitly define
Mahalanobis distance as a potential conformity score in full conformal
prediction. We also compare the coverage and optimization performance of
conformal uncertainty sets, specifically generated with Mahalanobis distance,
to traditional ellipsoidal uncertainty sets on a collection of simulated robust
optimization examples.",http://arxiv.org/pdf/2105.14957v2,stat.ME
2021-05-25 12:44:14+00:00,Conformal Anomaly Detection on Spatio-Temporal Observations with Missing Data,"['Chen Xu', 'Yao Xie']","We develop a distribution-free, unsupervised anomaly detection method called
ECAD, which wraps around any regression algorithm and sequentially detects
anomalies. Rooted in conformal prediction, ECAD does not require data
exchangeability but approximately controls the Type-I error when data are
normal. Computationally, it involves no data-splitting and efficiently trains
ensemble predictors to increase statistical power. We demonstrate the superior
performance of ECAD on detecting anomalous spatio-temporal traffic flow.",http://arxiv.org/pdf/2105.11886v2,stat.AP
2021-05-24 13:14:41+00:00,Optimized conformal classification using gradient descent approximation,['Anthony Bellotti'],"Conformal predictors are an important class of algorithms that allow
predictions to be made with a user-defined confidence level. They are able to
do this by outputting prediction sets, rather than simple point predictions.
The conformal predictor is valid in the sense that the accuracy of its
predictions is guaranteed to meet the confidence level, only assuming
exchangeability in the data. Since accuracy is guaranteed, the performance of a
conformal predictor is measured through the efficiency of the prediction sets.
Typically, a conformal predictor is built on an underlying machine learning
algorithm and hence its predictive power is inherited from this algorithm.
However, since the underlying machine learning algorithm is not trained with
the objective of minimizing predictive efficiency it means that the resulting
conformal predictor may be sub-optimal and not aligned sufficiently to this
objective. Hence, in this study we consider an approach to train the conformal
predictor directly with maximum predictive efficiency as the optimization
objective, and we focus specifically on the inductive conformal predictor for
classification. To do this, the conformal predictor is approximated by a
differentiable objective function and gradient descent used to optimize it. The
resulting parameter estimates are then passed to a proper inductive conformal
predictor to give valid prediction sets. We test the method on several real
world data sets and find that the method is promising and in most cases gives
improved predictive efficiency against a baseline conformal predictor.",http://arxiv.org/pdf/2105.11255v1,cs.LG
2021-05-18 18:05:02+00:00,Conformal Prediction using Conditional Histograms,"['Matteo Sesia', 'Yaniv Romano']","This paper develops a conformal method to compute prediction intervals for
non-parametric regression that can automatically adapt to skewed data.
Leveraging black-box machine learning algorithms to estimate the conditional
distribution of the outcome using histograms, it translates their output into
the shortest prediction intervals with approximate conditional coverage. The
resulting prediction intervals provably have marginal coverage in finite
samples, while asymptotically achieving conditional coverage and optimal length
if the black-box model is consistent. Numerical experiments with simulated and
real data demonstrate improved performance compared to state-of-the-art
alternatives, including conformalized quantile regression and other
distributional conformal prediction approaches.",http://arxiv.org/pdf/2105.08747v2,stat.ME
2021-05-18 16:53:00+00:00,Enhancement of prediction algorithms by betting,['Vladimir Vovk'],"This note proposes a procedure for enhancing the quality of probabilistic
prediction algorithms via betting against their predictions. It is inspired by
the success of the conformal test martingales that have been developed
recently.",http://arxiv.org/pdf/2105.08669v1,cs.LG
2021-05-15 15:22:29+00:00,An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming,"['Minkai Xu', 'Wujie Wang', 'Shitong Luo', 'Chence Shi', 'Yoshua Bengio', 'Rafael Gomez-Bombarelli', 'Jian Tang']","Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches. Code is available at
\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.",http://arxiv.org/pdf/2105.07246v2,cs.LG
2021-05-11 03:40:29+00:00,EBM-Fold: Fully-Differentiable Protein Folding Powered by Energy-based Models,"['Jiaxiang Wu', 'Shitong Luo', 'Tao Shen', 'Haidong Lan', 'Sheng Wang', 'Junzhou Huang']","Accurate protein structure prediction from amino-acid sequences is critical
to better understanding the protein function. Recent advances in this area
largely benefit from more precise inter-residue distance and orientation
predictions, powered by deep neural networks. However, the structure
optimization procedure is still dominated by traditional tools, e.g. Rosetta,
where the structure is solved via minimizing a pre-defined statistical energy
function (with optional prediction-based restraints). Such energy function may
not be optimal in formulating the whole conformation space of proteins. In this
paper, we propose a fully-differentiable approach for protein structure
optimization, guided by a data-driven generative network. This network is
trained in a denoising manner, attempting to predict the correction signal from
corrupted distance matrices between Ca atoms. Once the network is well trained,
Langevin dynamics based sampling is adopted to gradually optimize structures
from random initialization. Extensive experiments demonstrate that our EBM-Fold
approach can efficiently produce high-quality decoys, compared against
traditional Rosetta-based structure optimization routines.",http://arxiv.org/pdf/2105.04771v2,cs.LG
2021-05-09 10:30:35+00:00,Learning Gradient Fields for Molecular Conformation Generation,"['Chence Shi', 'Shitong Luo', 'Minkai Xu', 'Jian Tang']","We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.",http://arxiv.org/pdf/2105.03902v3,cs.LG
2021-05-09 10:00:03+00:00,Conformer: Local Features Coupling Global Representations for Visual Recognition,"['Zhiliang Peng', 'Wei Huang', 'Shanzhi Gu', 'Lingxi Xie', 'Yaowei Wang', 'Jianbin Jiao', 'Qixiang Ye']","Within Convolutional Neural Network (CNN), the convolution operations are
good at extracting local features but experience difficulty to capture global
representations. Within visual transformer, the cascaded self-attention modules
can capture long-distance feature dependencies but unfortunately deteriorate
local feature details. In this paper, we propose a hybrid network structure,
termed Conformer, to take advantage of convolutional operations and
self-attention mechanisms for enhanced representation learning. Conformer roots
in the Feature Coupling Unit (FCU), which fuses local features and global
representations under different resolutions in an interactive fashion.
Conformer adopts a concurrent structure so that local features and global
representations are retained to the maximum extent. Experiments show that
Conformer, under the comparable parameter complexity, outperforms the visual
transformer (DeiT-B) by 2.3% on ImageNet. On MSCOCO, it outperforms ResNet-101
by 3.7% and 3.6% mAPs for object detection and instance segmentation,
respectively, demonstrating the great potential to be a general backbone
network. Code is available at https://github.com/pengzhiliang/Conformer.",http://arxiv.org/pdf/2105.03889v1,cs.CV
2021-04-28 16:36:05+00:00,Finite-sample Efficient Conformal Prediction,"['Yachong Yang', 'Arun Kumar Kuchibhotla']","Conformal prediction is a generic methodology for finite-sample valid
distribution-free prediction. This technique has garnered a lot of attention in
the literature partly because it can be applied with any machine learning
algorithm that provides point predictions to yield valid prediction regions. Of
course, the efficiency (width/volume) of the resulting prediction region
depends on the performance of the machine learning algorithm. In this paper, we
consider the problem of obtaining the smallest conformal prediction region
given a family of machine learning algorithms. We provide two general-purpose
selection algorithms and consider coverage as well as width properties of the
final prediction region. The first selection method yields the smallest width
prediction region among the family of conformal prediction regions for all
sample sizes, but only has an approximate coverage guarantee. The second
selection method has a finite sample coverage guarantee but only attains close
to the smallest width. The approximate optimal width property of the second
method is quantified via an oracle inequality. Asymptotic oracle inequalities
are also considered when the family of algorithms is given by ridge regression
with different penalty parameters.",http://arxiv.org/pdf/2104.13871v2,stat.ME
2021-04-19 16:18:00+00:00,Advanced Long-context End-to-end Speech Recognition Using Context-expanded Transformers,"['Takaaki Hori', 'Niko Moritz', 'Chiori Hori', 'Jonathan Le Roux']","This paper addresses end-to-end automatic speech recognition (ASR) for long
audio recordings such as lecture and conversational speeches. Most end-to-end
ASR models are designed to recognize independent utterances, but contextual
information (e.g., speaker or topic) over multiple utterances is known to be
useful for ASR. In our prior work, we proposed a context-expanded Transformer
that accepts multiple consecutive utterances at the same time and predicts an
output sequence for the last utterance, achieving 5-15% relative error
reduction from utterance-based baselines in lecture and conversational ASR
benchmarks. Although the results have shown remarkable performance gain, there
is still potential to further improve the model architecture and the decoding
process. In this paper, we extend our prior work by (1) introducing the
Conformer architecture to further improve the accuracy, (2) accelerating the
decoding process with a novel activation recycling technique, and (3) enabling
streaming decoding with triggered attention. We demonstrate that the extended
Transformer provides state-of-the-art end-to-end ASR performance, obtaining a
17.3% character error rate for the HKUST dataset and 12.0%/6.3% word error
rates for the Switchboard-300 Eval2000 CallHome/Switchboard test sets. The new
decoding method reduces decoding time by more than 50% and further enables
streaming ASR with limited accuracy degradation.",http://arxiv.org/pdf/2104.09426v1,cs.CL
2021-04-18 10:22:28+00:00,Consistent Accelerated Inference via Confident Adaptive Transformers,"['Tal Schuster', 'Adam Fisch', 'Tommi Jaakkola', 'Regina Barzilay']","We develop a novel approach for confidently accelerating inference in the
large and expensive multilayer Transformers that are now ubiquitous in natural
language processing (NLP). Amortized or approximate computational methods
increase efficiency, but can come with unpredictable performance costs. In this
work, we present CATs -- Confident Adaptive Transformers -- in which we
simultaneously increase computational efficiency, while guaranteeing a
specifiable degree of consistency with the original model with high confidence.
Our method trains additional prediction heads on top of intermediate layers,
and dynamically decides when to stop allocating computational effort to each
input using a meta consistency classifier. To calibrate our early prediction
stopping rule, we formulate a unique extension of conformal prediction. We
demonstrate the effectiveness of this approach on four classification and
regression tasks.",http://arxiv.org/pdf/2104.08803v2,cs.CL
2021-04-14 13:56:32+00:00,Efficient conformer-based speech recognition with linear attention,"['Shengqiang Li', 'Menglong Xu', 'Xiao-Lei Zhang']","Recently, conformer-based end-to-end automatic speech recognition, which
outperforms recurrent neural network based ones, has received much attention.
Although the parallel computing of conformer is more efficient than recurrent
neural networks, the computational complexity of its dot-product self-attention
is quadratic with respect to the length of the input feature. To reduce the
computational complexity of the self-attention layer, we propose multi-head
linear self-attention for the self-attention layer, which reduces its
computational complexity to linear order. In addition, we propose to factorize
the feed forward module of the conformer by low-rank matrix factorization,
which successfully reduces the number of the parameters by approximate 50% with
little performance loss. The proposed model, named linear attention based
conformer (LAC), can be trained and inferenced jointly with the connectionist
temporal classification objective, which further improves the performance of
LAC. To evaluate the effectiveness of LAC, we conduct experiments on the
AISHELL-1 and LibriSpeech corpora. Results show that the proposed LAC achieves
better performance than 7 recently proposed speech recognition models, and is
competitive with the state-of-the-art conformer. Meanwhile, the proposed LAC
has a number of parameters of only 50% over the conformer with faster training
speed than the latter.",http://arxiv.org/pdf/2104.06865v2,cs.SD
2021-04-14 06:41:12+00:00,Root-finding Approaches for Computing Conformal Prediction Set,"['Eugene Ndiaye', 'Ichiro Takeuchi']","Conformal prediction constructs a confidence set for an unobserved response
of a feature vector based on previous identically distributed and exchangeable
observations of responses and features. It has a coverage guarantee at any
nominal level without additional assumptions on their distribution. Its
computation deplorably requires a refitting procedure for all replacement
candidates of the target response. In regression settings, this corresponds to
an infinite number of model fits. Apart from relatively simple estimators that
can be written as pieces of linear function of the response, efficiently
computing such sets is difficult, and is still considered as an open problem.
We exploit the fact that, \emph{often}, conformal prediction sets are intervals
whose boundaries can be efficiently approximated by classical root-finding
algorithms. We investigate how this approach can overcome many limitations of
formerly used strategies; we discuss its complexity and drawbacks.",http://arxiv.org/pdf/2104.06648v3,stat.ML
2021-04-14 03:46:37+00:00,Detection of Dataset Shifts in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression,"['Feiyang Cai', 'Ali I. Ozdagli', 'Xenofon Koutsoukos']","Cyber-physical systems (CPSs) use learning-enabled components (LECs)
extensively to cope with various complex tasks under high-uncertainty
environments. However, the dataset shifts between the training and testing
phase may lead the LECs to become ineffective to make large-error predictions,
and further, compromise the safety of the overall system. In our paper, we
first provide the formal definitions for different types of dataset shifts in
learning-enabled CPS. Then, we propose an approach to detect the dataset shifts
effectively for regression problems. Our approach is based on the inductive
conformal anomaly detection and utilizes a variational autoencoder for
regression model which enables the approach to take into consideration both LEC
input and output for detecting dataset shifts. Additionally, in order to
improve the robustness of detection, layer-wise relevance propagation (LRP) is
incorporated into our approach. We demonstrate our approach by using an
advanced emergency braking system implemented in an open-source simulator for
self-driving cars. The evaluation results show that our approach can detect
different types of dataset shifts with a small number of false alarms while the
execution time is smaller than the sampling period of the system.",http://arxiv.org/pdf/2104.06613v1,cs.LG
2021-04-13 17:42:58+00:00,Nested Conformal Prediction Sets for Classification with Applications to Probation Data,"['Arun K. Kuchibhotla', 'Richard A. Berk']","Risk assessments to help inform criminal justice decisions have been used in
the United States since the 1920s. Over the past several years, statistical
learning risk algorithms have been introduced amid much controversy about
fairness, transparency and accuracy. In this paper, we focus on accuracy for a
large department of probation and parole that is considering a major revision
of its current, statistical learning risk methods. Because the content of each
offender's supervision is substantially shaped by a forecast of subsequent
conduct, forecasts have real consequences. Here we consider the probability
that risk forecasts are correct. We augment standard statistical learning
estimates of forecasting uncertainty (i.e., confusion tables) with uncertainty
estimates from nested conformal prediction sets. In a demonstration of concept
using data from the department of probation and parole, we show that the
standard uncertainty measures and uncertainty measures from nested conformal
prediction sets can differ dramatically in concept and output. We also provide
a modification of nested conformal called the localized conformal method to
match confusion tables more closely when possible. A strong case can be made
favoring the nested and localized conformal approach. As best we can tell, our
formulation of such comparisons and consequent recommendations is novel.",http://arxiv.org/pdf/2104.09358v1,stat.ME
2021-04-05 12:51:39+00:00,Conformal testing in a binary model situation,['Vladimir Vovk'],"Conformal testing is a way of testing the IID assumption based on conformal
prediction. The topic of this note is computational evaluation of the
performance of conformal testing in a model situation in which IID binary
observations generated from a Bernoulli distribution are followed by IID binary
observations generated from another Bernoulli distribution, with the parameters
of the distributions and changepoint unknown. Existing conformal test
martingales can be used for this task and work well in simple cases, but their
efficiency can be improved greatly.",http://arxiv.org/pdf/2104.01885v1,cs.LG
2021-03-31 16:30:58+00:00,Universal Prediction Band via Semi-Definite Programming,['Tengyuan Liang'],"We propose a computationally efficient method to construct nonparametric,
heteroscedastic prediction bands for uncertainty quantification, with or
without any user-specified predictive model. Our approach provides an
alternative to the now-standard conformal prediction for uncertainty
quantification, with novel theoretical insights and computational advantages.
The data-adaptive prediction band is universally applicable with minimal
distributional assumptions, has strong non-asymptotic coverage properties, and
is easy to implement using standard convex programs. Our approach can be viewed
as a novel variance interpolation with confidence and further leverages
techniques from semi-definite programming and sum-of-squares optimization.
Theoretical and numerical performances for the proposed approach for
uncertainty quantification are analyzed.",http://arxiv.org/pdf/2103.17203v3,stat.ML
2021-03-30 11:53:12+00:00,Multi-modal Trajectory Prediction for Autonomous Driving with Semantic Map and Dynamic Graph Attention Network,"['Bo Dong', 'Hao Liu', 'Yu Bai', 'Jinbiao Lin', 'Zhuoran Xu', 'Xinyu Xu', 'Qi Kong']","Predicting future trajectories of surrounding obstacles is a crucial task for
autonomous driving cars to achieve a high degree of road safety. There are
several challenges in trajectory prediction in real-world traffic scenarios,
including obeying traffic rules, dealing with social interactions, handling
traffic of multi-class movement, and predicting multi-modal trajectories with
probability. Inspired by people's natural habit of navigating traffic with
attention to their goals and surroundings, this paper presents a unique dynamic
graph attention network to solve all those challenges. The network is designed
to model the dynamic social interactions among agents and conform to traffic
rules with a semantic map. By extending the anchor-based method to multiple
types of agents, the proposed method can predict multi-modal trajectories with
probabilities for multi-class movements using a single model. We validate our
approach on the proprietary autonomous driving dataset for the logistic
delivery scenario and two publicly available datasets. The results show that
our method outperforms state-of-the-art techniques and demonstrates the
potential for trajectory prediction in real-world traffic.",http://arxiv.org/pdf/2103.16273v1,cs.CV
2021-03-29 14:04:18+00:00,Shape-constrained Symbolic Regression -- Improving Extrapolation with Prior Knowledge,"['Gabriel Kronberger', 'Fabricio Olivetti de França', 'Bogdan Burlacu', 'Christian Haider', 'Michael Kommenda']","We investigate the addition of constraints on the function image and its
derivatives for the incorporation of prior knowledge in symbolic regression.
The approach is called shape-constrained symbolic regression and allows us to
enforce e.g. monotonicity of the function over selected inputs. The aim is to
find models which conform to expected behaviour and which have improved
extrapolation capabilities. We demonstrate the feasibility of the idea and
propose and compare two evolutionary algorithms for shape-constrained symbolic
regression: i) an extension of tree-based genetic programming which discards
infeasible solutions in the selection step, and ii) a two population
evolutionary algorithm that separates the feasible from the infeasible
solutions. In both algorithms we use interval arithmetic to approximate bounds
for models and their partial derivatives. The algorithms are tested on a set of
19 synthetic and four real-world regression problems. Both algorithms are able
to identify models which conform to shape constraints which is not the case for
the unmodified symbolic regression algorithms. However, the predictive accuracy
of models with constraints is worse on the training set and the test set.
Shape-constrained polynomial regression produces the best results for the test
set but also significantly larger models.",http://arxiv.org/pdf/2103.15624v2,cs.NE
2021-03-28 09:00:42+00:00,The general conformable fractional grey system model and its applications,"['Wanli Xie', 'Mingyong Pang', 'Wen-Ze Wu', 'Chong Liu', 'Caixia Liu']","Grey system theory is an important mathematical tool for describing uncertain
information in the real world. It has been used to solve the uncertainty
problems specially caused by lack of information. As a novel theory, the theory
can deal with various fields and plays an important role in modeling the small
sample problems. But many modeling mechanisms of grey system need to be
answered, such as why grey accumulation can be successfully applied to grey
prediction model? What is the key role of grey accumulation? Some scholars have
already given answers to a certain extent. In this paper, we explain the role
from the perspective of complex networks. Further, we propose generalized
conformable accumulation and difference, and clarify its physical meaning in
the grey model. We use our newly proposed fractional accumulation and
difference to our generalized conformable fractional grey model, or GCFGM(1,1),
and employ practical cases to verify that GCFGM(1,1) has higher accuracy
compared to traditional models.",http://arxiv.org/pdf/2104.01114v2,stat.OT
2021-03-24 18:12:52+00:00,Learning to Generate Code Comments from Class Hierarchies,"['Jiyang Zhang', 'Sheena Panthaplackel', 'Pengyu Nie', 'Raymond J. Mooney', 'Junyi Jessy Li', 'Milos Gligoric']","Descriptive code comments are essential for supporting code comprehension and
maintenance. We propose the task of automatically generating comments for
overriding methods. We formulate a novel framework which accommodates the
unique contextual and linguistic reasoning that is required for performing this
task. Our approach features: (1) incorporating context from the class
hierarchy; (2) conditioning on learned, latent representations of specificity
to generate comments that capture the more specialized behavior of the
overriding method; and (3) unlikelihood training to discourage predictions
which do not conform to invariant characteristics of the comment corresponding
to the overridden method. Our experiments show that the proposed approach is
able to generate comments for overriding methods of higher quality compared to
prevailing comment generation techniques.",http://arxiv.org/pdf/2103.13426v2,cs.CL
2021-03-23 11:47:51+00:00,Model Independent Error Bound Estimation for Conformance Checking Approximation,"['Mohammadreza Fani Sani', 'Martin Kabierski', 'Sebastiaan J. van Zelst', 'Wil M. P. van der Aalst']","Conformance checking techniques allow us to quantify the correspondence of a
process's execution, captured in event data, w.r.t., a reference process model.
In this context, alignments have proven to be useful for calculating
conformance statistics. However, for extensive event data and complex process
models, the computation time of alignments is considerably high, hampering
their practical use. Simultaneously, it suffices to approximate either
alignments or their corresponding conformance value(s) for many applications.
Recent work has shown that using subsets of the process model behavior leads to
accurate conformance approximations. The accuracy of such an approximation
heavily depends on the selected subset of model behavior. Thus, in this paper,
we show that we can derive a priori error bounds for conformance checking
approximation based on arbitrary activity sequences, independently of the given
process model. Such error bounds subsequently let us select the most relevant
subset of process model behavior for the alignment approximation. Experiments
confirm that conformance approximation accuracy improves when using the
proposed error bound approximation to guide the selection of relevant subsets
of process model behavior.",http://arxiv.org/pdf/2103.13315v1,cs.SE
2021-03-17 16:32:26+00:00,Conformalized Survival Analysis,"['Emmanuel J. Candès', 'Lihua Lei', 'Zhimei Ren']","Existing survival analysis techniques heavily rely on strong modelling
assumptions and are, therefore, prone to model misspecification errors. In this
paper, we develop an inferential method based on ideas from conformal
prediction, which can wrap around any survival prediction algorithm to produce
calibrated, covariate-dependent lower predictive bounds on survival times. In
the Type I right-censoring setting, when the censoring times are completely
exogenous, the lower predictive bounds have guaranteed coverage in finite
samples without any assumptions other than that of operating on independent and
identically distributed data points. Under a more general conditionally
independent censoring assumption, the bounds satisfy a doubly robust property
which states the following: marginal coverage is approximately guaranteed if
either the censoring mechanism or the conditional survival function is
estimated well. Further, we demonstrate that the lower predictive bounds remain
valid and informative for other types of censoring. The validity and efficiency
of our procedure are demonstrated on synthetic data and real COVID-19 data from
the UK Biobank.",http://arxiv.org/pdf/2103.09763v3,stat.ME
2021-03-05 01:56:58+00:00,Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images,"['Yu Tian', 'Guansong Pang', 'Fengbei Liu', 'Yuanhong chen', 'Seon Ho Shin', 'Johan W. Verjans', 'Rajvinder Singh', 'Gustavo Carneiro']","Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively
with normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)
samples that do not conform to the expected normal patterns. UAD has two main
advantages over its fully supervised counterpart. Firstly, it is able to
directly leverage large datasets available from health screening programs that
contain mostly normal image samples, avoiding the costly manual labelling of
abnormal samples and the subsequent issues involved in training with extremely
class-imbalanced data. Further, UAD approaches can potentially detect and
localise any type of lesions that deviate from the normal patterns. One
significant challenge faced by UAD methods is how to learn effective
low-dimensional image representations to detect and localise subtle
abnormalities, generally consisting of small lesions. To address this
challenge, we propose a novel self-supervised representation learning method,
called Constrained Contrastive Distribution learning for anomaly detection
(CCD), which learns fine-grained feature representations by simultaneously
predicting the distribution of augmented data and image contexts using
contrastive learning with pretext constraints. The learned representations can
be leveraged to train more anomaly-sensitive detection models. Extensive
experiment results show that our method outperforms current state-of-the-art
UAD approaches on three different colonoscopy and fundus screening datasets.
Our code is available at https://github.com/tianyu0207/CCD.",http://arxiv.org/pdf/2103.03423v2,cs.CV
2021-03-04 20:51:03+00:00,Distribution-free uncertainty quantification for classification under label shift,"['Aleksandr Podkopaev', 'Aaditya Ramdas']","Trustworthy deployment of ML models requires a proper measure of uncertainty,
especially in safety-critical applications. We focus on uncertainty
quantification (UQ) for classification problems via two avenues -- prediction
sets using conformal prediction and calibration of probabilistic predictors by
post-hoc binning -- since these possess distribution-free guarantees for i.i.d.
data. Two common ways of generalizing beyond the i.i.d. setting include
handling covariate and label shift. Within the context of distribution-free UQ,
the former has already received attention, but not the latter. It is known that
label shift hurts prediction, and we first argue that it also hurts UQ, by
showing degradation in coverage and calibration. Piggybacking on recent
progress in addressing label shift (for better prediction), we examine the
right way to achieve UQ by reweighting the aforementioned conformal and
calibration procedures whenever some unlabeled data from the target
distribution is available. We examine these techniques theoretically in a
distribution-free framework and demonstrate their excellent practical
performance.",http://arxiv.org/pdf/2103.03323v4,stat.ML
2021-02-28 21:31:42+00:00,Multi Split Conformal Prediction,"['Aldo Solari', 'Vera Djordjilović']","Split conformal prediction is a computationally efficient method for
performing distribution-free predictive inference in regression. It involves,
however, a one-time random split of the data, and the result depends on the
particular split. To address this problem, we propose multi split conformal
prediction, a simple method based on Markov's inequality to aggregate single
split conformal prediction intervals across multiple splits.",http://arxiv.org/pdf/2103.00627v2,stat.ME
2021-02-26 23:21:16+00:00,Flexible Model Aggregation for Quantile Regression,"['Rasool Fakoor', 'Taesup Kim', 'Jonas Mueller', 'Alexander J. Smola', 'Ryan J. Tibshirani']","Quantile regression is a fundamental problem in statistical learning
motivated by a need to quantify uncertainty in predictions, or to model a
diverse population without being overly reductive. For instance,
epidemiological forecasts, cost estimates, and revenue predictions all benefit
from being able to quantify the range of possible values accurately. As such,
many models have been developed for this problem over many years of research in
statistics, machine learning, and related fields. Rather than proposing yet
another (new) algorithm for quantile regression we adopt a meta viewpoint: we
investigate methods for aggregating any number of conditional quantile models,
in order to improve accuracy and robustness. We consider weighted ensembles
where weights may vary over not only individual models, but also over quantile
levels, and feature values. All of the models we consider in this paper can be
fit using modern deep learning toolkits, and hence are widely accessible (from
an implementation point of view) and scalable. To improve the accuracy of the
predicted quantiles (or equivalently, prediction intervals), we develop tools
for ensuring that quantiles remain monotonically ordered, and apply conformal
calibration methods. These can be used without any modification of the original
library of base models. We also review some basic theory surrounding quantile
aggregation and related scoring rules, and contribute a few new results to this
literature (for example, the fact that post sorting or post isotonic regression
can only improve the weighted interval score). Finally, we provide an extensive
suite of empirical comparisons across 34 data sets from two different benchmark
repositories.",http://arxiv.org/pdf/2103.00083v5,stat.ML
2021-02-20 20:39:05+00:00,Retrain or not retrain: Conformal test martingales for change-point detection,"['Vladimir Vovk', 'Ivan Petej', 'Ilia Nouretdinov', 'Ernst Ahlberg', 'Lars Carlsson', 'Alex Gammerman']","We argue for supplementing the process of training a prediction algorithm by
setting up a scheme for detecting the moment when the distribution of the data
changes and the algorithm needs to be retrained. Our proposed schemes are based
on exchangeability martingales, i.e., processes that are martingales under any
exchangeable distribution for the data. Our method, based on conformal
prediction, is general and can be applied on top of any modern prediction
algorithm. Its validity is guaranteed, and in this paper we make first steps in
exploring its efficiency.",http://arxiv.org/pdf/2102.10439v1,cs.LG
2021-02-20 03:17:58+00:00,Learning Neural Generative Dynamics for Molecular Conformation Generation,"['Minkai Xu', 'Shitong Luo', 'Yoshua Bengio', 'Jian Peng', 'Jian Tang']","We study how to generate molecule conformations (i.e., 3D structures) from a
molecular graph. Traditional methods, such as molecular dynamics, sample
conformations via computationally expensive simulations. Recently, machine
learning methods have shown great potential by training on a large collection
of conformation data. Challenges arise from the limited model capacity for
capturing complex distributions of conformations and the difficulty in modeling
long-range dependencies between atoms. Inspired by the recent progress in deep
generative models, in this paper, we propose a novel probabilistic framework to
generate valid and diverse conformations given a molecular graph. We propose a
method combining the advantages of both flow-based and energy-based models,
enjoying: (1) a high model capacity to estimate the multimodal conformation
distribution; (2) explicitly capturing the complex long-range dependencies
between atoms in the observation space. Extensive experiments demonstrate the
superior performance of the proposed method on several benchmarks, including
conformation generation and distance modeling tasks, with a significant
improvement over existing generative models for molecular conformation
sampling.",http://arxiv.org/pdf/2102.10240v3,cs.LG
2021-02-17 17:46:57+00:00,Few-shot Conformal Prediction with Auxiliary Tasks,"['Adam Fisch', 'Tal Schuster', 'Tommi Jaakkola', 'Regina Barzilay']","We develop a novel approach to conformal prediction when the target task has
limited data available for training. Conformal prediction identifies a small
set of promising output candidates in place of a single prediction, with
guarantees that the set contains the correct answer with high probability. When
training data is limited, however, the predicted set can easily become unusably
large. In this work, we obtain substantially tighter prediction sets while
maintaining desirable marginal guarantees by casting conformal prediction as a
meta-learning paradigm over exchangeable collections of auxiliary tasks. Our
conformalization algorithm is simple, fast, and agnostic to the choice of
underlying model, learning algorithm, or dataset. We demonstrate the
effectiveness of this approach across a number of few-shot classification and
regression tasks in natural language processing, computer vision, and
computational chemistry for drug discovery.",http://arxiv.org/pdf/2102.08898v2,cs.LG
2021-02-15 10:14:44+00:00,Approximation to Object Conditional Validity with Inductive Conformal Predictors,['Anthony Bellotti'],"Conformal predictors are machine learning algorithms that output prediction
sets that have a guarantee of marginal validity for finite samples with minimal
distributional assumptions. This is a property that makes conformal predictors
useful for machine learning tasks where we require reliable predictions. It
would also be desirable to achieve conditional validity in the same setting, in
the sense that validity of the prediction intervals remains valid regardless of
conditioning on any particular property of the object of the prediction.
Unfortunately, it has been shown that such conditional validity is impossible
to guarantee for non-trivial prediction problems for finite samples. In this
article, instead of trying to achieve a strong conditional validity result, the
weaker goal of achieving an approximation to conditional validity is
considered. A new algorithm is introduced to do this by iteratively adjusting a
conformity measure to deviations from object conditional validity measured in
the training data. Along with some theoretical results, experimental results
are provided for three data sets that demonstrate (1) in real world machine
learning tasks, lack of conditional validity is a measurable problem and (2)
that the proposed algorithm is effective at alleviating this problem.",http://arxiv.org/pdf/2102.07436v2,cs.LG
2021-02-12 20:03:38+00:00,The Importance of Being a Band: Finite-Sample Exact Distribution-Free Prediction Sets for Functional Data,"['Jacopo Diquigiovanni', 'Matteo Fontana', 'Simone Vantini']","Functional Data Analysis represents a field of growing interest in
statistics. Despite several studies have been proposed leading to fundamental
results, the problem of obtaining valid and efficient prediction sets has not
been thoroughly covered. Indeed, the great majority of methods currently in the
literature rely on strong distributional assumptions (e.g, Gaussianity),
dimension reduction techniques and/or asymptotic arguments. In this work, we
propose a new nonparametric approach in the field of Conformal Prediction based
on a new family of nonconformity measures inducing conformal predictors able to
create closed-form finite-sample valid or exact prediction sets under very
minimal distributional assumptions. In addition, our proposal ensures that the
prediction sets obtained are bands, an essential feature in the functional
setting that allows the visualization and interpretation of such sets. The
procedure is also fast, scalable, does not rely on functional dimension
reduction techniques and allows the user to select different nonconformity
measures depending on the problem at hand always obtaining valid bands. Within
this family of measures, we propose also a specific measure leading to
prediction bands asymptotically no less efficient than those with constant
width.",http://arxiv.org/pdf/2102.06746v2,stat.ME
2021-02-11 18:59:11+00:00,Private Prediction Sets,"['Anastasios N. Angelopoulos', 'Stephen Bates', 'Tijana Zrnic', 'Michael I. Jordan']","In real-world settings involving consequential decision-making, the
deployment of machine learning systems generally requires both reliable
uncertainty quantification and protection of individuals' privacy. We present a
framework that treats these two desiderata jointly. Our framework is based on
conformal prediction, a methodology that augments predictive models to return
prediction sets that provide uncertainty quantification -- they provably cover
the true response with a user-specified probability, such as 90%. One might
hope that when used with privately-trained models, conformal prediction would
yield privacy guarantees for the resulting prediction sets; unfortunately this
is not the case. To remedy this key problem, we develop a method that takes any
pre-trained predictive model and outputs differentially private prediction
sets. Our method follows the general approach of split conformal prediction; we
use holdout data to calibrate the size of the prediction sets but preserve
privacy by using a privatized quantile subroutine. This subroutine compensates
for the noise introduced to preserve privacy in order to guarantee correct
coverage. We evaluate the method on large-scale computer vision datasets.",http://arxiv.org/pdf/2102.06202v2,cs.LG
2021-02-11 11:57:40+00:00,Empirical Analysis on Productivity Prediction and Locality for Use Case Points Method,"['Mohammad Azzeh', 'Ali Bou Nassif', 'Cuauhtemoc Lopez Martin']","Use Case Points (UCP) method has been around for over two decades. Although,
there was a substantial criticism concerning the algebraic construction and
factors assessment of UCP, it remains an efficient early size estimation
method. Predicting software effort from UCP is still an ever-present challenge.
The earlier version of UCP method suggested using productivity as a cost
driver, where fixed or a few pre-defined productivity ratios have been widely
agreed. While this approach was successful when no enough historical data is
available, it is no longer acceptable because software projects are different
in terms of development aspects. Therefore, it is better to understand the
relationship between productivity and other UCP variables. This paper examines
the impact of data locality approaches on productivity and effort prediction
from multiple UCP variables. The environmental factors are used as partitioning
factors to produce local homogeneous data either based on their influential
levels or using clustering algorithms. Different machine learning methods,
including solo and ensemble methods, are used to construct productivity and
effort prediction models based on the local data. The results demonstrate that
the prediction models that are created based on local data surpass models that
use entire data. Also, the results show that conforming the hypothetical
assumption between productivity and environmental factors is not necessarily a
requirement for success of locality.",http://arxiv.org/pdf/2102.05961v1,cs.SE
2021-02-05 10:25:36+00:00,"Boost AI Power: Data Augmentation Strategies with unlabelled Data and Conformal Prediction, a Case in Alternative Herbal Medicine Discrimination with Electronic Nose","['Li Liu', 'Xianghao Zhan', 'Rumeng Wu', 'Xiaoqing Guan', 'Zhan Wang', 'Wei Zhang', 'Mert Pilanci', 'You Wang', 'Zhiyuan Luo', 'Guang Li']","Electronic nose has been proven to be effective in alternative herbal
medicine classification, but due to the nature of supervised learning, previous
research heavily relies on the labelled training data, which are time-costly
and labor-intensive to collect. To alleviate the critical dependency on the
training data in real-world applications, this study aims to improve
classification accuracy via data augmentation strategies. The effectiveness of
five data augmentation strategies under different training data inadequacy are
investigated in two scenarios: the noise-free scenario where different
availabilities of unlabelled data were considered, and the noisy scenario where
different levels of Gaussian noises and translational shifts were added to
represent sensor drifts. The five augmentation strategies, namely noise-adding
data augmentation, semi-supervised learning, classifier-based online learning,
Inductive Conformal Prediction (ICP) online learning and our novel ensemble ICP
online learning proposed in this study, are experimented and compared against
supervised learning baseline, with Linear Discriminant Analysis (LDA) and
Support Vector Machine (SVM) as the classifiers. Our novel strategy, ensemble
ICP online learning, outperforms the others by showing non-decreasing
classification accuracy on all tasks and a significant improvement on most
simulated tasks (25out of 36 tasks,p<=0.05). Furthermore, this study provides a
systematic analysis of different augmentation strategies. It shows at least one
strategy significantly improved the classification accuracy with LDA (p<=0.05)
and non-decreasing classification accuracy with SVM in each task. In
particular, our proposed strategy demonstrated both effectiveness and
robustness in boosting the classification model generalizability, which can be
employed in other machine learning applications.",http://arxiv.org/pdf/2102.03088v3,cs.LG
2021-01-28 14:06:25+00:00,Copula-based conformal prediction for Multi-Target Regression,"['Soundouss Messoudi', 'Sébastien Destercke', 'Sylvain Rousseau']","There are relatively few works dealing with conformal prediction for
multi-task learning issues, and this is particularly true for multi-target
regression. This paper focuses on the problem of providing valid (i.e.,
frequency calibrated) multi-variate predictions. To do so, we propose to use
copula functions applied to deep neural networks for inductive conformal
prediction. We show that the proposed method ensures efficiency and validity
for multi-target regression problems on various data sets.",http://arxiv.org/pdf/2101.12002v1,cs.LG
2021-01-22 10:20:50+00:00,Bending behavior of additively manufactured lattice structures: numerical characterization and experimental validation,"['Nina Korshunova', 'Gianluca Alaimo', 'Seyyed Bahram Hosseini', 'Massimo Carraturo', 'Alessandro Reali', 'Jarkko Niiranen', 'Ferdinando Auricchio', 'Ernst Rank', 'Stefan Kollmannsberger']","Selective Laser Melting (SLM) technology has undergone significant
development in the past years providing unique flexibility for the fabrication
of complex metamaterials such as octet-truss lattices. However, the
microstructure of the final parts can exhibit significant variations due to the
high complexity of the manufacturing process. Consequently, the mechanical
behavior of these lattices is strongly dependent on the process-induced
defects, raising the importance on the incorporation of as-manufactured
geometries into the computational structural analysis. This, in turn,
challenges the traditional mesh-conforming methods making the computational
costs prohibitively large. In the present work, an immersed image-to-analysis
framework is applied to efficiently evaluate the bending behavior of AM
lattices. To this end, we employ the Finite Cell Method (FCM) to perform a
three-dimensional numerical analysis of the three-point bending test of a
lattice structure and compare the as-designed to as-manufactured effective
properties. Furthermore, we undertake a comprehensive study on the
applicability of dimensionally reduced beam models to the prediction of the
bending behavior of lattice beams and validate classical and strain gradient
beam theories applied in combination with the FCM. The numerical findings
suggest that the SLM octet-truss lattices exhibit size effects, thus, requiring
a flexible framework to incorporate high-order continuum theories.",http://arxiv.org/pdf/2101.09034v1,cs.CE
2021-01-18 21:45:35+00:00,Dissonance Between Human and Machine Understanding,"['Zijian Zhang', 'Jaspreet Singh', 'Ujwal Gadiraju', 'Avishek Anand']","Complex machine learning models are deployed in several critical domains
including healthcare and autonomous vehicles nowadays, albeit as functional
black boxes. Consequently, there has been a recent surge in interpreting
decisions of such complex models in order to explain their actions to humans.
Models that correspond to human interpretation of a task are more desirable in
certain contexts and can help attribute liability, build trust, expose biases
and in turn build better models. It is, therefore, crucial to understand how
and which models conform to human understanding of tasks. In this paper, we
present a large-scale crowdsourcing study that reveals and quantifies the
dissonance between human and machine understanding, through the lens of an
image classification task. In particular, we seek to answer the following
questions: Which (well-performing) complex ML models are closer to humans in
their use of features to make accurate predictions? How does task difficulty
affect the feature selection capability of machines in comparison to humans?
Are humans consistently better at selecting features that make image
recognition more accurate? Our findings have important implications on
human-machine collaboration, considering that a long term goal in the field of
artificial intelligence is to make machines capable of learning and reasoning
like humans.",http://arxiv.org/pdf/2101.07337v1,cs.AI
2021-01-13 17:54:08+00:00,On Misspecification in Prediction Problems and Robustness via Improper Learning,"['Annie Marsden', 'John Duchi', 'Gregory Valiant']","We study probabilistic prediction games when the underlying model is
misspecified, investigating the consequences of predicting using an incorrect
parametric model. We show that for a broad class of loss functions and
parametric families of distributions, the regret of playing a ""proper""
predictor -- one from the putative model class -- relative to the best
predictor in the same model class has lower bound scaling at least as
$\sqrt{\gamma n}$, where $\gamma$ is a measure of the model misspecification to
the true distribution in terms of total variation distance. In contrast, using
an aggregation-based (improper) learner, one can obtain regret $d \log n$ for
any underlying generating distribution, where $d$ is the dimension of the
parameter; we exhibit instances in which this is unimprovable even over the
family of all learners that may play distributions in the convex hull of the
parametric family. These results suggest that simple strategies for aggregating
multiple learners together should be more robust, and several experiments
conform to this hypothesis.",http://arxiv.org/pdf/2101.05234v2,stat.ML
2021-01-05 19:08:11+00:00,"Online Multivalid Learning: Means, Moments, and Prediction Intervals","['Varun Gupta', 'Christopher Jung', 'Georgy Noarov', 'Mallesh M. Pai', 'Aaron Roth']","We present a general, efficient technique for providing contextual
predictions that are ""multivalid"" in various senses, against an online sequence
of adversarially chosen examples $(x,y)$. This means that the resulting
estimates correctly predict various statistics of the labels $y$ not just
marginally -- as averaged over the sequence of examples -- but also
conditionally on $x \in G$ for any $G$ belonging to an arbitrary intersecting
collection of groups $\mathcal{G}$.
  We provide three instantiations of this framework. The first is mean
prediction, which corresponds to an online algorithm satisfying the notion of
multicalibration from Hebert-Johnson et al. The second is variance and higher
moment prediction, which corresponds to an online algorithm satisfying the
notion of mean-conditioned moment multicalibration from Jung et al. Finally, we
define a new notion of prediction interval multivalidity, and give an algorithm
for finding prediction intervals which satisfy it. Because our algorithms
handle adversarially chosen examples, they can equally well be used to predict
statistics of the residuals of arbitrary point prediction methods, giving rise
to very general techniques for quantifying the uncertainty of predictions of
black box algorithms, even in an online adversarial setting. When instantiated
for prediction intervals, this solves a similar problem as conformal
prediction, but in an adversarial environment and with multivalidity guarantees
stronger than simple marginal coverage guarantees.",http://arxiv.org/pdf/2101.01739v1,cs.LG
2021-01-05 16:09:10+00:00,Auto-Encoding Molecular Conformations,"['Robin Winter', 'Frank Noé', 'Djork-Arné Clevert']","In this work we introduce an Autoencoder for molecular conformations. Our
proposed model converts the discrete spatial arrangements of atoms in a given
molecular graph (conformation) into and from a continuous fixed-sized latent
representation. We demonstrate that in this latent representation, similar
conformations cluster together while distinct conformations split apart.
Moreover, by training a probabilistic model on a large dataset of molecular
conformations, we demonstrate how our model can be used to generate diverse
sets of energetically favorable conformations for a given molecule. Finally, we
show that the continuous representation allows us to utilize optimization
methods to find molecules that have conformations with favourable spatial
properties.",http://arxiv.org/pdf/2101.01618v1,cs.LG
2020-12-28 14:33:03+00:00,Testing for concept shift online,['Vladimir Vovk'],"This note continues study of exchangeability martingales, i.e., processes
that are martingales under any exchangeable distribution for the observations.
Such processes can be used for detecting violations of the IID assumption,
which is commonly made in machine learning. Violations of the IID assumption
are sometimes referred to as dataset shift, and dataset shift is sometimes
subdivided into concept shift, covariate shift, etc. Our primary interest is in
concept shift, but we will also discuss exchangeability martingales that
decompose perfectly into two components one of which detects concept shift and
the other detects what we call label shift. Our methods will be based on
techniques of conformal prediction.",http://arxiv.org/pdf/2012.14246v1,cs.LG
2020-12-28 12:33:52+00:00,TransPose: Keypoint Localization via Transformer,"['Sen Yang', 'Zhibin Quan', 'Mu Nie', 'Wankou Yang']","While CNN-based models have made remarkable progress on human pose
estimation, what spatial dependencies they capture to localize keypoints
remains unclear. In this work, we propose a model called \textbf{TransPose},
which introduces Transformer for human pose estimation. The attention layers
built in Transformer enable our model to capture long-range relationships
efficiently and also can reveal what dependencies the predicted keypoints rely
on. To predict keypoint heatmaps, the last attention layer acts as an
aggregator, which collects contributions from image clues and forms maximum
positions of keypoints. Such a heatmap-based localization approach via
Transformer conforms to the principle of Activation
Maximization~\cite{erhan2009visualizing}. And the revealed dependencies are
image-specific and fine-grained, which also can provide evidence of how the
model handles special cases, e.g., occlusion. The experiments show that
TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets,
while being more lightweight and faster than mainstream CNN architectures. The
TransPose model also transfers very well on MPII benchmark, achieving superior
performance on the test set when fine-tuned with small training costs. Code and
pre-trained models are publicly
available\footnote{\url{https://github.com/yangsenius/TransPose}}.",http://arxiv.org/pdf/2012.14214v5,cs.CV
2020-12-22 09:45:40+00:00,Scalable Online Conformance Checking Using Incremental Prefix-Alignment Computation,"['Daniel Schuster', 'Gero J. Kolhof']","Conformance checking techniques aim to collate observed process behavior with
normative/modeled process models. The majority of existing approaches focuses
on completed process executions, i.e., offline conformance checking. Recently,
novel approaches have been designed to monitor ongoing processes, i.e., online
conformance checking. Such techniques detect deviations of an ongoing process
execution from a normative process model at the moment they occur. Thereby,
countermeasures can be taken immediately to prevent a process deviation from
causing further, undesired consequences. Most online approaches only allow to
detect approximations of deviations. This causes the problem of falsely
detected deviations, i.e., detected deviations that are actually no deviations.
We have, therefore, recently introduced a novel approach to compute exact
conformance checking results in an online environment. In this paper, we focus
on the practical application and present a scalable, distributed implementation
of the proposed online conformance checking approach. Moreover, we present two
extensions to said approach to reduce its computational effort and its
practical applicability. We evaluate our implementation using data sets
capturing the execution of real processes.",http://arxiv.org/pdf/2101.00958v1,cs.LO
2020-12-15 17:44:48+00:00,Molecular machine learning with conformer ensembles,"['Simon Axelrod', 'Rafael Gomez-Bombarelli']","Virtual screening can accelerate drug discovery by identifying promising
candidates for experimental evaluation. Machine learning is a powerful method
for screening, as it can learn complex structure-property relationships from
experimental data and make rapid predictions over virtual libraries. Molecules
inherently exist as a three-dimensional ensemble and their biological action
typically occurs through supramolecular recognition. However, most deep
learning approaches to molecular property prediction use a 2D graph
representation as input, and in some cases a single 3D conformation. Here we
investigate how the 3D information of multiple conformers, traditionally known
as 4D information in the cheminformatics community, can improve molecular
property prediction in deep learning models. We introduce multiple deep
learning models that expand upon key architectures such as ChemProp and Schnet,
adding elements such as multiple-conformer inputs and conformer attention. We
then benchmark the performance trade-offs of these models on 2D, 3D and 4D
representations in the prediction of drug activity using a large training set
of geometrically resolved molecules. The new architectures perform
significantly better than 2D models, but their performance is often just as
strong with a single conformer as with many. We also find that 4D deep learning
models learn interpretable attention weights for each conformer.",http://arxiv.org/pdf/2012.08452v2,cs.LG
2020-12-15 13:19:03+00:00,High throughput screening with machine learning,"['Oleksandr Gurbych', 'Maksym Druchok', 'Dzvenymyra Yarish', 'Sofiya Garkot']","This study assesses the efficiency of several popular machine learning
approaches in the prediction of molecular binding affinity: CatBoost, Graph
Attention Neural Network, and Bidirectional Encoder Representations from
Transformers. The models were trained to predict binding affinities in terms of
inhibition constants $K_i$ for pairs of proteins and small organic molecules.
First two approaches use thoroughly selected physico-chemical features, while
the third one is based on textual molecular representations - it is one of the
first attempts to apply Transformer-based predictors for the binding affinity.
We also discuss the visualization of attention layers within the Transformer
approach in order to highlight the molecular sites responsible for
interactions. All approaches are free from atomic spatial coordinates thus
avoiding bias from known structures and being able to generalize for compounds
with unknown conformations. The achieved accuracy for all suggested approaches
prove their potential in high throughput screening.",http://arxiv.org/pdf/2012.08275v1,cs.LG
2020-12-11 18:51:44+00:00,"Glucose values prediction five years ahead with a new framework of missing responses in reproducing kernel Hilbert spaces, and the use of continuous glucose monitoring technology","['Marcos Matabuena', 'Paulo Félix', 'Carlos Meijide-Garcia', 'Francisco Gude']","AEGIS study possesses unique information on longitudinal changes in
circulating glucose through continuous glucose monitoring technology (CGM).
However, as usual in longitudinal medical studies, there is a significant
amount of missing data in the outcome variables. For example, 40 percent of
glycosylated hemoglobin (A1C) biomarker data are missing five years ahead. With
the purpose to reduce the impact of this issue, this article proposes a new
data analysis framework based on learning in reproducing kernel Hilbert spaces
(RKHS) with missing responses that allows to capture non-linear relations
between variable studies in different supervised modeling tasks. First, we
extend the Hilbert-Schmidt dependence measure to test statistical independence
in this context introducing a new bootstrap procedure, for which we prove
consistency. Next, we adapt or use existing models of variable selection,
regression, and conformal inference to obtain new clinical findings about
glucose changes five years ahead with the AEGIS data. The most relevant
findings are summarized below: i) We identify new factors associated with
long-term glucose evolution; ii) We show the clinical sensibility of CGM data
to detect changes in glucose metabolism; iii) We can improve clinical
interventions based on our algorithms' expected glucose changes according to
patients' baseline characteristics.",http://arxiv.org/pdf/2012.06564v2,stat.ML
2020-12-11 06:17:43+00:00,Theory-guided hard constraint projection (HCP): a knowledge-based data-driven scientific machine learning method,"['Yuntian Chen', 'Dou Huang', 'Dongxiao Zhang', 'Junsheng Zeng', 'Nanzhe Wang', 'Haoran Zhang', 'Jinyue Yan']","Machine learning models have been successfully used in many scientific and
engineering fields. However, it remains difficult for a model to simultaneously
utilize domain knowledge and experimental observation data. The application of
knowledge-based symbolic AI represented by an expert system is limited by the
expressive ability of the model, and data-driven connectionism AI represented
by neural networks is prone to produce predictions that violate physical
mechanisms. In order to fully integrate domain knowledge with observations, and
make full use of the prior information and the strong fitting ability of neural
networks, this study proposes theory-guided hard constraint projection (HCP).
This model converts physical constraints, such as governing equations, into a
form that is easy to handle through discretization, and then implements hard
constraint optimization through projection. Based on rigorous mathematical
proofs, theory-guided HCP can ensure that model predictions strictly conform to
physical mechanisms in the constraint patch. The performance of the
theory-guided HCP is verified by experiments based on the heterogeneous
subsurface flow problem. Due to the application of hard constraints, compared
with fully connected neural networks and soft constraint models, such as
theory-guided neural networks and physics-informed neural networks,
theory-guided HCP requires fewer data, and achieves higher prediction accuracy
and stronger robustness to noisy observations.",http://arxiv.org/pdf/2012.06148v2,cs.LG
2020-12-04 07:55:15+00:00,Spread Mechanism and Influence Measurement of Online Rumors in China During the COVID-19 Pandemic,"['Yiou Lin', 'Hang Lei', 'Yu Deng']","In early 2020, the Corona Virus Disease 2019 (COVID-19) pandemic swept the
world.In China, COVID-19 has caused severe consequences. Moreover, online
rumors during the COVID-19 pandemic increased people's panic about public
health and social stability. At present, understanding and curbing the spread
of online rumors is an urgent task. Therefore, we analyzed the rumor spreading
mechanism and propose a method to quantify a rumors' influence by the speed of
new insiders. The search frequency of the rumor is used as an observation
variable of new insiders. The peak coefficient and the attenuation coefficient
are calculated for the search frequency, which conforms to the exponential
distribution. We designed several rumor features and used the above two
coefficients as predictable labels. A 5-fold cross-validation experiment using
the mean square error (MSE) as the loss function showed that the decision tree
was suitable for predicting the peak coefficient, and the linear regression
model was ideal for predicting the attenuation coefficient. Our feature
analysis showed that precursor features were the most important for the
outbreak coefficient, while location information and rumor entity information
were the most important for the attenuation coefficient. Meanwhile, features
that were conducive to the outbreak were usually harmful to the continued
spread of rumors. At the same time, anxiety was a crucial rumor causing factor.
Finally, we discuss how to use deep learning technology to reduce the forecast
loss by using the Bidirectional Encoder Representations from Transformers
(BERT) model.",http://arxiv.org/pdf/2012.02446v2,cs.SI
2020-11-25 15:23:52+00:00,Attention-Based Learning on Molecular Ensembles,"['Kangway V. Chuang', 'Michael J. Keiser']","The three-dimensional shape and conformation of small-molecule ligands are
critical for biomolecular recognition, yet encoding 3D geometry has not
improved ligand-based virtual screening approaches. We describe an end-to-end
deep learning approach that operates directly on small-molecule conformational
ensembles and identifies key conformational poses of small-molecules. Our
networks leverage two levels of representation learning: 1) individual
conformers are first encoded as spatial graphs using a graph neural network,
and 2) sampled conformational ensembles are represented as sets using an
attention mechanism to aggregate over individual instances. We demonstrate the
feasibility of this approach on a simple task based on bidentate coordination
of biaryl ligands, and show how attention-based pooling can elucidate key
conformational poses in tasks based on molecular geometry. This work
illustrates how set-based learning approaches may be further developed for
small molecule-based virtual screening.",http://arxiv.org/pdf/2011.12820v1,cs.LG
2020-11-22 13:41:54+00:00,Enriching ImageNet with Human Similarity Judgments and Psychological Embeddings,"['Brett D. Roads', 'Bradley C. Love']","Advances in object recognition flourished in part because of the availability
of high-quality datasets and associated benchmarks. However, these
benchmarks---such as ILSVRC---are relatively task-specific, focusing
predominately on predicting class labels. We introduce a publicly-available
dataset that embodies the task-general capabilities of human perception and
reasoning. The Human Similarity Judgments extension to ImageNet (ImageNet-HSJ)
is composed of human similarity judgments that supplement the ILSVRC validation
set. The new dataset supports a range of task and performance metrics,
including the evaluation of unsupervised learning algorithms. We demonstrate
two methods of assessment: using the similarity judgments directly and using a
psychological embedding trained on the similarity judgments. This embedding
space contains an order of magnitude more points (i.e., images) than previous
efforts based on human judgments. Scaling to the full 50,000 image set was made
possible through a selective sampling process that used variational Bayesian
inference and model ensembles to sample aspects of the embedding space that
were most uncertain. This methodological innovation not only enables scaling,
but should also improve the quality of solutions by focusing sampling where it
is needed. To demonstrate the utility of ImageNet-HSJ, we used the similarity
ratings and the embedding space to evaluate how well several popular models
conform to human similarity judgments. One finding is that more complex models
that perform better on task-specific benchmarks do not better conform to human
semantic judgments. In addition to the human similarity judgments, pre-trained
psychological embeddings and code for inferring variational embeddings are made
publicly available. Collectively, ImageNet-HSJ assets support the appraisal of
internal representations and the development of more human-like models.",http://arxiv.org/pdf/2011.11015v1,cs.CV
2020-11-12 22:51:25+00:00,Trajectory Prediction in Autonomous Driving with a Lane Heading Auxiliary Loss,"['Ross Greer', 'Nachiket Deo', 'Mohan Trivedi']","Predicting a vehicle's trajectory is an essential ability for autonomous
vehicles navigating through complex urban traffic scenes. Bird's-eye-view
roadmap information provides valuable information for making trajectory
predictions, and while state-of-the-art models extract this information via
image convolution, auxiliary loss functions can augment patterns inferred from
deep learning by further encoding common knowledge of social and legal driving
behaviors. Since human driving behavior is inherently multimodal, models which
allow for multimodal output tend to outperform single-prediction models on
standard metrics. We propose a loss function which enhances such models by
enforcing expected driving rules on all predicted modes. Our contribution to
trajectory prediction is twofold; we propose a new metric which addresses
failure cases of the off-road rate metric by penalizing trajectories that
oppose the ascribed heading (flow direction) of a driving lane, and we show
this metric to be differentiable and therefore suitable as an auxiliary loss
function. We then use this auxiliary loss to extend the the standard multiple
trajectory prediction (MTP) and MultiPath models, achieving improved results on
the nuScenes prediction benchmark by predicting trajectories which better
conform to the lane-following rules of the road.",http://arxiv.org/pdf/2011.06679v2,cs.CV
2020-11-06 20:37:15+00:00,Reactive motion planning with probabilistic safety guarantees,"['Yuxiao Chen', 'Ugo Rosolia', 'Chuchu Fan', 'Aaron D. Ames', 'Richard Murray']","Motion planning in environments with multiple agents is critical to many
important autonomous applications such as autonomous vehicles and assistive
robots. This paper considers the problem of motion planning, where the
controlled agent shares the environment with multiple uncontrolled agents.
First, a predictive model of the uncontrolled agents is trained to predict all
possible trajectories within a short horizon based on the scenario. The
prediction is then fed to a motion planning module based on model predictive
control. We proved generalization bound for the predictive model using three
different methods, post-bloating, support vector machine (SVM), and conformal
analysis, all capable of generating stochastic guarantees of the correctness of
the predictor. The proposed approach is demonstrated in simulation in a
scenario emulating autonomous highway driving.",http://arxiv.org/pdf/2011.03590v2,cs.RO
2020-10-28 11:17:24+00:00,Object shape error modelling and simulation during early design stage by morphing Gaussian Random Fields,"['Manoj Babu', 'Pasquale Franciosa', 'Prashanth Shekar', 'Darek Ceglarek']","Geometric and dimensional variations in objects are caused by inevitable
uncertainties in manufacturing processes and often lead to product quality
issues. Failing to model the effect object shape errors, i.e., geometric and
dimensional errors of parts, early during design phase inhibits the ability to
predict such quality issues; consequently leading to expensive design changes
after freezing of design. State-of-Art methodologies for modelling and
simulating object shape error have limited defect fidelity, data versatility,
and designer centricity that prevent their effective application during early
design phase. Overcoming these limitations a novel Morphing Gaussian Random
Field (MGRF) methodology for object shape error modelling and simulation is
presented in this paper. The MGRF methodology has (i) high defect fidelity and
is capable of simulating various part defects including local and global
deformations, and technological patterns; (ii) high data versatility and can
effectively simulate non-ideal parts under the constraint of limited data
availability and can utilise historical non-ideal part data; (iii) designer
centric capabilities such as performing `what if?' analysis of practically
relevant defects; and (iv) capability to generate non-ideal parts conforming to
statistical form tolerance specification. The aforementioned capabilities
enable MGRF methodology to accurately model and simulate the effect of object
shape variations on product quality during the early design phase. This is
achieved by first, modelling the spatial correlation in the deviations of the
part from its design nominal using Gaussian Random Field and then, utilising
the modelled spatial correlations to generate non-ideal parts by conditional
simulations. Practical applications of developed MGRF methodology and its
advantages are demonstrated using sport-utility-vehicle door parts.",http://arxiv.org/pdf/2010.14889v2,cs.CE
2020-10-26 00:53:05+00:00,COVID-19 in Spain and India: Comparing Policy Implications by Analyzing Epidemiological and Social Media Data,"['Parth Asawa', 'Manas Gaur', 'Kaushik Roy', 'Amit Sheth']","The COVID-19 pandemic has forced public health experts to develop contingent
policies to stem the spread of infection, including measures such as
partial/complete lockdowns. The effectiveness of these policies has varied with
geography, population distribution, and effectiveness in implementation.
Consequently, some nations (e.g., Taiwan, Haiti) have been more successful than
others (e.g., United States) in curbing the outbreak. A data-driven
investigation into effective public health policies of a country would allow
public health experts in other nations to decide future courses of action to
control the outbreaks of disease and epidemics. We chose Spain and India to
present our analysis on regions that were similar in terms of certain factors:
(1) population density, (2) unemployment rate, (3) tourism, and (4) quality of
living. We posit that citizen ideology obtainable from twitter conversations
can provide insights into conformity to policy and suitably reflect on future
case predictions. A milestone when the curves show the number of new cases
diverging from each other is used to define a time period to extract
policy-related tweets while the concepts from a causality network of
policy-dependent sub-events are used to generate concept clouds. The number of
new cases is predicted using sentiment scores in a regression model. We see
that the new case predictions reflects twitter sentiment, meaningfully tied to
a trigger sub-event that enables policy-related findings for Spain and India to
be effectively compared.",http://arxiv.org/pdf/2010.14628v1,cs.SI
2020-10-23 02:40:28+00:00,Graph Geometry Interaction Learning,"['Shichao Zhu', 'Shirui Pan', 'Chuan Zhou', 'Jia Wu', 'Yanan Cao', 'Bin Wang']","While numerous approaches have been developed to embed graphs into either
Euclidean or hyperbolic spaces, they do not fully utilize the information
available in graphs, or lack the flexibility to model intrinsic complex graph
geometry. To utilize the strength of both Euclidean and hyperbolic geometries,
we develop a novel Geometry Interaction Learning (GIL) method for graphs, a
well-suited and efficient alternative for learning abundant geometric
properties in graph. GIL captures a more informative internal structural
features with low dimensions while maintaining conformal invariance of each
space. Furthermore, our method endows each node the freedom to determine the
importance of each geometry space via a flexible dual feature interaction
learning and probability assembling mechanism. Promising experimental results
are presented for five benchmark datasets on node classification and link
prediction tasks.",http://arxiv.org/pdf/2010.12135v1,cs.LG
2020-10-18 21:05:32+00:00,Conformal prediction for time series,"['Chen Xu', 'Yao Xie']","We develop a general framework for constructing distribution-free prediction
intervals for time series. Theoretically, we establish explicit bounds on
conditional and marginal coverage gaps of estimated prediction intervals, which
asymptotically converge to zero under additional assumptions. We obtain similar
bounds on the size of set differences between oracle and estimated prediction
intervals. Methodologically, we introduce a computationally efficient algorithm
called \texttt{EnbPI} that wraps around ensemble predictors, which is closely
related to conformal prediction (CP) but does not require data exchangeability.
\texttt{EnbPI} avoids data-splitting and is computationally efficient by
avoiding retraining and thus scalable to sequentially producing prediction
intervals. We perform extensive simulation and real-data analyses to
demonstrate its effectiveness compared with existing methods. We also discuss
the extension of \texttt{EnbPI} on various other applications.",http://arxiv.org/pdf/2010.09107v15,stat.ME
2020-10-14 15:03:29+00:00,A Two-Sample Conditional Distribution Test Using Conformal Prediction and Weighted Rank Sum,"['Xiaoyu Hu', 'Jing Lei']","We consider the problem of testing the equality of conditional distributions
of a response variable given a vector of covariates between two populations.
Such a hypothesis testing problem can be motivated from various machine
learning and statistical inference scenarios, including transfer learning and
causal predictive inference. We develop a nonparametric test procedure inspired
from the conformal prediction framework. The construction of our test statistic
combines recent developments in conformal prediction with a novel choice of
conformity score, resulting in a weighted rank-sum test statistic that is valid
and powerful under general settings. To our knowledge, this is the first
successful attempt of using conformal prediction for testing statistical
hypotheses beyond exchangeability. Our method is suitable for modern machine
learning scenarios where the data has high dimensionality and large sample
sizes, and can be effectively combined with existing classification algorithms
to find good conformity score functions. The performance of the proposed method
is demonstrated in various numerical examples.",http://arxiv.org/pdf/2010.07147v2,stat.ME
2020-10-09 23:06:57+00:00,Conformal retrofitting via Riemannian manifolds: distilling task-specific graphs into pretrained embeddings,"['Justin Dieter', 'Arun Tejasvi Chaganty']","Pretrained (language) embeddings are versatile, task-agnostic feature
representations of entities, like words, that are central to many machine
learning applications. These representations can be enriched through
retrofitting, a class of methods that incorporate task-specific domain
knowledge encoded as a graph over a subset of these entities. However, existing
retrofitting algorithms face two limitations: they overfit the observed graph
by failing to represent relationships with missing entities; and they underfit
the observed graph by only learning embeddings in Euclidean manifolds, which
cannot faithfully represent even simple tree-structured or cyclic graphs. We
address these problems with two key contributions: (i) we propose a novel
regularizer, a conformality regularizer, that preserves local geometry from the
pretrained embeddings---enabling generalization to missing entities and (ii) a
new Riemannian feedforward layer that learns to map pre-trained embeddings onto
a non-Euclidean manifold that can better represent the entire graph. Through
experiments on WordNet, we demonstrate that the conformality regularizer
prevents even existing (Euclidean-only) methods from overfitting on link
prediction for missing entities, and---together with the Riemannian feedforward
layer---learns non-Euclidean embeddings that outperform them.",http://arxiv.org/pdf/2010.04842v1,cs.LG
2020-10-08 09:21:36+00:00,Transcending Transcend: Revisiting Malware Classification in the Presence of Concept Drift,"['Federico Barbero', 'Feargus Pendlebury', 'Fabio Pierazzi', 'Lorenzo Cavallaro']","Machine learning for malware classification shows encouraging results, but
real deployments suffer from performance degradation as malware authors adapt
their techniques to evade detection. This phenomenon, known as concept drift,
occurs as new malware examples evolve and become less and less like the
original training examples. One promising method to cope with concept drift is
classification with rejection in which examples that are likely to be
misclassified are instead quarantined until they can be expertly analyzed.
  We propose TRANSCENDENT, a rejection framework built on Transcend, a recently
proposed strategy based on conformal prediction theory. In particular, we
provide a formal treatment of Transcend, enabling us to refine conformal
evaluation theory -- its underlying statistical engine -- and gain a better
understanding of the theoretical reasons for its effectiveness. In the process,
we develop two additional conformal evaluators that match or surpass the
performance of the original while significantly decreasing the computational
overhead. We evaluate TRANSCENDENT on a malware dataset spanning 5 years that
removes sources of experimental bias present in the original evaluation.
TRANSCENDENT outperforms state-of-the-art approaches while generalizing across
different malware domains and classifiers.
  To further assist practitioners, we determine the optimal operational
settings for a TRANSCENDENT deployment and show how it can be applied to many
popular learning algorithms. These insights support both old and new empirical
findings, making Transcend a sound and practical solution for the first time.
To this end, we release TRANSCENDENT as open source, to aid the adoption of
rejection strategies by the security community.",http://arxiv.org/pdf/2010.03856v5,cs.CR
2020-10-05 18:00:15+00:00,Acrostic Poem Generation,"['Rajat Agarwal', 'Katharina Kann']","We propose a new task in the area of computational creativity: acrostic poem
generation in English. Acrostic poems are poems that contain a hidden message;
typically, the first letter of each line spells out a word or short phrase. We
define the task as a generation task with multiple constraints: given an input
word, 1) the initial letters of each line should spell out the provided word,
2) the poem's semantics should also relate to it, and 3) the poem should
conform to a rhyming scheme. We further provide a baseline model for the task,
which consists of a conditional neural language model in combination with a
neural rhyming model. Since no dedicated datasets for acrostic poem generation
exist, we create training data for our task by first training a separate topic
prediction model on a small set of topic-annotated poems and then predicting
topics for additional poems. Our experiments show that the acrostic poems
generated by our baseline are received well by humans and do not lose much
quality due to the additional constraints. Last, we confirm that poems
generated by our model are indeed closely related to the provided prompts, and
that pretraining on Wikipedia can boost performance.",http://arxiv.org/pdf/2010.02239v1,cs.CL
2020-10-02 17:17:45+00:00,Goal-GAN: Multimodal Trajectory Prediction Based on Goal Position Estimation,"['Patrick Dendorfer', 'Aljoša Ošep', 'Laura Leal-Taixé']","In this paper, we present Goal-GAN, an interpretable and end-to-end trainable
model for human trajectory prediction. Inspired by human navigation, we model
the task of trajectory prediction as an intuitive two-stage process: (i) goal
estimation, which predicts the most likely target positions of the agent,
followed by a (ii) routing module which estimates a set of plausible
trajectories that route towards the estimated goal. We leverage information
about the past trajectory and visual context of the scene to estimate a
multi-modal probability distribution over the possible goal positions, which is
used to sample a potential goal during the inference. The routing is governed
by a recurrent neural network that reacts to physical constraints in the nearby
surroundings and generates feasible paths that route towards the sampled goal.
Our extensive experimental evaluation shows that our method establishes a new
state-of-the-art on several benchmarks while being able to generate a realistic
and diverse set of trajectories that conform to physical constraints.",http://arxiv.org/pdf/2010.01114v1,cs.CV
2020-09-29 17:58:04+00:00,Uncertainty Sets for Image Classifiers using Conformal Prediction,"['Anastasios Angelopoulos', 'Stephen Bates', 'Jitendra Malik', 'Michael I. Jordan']","Convolutional image classifiers can achieve high predictive accuracy, but
quantifying their uncertainty remains an unresolved challenge, hindering their
deployment in consequential settings. Existing uncertainty quantification
techniques, such as Platt scaling, attempt to calibrate the network's
probability estimates, but they do not have formal guarantees. We present an
algorithm that modifies any classifier to output a predictive set containing
the true label with a user-specified probability, such as 90%. The algorithm is
simple and fast like Platt scaling, but provides a formal finite-sample
coverage guarantee for every model and dataset. Our method modifies an existing
conformal prediction algorithm to give more stable predictive sets by
regularizing the small scores of unlikely classes after Platt scaling. In
experiments on both Imagenet and Imagenet-V2 with ResNet-152 and other
classifiers, our scheme outperforms existing approaches, achieving coverage
with sets that are often factors of 5 to 10 smaller than a stand-alone Platt
scaling baseline.",http://arxiv.org/pdf/2009.14193v5,cs.CV
2020-09-27 01:04:06+00:00,Stylized Dialogue Response Generation Using Stylized Unpaired Texts,"['Yinhe Zheng', 'Zikai Chen', 'Rongsheng Zhang', 'Shilei Huang', 'Xiaoxi Mao', 'Minlie Huang']","Generating stylized responses is essential to build intelligent and engaging
dialogue systems. However, this task is far from well-explored due to the
difficulties of rendering a particular style in coherent responses, especially
when the target style is embedded only in unpaired texts that cannot be
directly used to train the dialogue model. This paper proposes a stylized
dialogue generation method that can capture stylistic features embedded in
unpaired texts. Specifically, our method can produce dialogue responses that
are both coherent to the given context and conform to the target style. In this
study, an inverse dialogue model is first introduced to predict possible posts
for the input responses, and then this inverse model is used to generate
stylized pseudo dialogue pairs based on these stylized unpaired texts. Further,
these pseudo pairs are employed to train the stylized dialogue model with a
joint training process, and a style routing approach is proposed to intensify
stylistic features in the decoder. Automatic and manual evaluations on two
datasets demonstrate that our method outperforms competitive baselines in
producing coherent and style-intensive dialogue responses.",http://arxiv.org/pdf/2009.12719v2,cs.CL
2020-09-23 12:33:09+00:00,A Variational Auto-Encoder for Reservoir Monitoring,"['Kristian Gundersen', 'Seyyed A. Hosseini', 'Anna Oleynik', 'Guttorm Alendal']","Carbon dioxide Capture and Storage (CCS) is an important strategy in
mitigating anthropogenic CO$_2$ emissions. In order for CCS to be successful,
large quantities of CO$_2$ must be stored and the storage site conformance must
be monitored. Here we present a deep learning method to reconstruct pressure
fields and classify the flux out of the storage formation based on the pressure
data from Above Zone Monitoring Interval (AZMI) wells. The deep learning method
is a version of a semi conditional variational auto-encoder tailored to solve
two tasks: reconstruction of an incremental pressure field and leakage rate
classification. The method, predictions and associated uncertainty estimates
are illustrated on the synthetic data from a high-fidelity heterogeneous 2D
numerical reservoir model, which was used to simulate subsurface CO$_2$
movement and pressure changes in the AZMI due to a CO$_2$ leakage.",http://arxiv.org/pdf/2009.11693v2,cs.LG
2020-09-10 03:38:23+00:00,Efficient conformal parameterization of multiply-connected surfaces using quasi-conformal theory,['Gary P. T. Choi'],"Conformal mapping, a classical topic in complex analysis and differential
geometry, has become a subject of great interest in the area of surface
parameterization in recent decades with various applications in science and
engineering. However, most of the existing conformal parameterization
algorithms only focus on simply-connected surfaces and cannot be directly
applied to surfaces with holes. In this work, we propose two novel algorithms
for computing the conformal parameterization of multiply-connected surfaces. We
first develop an efficient method for conformally parameterizing an open
surface with one hole to an annulus on the plane. Based on this method, we then
develop an efficient method for conformally parameterizing an open surface with
$k$ holes onto a unit disk with $k$ circular holes. The conformality and
bijectivity of the mappings are ensured by quasi-conformal theory. Numerical
experiments and applications are presented to demonstrate the effectiveness of
the proposed methods.",http://arxiv.org/pdf/2009.08279v2,cs.GR
2020-08-31 13:47:12+00:00,$β$-Cores: Robust Large-Scale Bayesian Data Summarization in the Presence of Outliers,"['Dionysis Manousakas', 'Cecilia Mascolo']","Modern machine learning applications should be able to address the intrinsic
challenges arising over inference on massive real-world datasets, including
scalability and robustness to outliers. Despite the multiple benefits of
Bayesian methods (such as uncertainty-aware predictions, incorporation of
experts knowledge, and hierarchical modeling), the quality of classic Bayesian
inference depends critically on whether observations conform with the assumed
data generating model, which is impossible to guarantee in practice. In this
work, we propose a variational inference method that, in a principled way, can
simultaneously scale to large datasets, and robustify the inferred posterior
with respect to the existence of outliers in the observed data. Reformulating
Bayes theorem via the $\beta$-divergence, we posit a robustified
pseudo-Bayesian posterior as the target of inference. Moreover, relying on the
recent formulations of Riemannian coresets for scalable Bayesian inference, we
propose a sparse variational approximation of the robustified posterior and an
efficient stochastic black-box algorithm to construct it. Overall our method
allows releasing cleansed data summaries that can be applied broadly in
scenarios including structured data corruption. We illustrate the applicability
of our approach in diverse simulated and real datasets, and various statistical
models, including Gaussian mean inference, logistic and neural linear
regression, demonstrating its superiority to existing Bayesian summarization
methods in the presence of outliers.",http://arxiv.org/pdf/2008.13600v2,cs.LG
2020-08-26 16:47:02+00:00,Improving Fairness in Criminal Justice Algorithmic Risk Assessments Using Conformal Prediction Sets,"['Richard A. Berk', 'Arun Kumar Kuchibhotla']","Risk assessment algorithms have been correctly criticized for potential
unfairness, and there is an active cottage industry trying to make repairs. In
this paper, we adopt a framework from conformal prediction sets to remove
unfairness from risk algorithms themselves and the covariates used for
forecasting. From a sample of 300,000 offenders at their arraignments, we
construct a confusion table and its derived measures of fairness that are
effectively free any meaningful differences between Black and White offenders.
We also produce fair forecasts for individual offenders coupled with valid
probability guarantees that the forecasted outcome is the true outcome. We see
our work as a demonstration of concept for application in a wide variety of
criminal justice decisions. The procedures provided can be routinely
implemented in jurisdictions with the usual criminal justice datasets used by
administrators. The requisite procedures can be found in the scripting software
R. However, whether stakeholders will accept our approach as a means to achieve
risk assessment fairness is unknown. There also are legal issues that would
need to be resolved although we offer a Pareto improvement.",http://arxiv.org/pdf/2008.11664v3,stat.AP
2020-08-23 12:49:55+00:00,A critical assessment of conformal prediction methods applied in binary classification settings,['Damjan Krstajic'],"In recent years there has been an increase in the number of scientific papers
that suggest using conformal predictions in drug discovery. We consider that
some versions of conformal predictions applied in binary settings are embroiled
in pitfalls, not obvious at first sight, and that it is important to inform the
scientific community about them. In the paper we first introduce the general
theory of conformal predictions and follow with an explanation of the versions
currently dominant in drug discovery research today. Finally, we provide cases
for their critical assessment in binary classification settings.",http://arxiv.org/pdf/2008.10034v2,stat.AP
2020-08-19 22:33:54+00:00,Prescriptive Business Process Monitoring for Recommending Next Best Actions,"['Sven Weinzierl', 'Sebastian Dunzer', 'Sandra Zilker', 'Martin Matzner']","Predictive business process monitoring (PBPM) techniques predict future
process behaviour based on historical event log data to improve operational
business processes. Concerning the next activity prediction, recent PBPM
techniques use state-of-the-art deep neural networks (DNNs) to learn predictive
models for producing more accurate predictions in running process instances.
Even though organisations measure process performance by key performance
indicators (KPIs), the DNN`s learning procedure is not directly affected by
them. Therefore, the resulting next most likely activity predictions can be
less beneficial in practice. Prescriptive business process monitoring (PrBPM)
approaches assess predictions regarding their impact on the process performance
(typically measured by KPIs) to prevent undesired process activities by raising
alarms or recommending actions. However, none of these approaches recommends
actual process activities as actions that are optimised according to a given
KPI. We present a PrBPM technique that transforms the next most likely
activities into the next best actions regarding a given KPI. Thereby, our
technique uses business process simulation to ensure the control-flow
conformance of the recommended actions. Based on our evaluation with two
real-life event logs, we show that our technique`s next best actions can
outperform next activity predictions regarding the optimisation of a KPI and
the distance from the actual process instances.",http://arxiv.org/pdf/2008.08693v1,cs.AI
2020-08-11 15:01:32+00:00,Transfer Learning for Protein Structure Classification at Low Resolution,"['Alexander Hudson', 'Shaogang Gong']","Structure determination is key to understanding protein function at a
molecular level. Whilst significant advances have been made in predicting
structure and function from amino acid sequence, researchers must still rely on
expensive, time-consuming analytical methods to visualise detailed protein
conformation. In this study, we demonstrate that it is possible to make
accurate ($\geq$80%) predictions of protein class and architecture from
structures determined at low ($>$3A) resolution, using a deep convolutional
neural network trained on high-resolution ($\leq$3A) structures represented as
2D matrices. Thus, we provide proof of concept for high-speed, low-cost protein
structure classification at low resolution, and a basis for extension to
prediction of function. We investigate the impact of the input representation
on classification performance, showing that side-chain information may not be
necessary for fine-grained structure predictions. Finally, we confirm that
high-resolution, low-resolution and NMR-determined structures inhabit a common
feature space, and thus provide a theoretical foundation for boosting with
single-image super-resolution.",http://arxiv.org/pdf/2008.04757v4,cs.CV
2020-08-10 17:09:16+00:00,Robust Validation: Confident Predictions Even When Distributions Shift,"['Maxime Cauchois', 'Suyash Gupta', 'Alnur Ali', 'John C. Duchi']","While the traditional viewpoint in machine learning and statistics assumes
training and testing samples come from the same population, practice belies
this fiction. One strategy -- coming from robust statistics and optimization --
is thus to build a model robust to distributional perturbations. In this paper,
we take a different approach to describe procedures for robust predictive
inference, where a model provides uncertainty estimates on its predictions
rather than point predictions. We present a method that produces prediction
sets (almost exactly) giving the right coverage level for any test distribution
in an $f$-divergence ball around the training population. The method, based on
conformal inference, achieves (nearly) valid coverage in finite samples, under
only the condition that the training data be exchangeable. An essential
component of our methodology is to estimate the amount of expected future data
shift and build robustness to it; we develop estimators and prove their
consistency for protection and validity of uncertainty estimates under shifts.
By experimenting on several large-scale benchmark datasets, including Recht et
al.'s CIFAR-v4 and ImageNet-V2 datasets, we provide complementary empirical
results that highlight the importance of robust predictive validity.",http://arxiv.org/pdf/2008.04267v2,stat.ML
2020-07-24 21:42:34+00:00,CD-split and HPD-split: efficient conformal regions in high dimensions,"['Rafael Izbicki', 'Gilson Shimizu', 'Rafael B. Stern']","Conformal methods create prediction bands that control average coverage
assuming solely i.i.d. data. Although the literature has mostly focused on
prediction intervals, more general regions can often better represent
uncertainty. For instance, a bimodal target is better represented by the union
of two intervals. Such prediction regions are obtained by CD-split , which
combines the split method and a data-driven partition of the feature space
which scales to high dimensions. CD-split however contains many tuning
parameters, and their role is not clear. In this paper, we provide new insights
on CD-split by exploring its theoretical properties. In particular, we show
that CD-split converges asymptotically to the oracle highest predictive density
set and satisfies local and asymptotic conditional validity. We also present
simulations that show how to tune CD-split. Finally, we introduce HPD-split, a
variation of CD-split that requires less tuning, and show that it shares the
same theoretical guarantees as CD-split. In a wide variety of our simulations,
CD-split and HPD-split have better conditional coverage and yield smaller
prediction regions than other methods.",http://arxiv.org/pdf/2007.12778v3,stat.ML
2020-07-21 15:48:41+00:00,Conformance checking: A state-of-the-art literature review,"['Sebastian Dunzer', 'Matthias Stierle', 'Martin Matzner', 'Stephan Baier']","Conformance checking is a set of process mining functions that compare
process instances with a given process model. It identifies deviations between
the process instances' actual behaviour (""as-is"") and its modelled behaviour
(""to-be""). Especially in the context of analyzing compliance in organizations,
it is currently gaining momentum -- e.g. for auditors. Researchers have
proposed a variety of conformance checking techniques that are geared towards
certain process model notations or specific applications such as process model
evaluation. This article reviews a set of conformance checking techniques
described in 37 scholarly publications. It classifies the techniques along the
dimensions ""modelling language"", ""algorithm type"", ""quality metric"", and
""perspective"" using a concept matrix so that the techniques can be better
accessed by practitioners and researchers. The matrix highlights the dimensions
where extant research concentrates and where blind spots exist. For instance,
process miners use declarative process modelling languages often, but
applications in conformance checking are rare. Likewise, process mining can
investigate process roles or process metrics such as duration, but conformance
checking techniques narrow on analyzing control-flow. Future research may
construct techniques that support these neglected approaches to conformance
checking.",http://arxiv.org/pdf/2007.10903v1,cs.SE
2020-07-16 07:08:35+00:00,Conformal Rule-Based Multi-label Classification,"['Eyke Hüllermeier', 'Johannes Fürnkranz', 'Eneldo Loza Mencia']","We advocate the use of conformal prediction (CP) to enhance rule-based
multi-label classification (MLC). In particular, we highlight the mutual
benefit of CP and rule learning: Rules have the ability to provide natural
(non-)conformity scores, which are required by CP, while CP suggests a way to
calibrate the assessment of candidate rules, thereby supporting better
predictions and more elaborate decision making. We illustrate the potential
usefulness of calibrated conformity scores in a case study on lazy multi-label
rule learning.",http://arxiv.org/pdf/2007.08145v1,cs.LG
2020-07-09 03:08:55+00:00,Less is More: Rejecting Unreliable Reviews for Product Question Answering,"['Shiwei Zhang', 'Xiuzhen Zhang', 'Jey Han Lau', 'Jeffrey Chan', 'Cecile Paris']","Promptly and accurately answering questions on products is important for
e-commerce applications. Manually answering product questions (e.g. on
community question answering platforms) results in slow response and does not
scale. Recent studies show that product reviews are a good source for
real-time, automatic product question answering (PQA). In the literature, PQA
is formulated as a retrieval problem with the goal to search for the most
relevant reviews to answer a given product question. In this paper, we focus on
the issue of answerability and answer reliability for PQA using reviews. Our
investigation is based on the intuition that many questions may not be
answerable with a finite set of reviews. When a question is not answerable, a
system should return nil answers rather than providing a list of irrelevant
reviews, which can have significant negative impact on user experience.
Moreover, for answerable questions, only the most relevant reviews that answer
the question should be included in the result. We propose a conformal
prediction based framework to improve the reliability of PQA systems, where we
reject unreliable answers so that the returned results are more concise and
accurate at answering the product question, including returning nil answers for
unanswerable questions. Experiments on a widely used Amazon dataset show
encouraging results of our proposed framework. More broadly, our results
demonstrate a novel and effective application of conformal methods to a
retrieval task.",http://arxiv.org/pdf/2007.04526v1,cs.CL
2020-07-09 00:29:55+00:00,Making learning more transparent using conformalized performance prediction,['Matthew J. Holland'],"In this work, we study some novel applications of conformal inference
techniques to the problem of providing machine learning procedures with more
transparent, accurate, and practical performance guarantees. We provide a
natural extension of the traditional conformal prediction framework, done in
such a way that we can make valid and well-calibrated predictive statements
about the future performance of arbitrary learning algorithms, when passed an
as-yet unseen training set. In addition, we include some nascent empirical
examples to illustrate potential applications.",http://arxiv.org/pdf/2007.04486v1,stat.ML
2020-07-06 23:13:07+00:00,Efficient Conformal Prediction via Cascaded Inference with Expanded Admission,"['Adam Fisch', 'Tal Schuster', 'Tommi Jaakkola', 'Regina Barzilay']","In this paper, we present a novel approach for conformal prediction (CP), in
which we aim to identify a set of promising prediction candidates -- in place
of a single prediction. This set is guaranteed to contain a correct answer with
high probability, and is well-suited for many open-ended classification tasks.
In the standard CP paradigm, the predicted set can often be unusably large and
also costly to obtain. This is particularly pervasive in settings where the
correct answer is not unique, and the number of total possible answers is high.
We first expand the CP correctness criterion to allow for additional, inferred
""admissible"" answers, which can substantially reduce the size of the predicted
set while still providing valid performance guarantees. Second, we amortize
costs by conformalizing prediction cascades, in which we aggressively prune
implausible labels early on by using progressively stronger classifiers --
again, while still providing valid performance guarantees. We demonstrate the
empirical effectiveness of our approach for multiple applications in natural
language processing and computational chemistry for drug discovery.",http://arxiv.org/pdf/2007.03114v3,cs.LG
2020-07-01 05:21:16+00:00,Unifying Model Explainability and Robustness via Machine-Checkable Concepts,"['Vedant Nanda', 'Till Speicher', 'John P. Dickerson', 'Krishna P. Gummadi', 'Muhammad Bilal Zafar']","As deep neural networks (DNNs) get adopted in an ever-increasing number of
applications, explainability has emerged as a crucial desideratum for these
models. In many real-world tasks, one of the principal reasons for requiring
explainability is to in turn assess prediction robustness, where predictions
(i.e., class labels) that do not conform to their respective explanations
(e.g., presence or absence of a concept in the input) are deemed to be
unreliable. However, most, if not all, prior methods for checking
explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant
manual intervention, which hinders their large-scale deployability. In this
paper, we propose a robustness-assessment framework, at the core of which is
the idea of using machine-checkable concepts. Our framework defines a large
number of concepts that the DNN explanations could be based on and performs the
explanation-conformity check at test time to assess prediction robustness. Both
steps are executed in an automated manner without requiring any human
intervention and are easily scaled to datasets with a very large number of
classes. Experiments on real-world datasets and human surveys show that our
framework is able to enhance prediction robustness significantly: the
predictions marked to be robust by our framework have significantly higher
accuracy and are more robust to adversarial perturbations.",http://arxiv.org/pdf/2007.00251v2,cs.AI
2020-06-30 16:23:28+00:00,Conformal Prediction Intervals for Neural Networks Using Cross Validation,"['Saeed Khaki', 'Dan Nettleton']","Neural networks are among the most powerful nonlinear models used to address
supervised learning problems. Similar to most machine learning algorithms,
neural networks produce point predictions and do not provide any prediction
interval which includes an unobserved response value with a specified
probability. In this paper, we proposed the $k$-fold prediction interval method
to construct prediction intervals for neural networks based on $k$-fold cross
validation. Simulation studies and analysis of 10 real datasets are used to
compare the finite-sample properties of the prediction intervals produced by
the proposed method and the split conformal (SC) method. The results suggest
that the proposed method tends to produce narrower prediction intervals
compared to the SC method while maintaining the same coverage probability. Our
experimental results also reveal that the proposed $k$-fold prediction interval
method produces effective prediction intervals and is especially advantageous
relative to competing approaches when the number of training observations is
limited.",http://arxiv.org/pdf/2006.16941v1,stat.ML
2020-06-29 20:45:52+00:00,Probabilistic Bounds on the End-to-End Delay of Service Function Chains using Deep MDN,"['Majid Raeis', 'Ali Tizghadam', 'Alberto Leon-Garcia']","Ensuring the conformance of a service system's end-to-end delay to service
level agreement (SLA) constraints is a challenging task that requires
statistical measures beyond the average delay. In this paper, we study the
real-time prediction of the end-to-end delay distribution in systems with
composite services such as service function chains. In order to have a general
framework, we use queueing theory to model service systems, while also adopting
a statistical learning approach to avoid the limitations of queueing-theoretic
methods such as stationarity assumptions or other approximations that are often
used to make the analysis mathematically tractable. Specifically, we use deep
mixture density networks (MDN) to predict the end-to-end distribution of the
delay given the network's state. As a result, our method is sufficiently
general to be applied in different contexts and applications. Our evaluations
show a good match between the learned distributions and the simulations, which
suggest that the proposed method is a good candidate for providing
probabilistic bounds on the end-to-end delay of more complex systems where
simulations or theoretical methods are not applicable.",http://arxiv.org/pdf/2006.16368v1,cs.PF
2020-06-29 17:45:03+00:00,Object Files and Schemata: Factorizing Declarative and Procedural Knowledge in Dynamical Systems,"['Anirudh Goyal', 'Alex Lamb', 'Phanideep Gampa', 'Philippe Beaudoin', 'Sergey Levine', 'Charles Blundell', 'Yoshua Bengio', 'Michael Mozer']","Modeling a structured, dynamic environment like a video game requires keeping
track of the objects and their states declarative knowledge) as well as
predicting how objects behave (procedural knowledge). Black-box models with a
monolithic hidden state often fail to apply procedural knowledge consistently
and uniformly, i.e., they lack systematicity. For example, in a video game,
correct prediction of one enemy's trajectory does not ensure correct prediction
of another's. We address this issue via an architecture that factorizes
declarative and procedural knowledge and that imposes modularity within each
form of knowledge. The architecture consists of active modules called object
files that maintain the state of a single object and invoke passive external
knowledge sources called schemata that prescribe state updates. To use a video
game as an illustration, two enemies of the same type will share schemata but
will have separate object files to encode their distinct state (e.g., health,
position). We propose to use attention to determine which object files to
update, the selection of schemata, and the propagation of information between
object files. The resulting architecture is a drop-in replacement conforming to
the same input-output interface as normal recurrent networks (e.g., LSTM, GRU)
yet achieves substantially better generalization on environments that have
multiple object tokens of the same type, including a challenging intuitive
physics benchmark.",http://arxiv.org/pdf/2006.16225v5,cs.LG
2020-06-28 16:12:57+00:00,Valid model-free spatial prediction,"['Huiying Mao', 'Ryan Martin', 'Brian Reich']","Predicting the response at an unobserved location is a fundamental problem in
spatial statistics. Given the difficulty in modeling spatial dependence,
especially in non-stationary cases, model-based prediction intervals are at
risk of misspecification bias that can negatively affect their validity. Here
we present a new approach for model-free nonparametric spatial prediction based
on the conformal prediction machinery. Our key observation is that spatial data
can be treated as exactly or approximately exchangeable in a wide range of
settings. In particular, under an infill asymptotic regime, we prove that the
response values are, in a certain sense, locally approximately exchangeable for
a broad class of spatial processes, and we develop a local spatial conformal
prediction algorithm that yields valid prediction intervals without strong
model assumptions like stationarity. Numerical examples with both real and
simulated data confirm that the proposed conformal prediction intervals are
valid and generally more efficient than existing model-based procedures for
large datasets across a range of non-stationary and non-Gaussian settings.",http://arxiv.org/pdf/2006.15640v2,stat.ME
2020-06-24 23:13:11+00:00,AutoCP: Automated Pipelines for Accurate Prediction Intervals,"['Yao Zhang', 'William Zame', 'Mihaela van der Schaar']","Successful application of machine learning models to real-world prediction
problems, e.g. financial forecasting and personalized medicine, has proved to
be challenging, because such settings require limiting and quantifying the
uncertainty in the model predictions, i.e. providing valid and accurate
prediction intervals. Conformal Prediction is a distribution-free approach to
construct valid prediction intervals in finite samples. However, the prediction
intervals constructed by Conformal Prediction are often (because of
over-fitting, inappropriate measures of nonconformity, or other issues) overly
conservative and hence inadequate for the application(s) at hand. This paper
proposes an AutoML framework called Automatic Machine Learning for Conformal
Prediction (AutoCP). Unlike the familiar AutoML frameworks that attempt to
select the best prediction model, AutoCP constructs prediction intervals that
achieve the user-specified target coverage rate while optimizing the interval
length to be accurate and less conservative. We tested AutoCP on a variety of
datasets and found that it significantly outperforms benchmark algorithms.",http://arxiv.org/pdf/2006.14099v2,cs.LG
2020-06-19 16:04:17+00:00,Sparse Quantile Regression,"['Le-Yu Chen', 'Sokbae Lee']","We consider both $\ell _{0}$-penalized and $\ell _{0}$-constrained quantile
regression estimators. For the $\ell _{0}$-penalized estimator, we derive an
exponential inequality on the tail probability of excess quantile prediction
risk and apply it to obtain non-asymptotic upper bounds on the mean-square
parameter and regression function estimation errors. We also derive analogous
results for the $\ell _{0}$-constrained estimator. The resulting rates of
convergence are nearly minimax-optimal and the same as those for $\ell
_{1}$-penalized and non-convex penalized estimators. Further, we characterize
expected Hamming loss for the $\ell _{0}$-penalized estimator. We implement the
proposed procedure via mixed integer linear programming and also a more
scalable first-order approximation algorithm. We illustrate the finite-sample
performance of our approach in Monte Carlo experiments and its usefulness in a
real data application concerning conformal prediction of infant birth weights
(with $n\approx 10^{3}$ and up to $p>10^{3}$). In sum, our $\ell _{0}$-based
method produces a much sparser estimator than the $\ell _{1}$-penalized and
non-convex penalized approaches without compromising precision.",http://arxiv.org/pdf/2006.11201v4,stat.ME
2020-06-12 11:03:29+00:00,TorsionNet: A Reinforcement Learning Approach to Sequential Conformer Search,"['Tarun Gogineni', 'Ziping Xu', 'Exequiel Punzalan', 'Runxuan Jiang', 'Joshua Kammeraad', 'Ambuj Tewari', 'Paul Zimmerman']","Molecular geometry prediction of flexible molecules, or conformer search, is
a long-standing challenge in computational chemistry. This task is of great
importance for predicting structure-activity relationships for a wide variety
of substances ranging from biomolecules to ubiquitous materials. Substantial
computational resources are invested in Monte Carlo and Molecular Dynamics
methods to generate diverse and representative conformer sets for medium to
large molecules, which are yet intractable to chemoinformatic conformer search
methods. We present TorsionNet, an efficient sequential conformer search
technique based on reinforcement learning under the rigid rotor approximation.
The model is trained via curriculum learning, whose theoretical benefit is
explored in detail, to maximize a novel metric grounded in thermodynamics
called the Gibbs Score. Our experimental results show that TorsionNet
outperforms the highest scoring chemoinformatics method by 4x on large branched
alkanes, and by several orders of magnitude on the previously unexplored
biopolymer lignin, with applications in renewable energy.",http://arxiv.org/pdf/2006.07078v1,cs.LG
2020-06-03 21:42:04+00:00,Classification with Valid and Adaptive Coverage,"['Yaniv Romano', 'Matteo Sesia', 'Emmanuel J. Candès']","Conformal inference, cross-validation+, and the jackknife+ are hold-out
methods that can be combined with virtually any machine learning algorithm to
construct prediction sets with guaranteed marginal coverage. In this paper, we
develop specialized versions of these techniques for categorical and unordered
response labels that, in addition to providing marginal coverage, are also
fully adaptive to complex data distributions, in the sense that they perform
favorably in terms of approximate conditional coverage compared to alternative
methods. The heart of our contribution is a novel conformity score, which we
explicitly demonstrate to be powerful and intuitive for classification
problems, but whose underlying principle is potentially far more general.
Experiments on synthetic and real data demonstrate the practical value of our
theoretical guarantees, as well as the statistical advantages of the proposed
methods over the existing alternatives.",http://arxiv.org/pdf/2006.02544v1,stat.ME
2020-06-03 16:02:57+00:00,Learning Robust Decision Policies from Observational Data,"['Muhammad Osama', 'Dave Zachariah', 'Peter Stoica']","We address the problem of learning a decision policy from observational data
of past decisions in contexts with features and associated outcomes. The past
policy maybe unknown and in safety-critical applications, such as medical
decision support, it is of interest to learn robust policies that reduce the
risk of outcomes with high costs. In this paper, we develop a method for
learning policies that reduce tails of the cost distribution at a specified
level and, moreover, provide a statistically valid bound on the cost of each
decision. These properties are valid under finite samples -- even in scenarios
with uneven or no overlap between features for different decisions in the
observed data -- by building on recent results in conformal prediction. The
performance and statistical properties of the proposed method are illustrated
using both real and synthetic data.",http://arxiv.org/pdf/2006.02355v1,cs.LG
2020-06-02 09:17:48+00:00,Conformal prediction intervals for the individual treatment effect,"['Danijel Kivaranovic', 'Robin Ristl', 'Martin Posch', 'Hannes Leeb']","We propose several prediction intervals procedures for the individual
treatment effect with either finite-sample or asymptotic coverage guarantee in
a non-parametric regression setting, where non-linear regression functions,
heteroskedasticity and non-Gaussianity are allowed. The construct the
prediction intervals we use the conformal method of Vovk et al. (2005). In
extensive simulations, we compare the coverage probability and interval length
of our prediction interval procedures. We demonstrate that complex learning
algorithms, such as neural networks, can lead to narrower prediction intervals
than simple algorithms, such as linear regression, if the sample size is large
enough.",http://arxiv.org/pdf/2006.01474v1,stat.ME
2020-05-26 03:08:43+00:00,CRUDE: Calibrating Regression Uncertainty Distributions Empirically,"['Eric Zelikman', 'Christopher Healy', 'Sharon Zhou', 'Anand Avati']","Calibrated uncertainty estimates in machine learning are crucial to many
fields such as autonomous vehicles, medicine, and weather and climate
forecasting. While there is extensive literature on uncertainty calibration for
classification, the classification findings do not always translate to
regression. As a result, modern models for predicting uncertainty in regression
settings typically produce uncalibrated and overconfident estimates. To address
these gaps, we present a calibration method for regression settings that does
not assume a particular uncertainty distribution over the error: Calibrating
Regression Uncertainty Distributions Empirically (CRUDE). CRUDE makes the
weaker assumption that error distributions have a constant arbitrary shape
across the output space, shifted by predicted mean and scaled by predicted
standard deviation. We detail a theoretical connection between CRUDE and
conformal inference. Across an extensive set of regression tasks, CRUDE
demonstrates consistently sharper, better calibrated, and more accurate
uncertainty estimates than state-of-the-art techniques.",http://arxiv.org/pdf/2005.12496v6,cs.LG
2020-05-16 12:38:19+00:00,Conformal Prediction: a Unified Review of Theory and New Challenges,"['Matteo Fontana', 'Gianluca Zeni', 'Simone Vantini']","In this work we provide a review of basic ideas and novel developments about
Conformal Prediction -- an innovative distribution-free, non-parametric
forecasting method, based on minimal assumptions -- that is able to yield in a
very straightforward way predictions sets that are valid in a statistical sense
also in in the finite sample case. The in-depth discussion provided in the
paper covers the theoretical underpinnings of Conformal Prediction, and then
proceeds to list the more advanced developments and adaptations of the original
idea.",http://arxiv.org/pdf/2005.07972v2,cs.LG
2020-05-14 14:47:30+00:00,Training conformal predictors,"['Nicolo Colombo', 'Vladimir Vovk']","Efficiency criteria for conformal prediction, such as \emph{observed
fuzziness} (i.e., the sum of p-values associated with false labels), are
commonly used to \emph{evaluate} the performance of given conformal predictors.
Here, we investigate whether it is possible to exploit efficiency criteria to
\emph{learn} classifiers, both conformal predictors and point classifiers, by
using such criteria as training objective functions. The proposed idea is
implemented for the problem of binary classification of hand-written digits. By
choosing a 1-dimensional model class (with one real-valued free parameter), we
can solve the optimization problems through an (approximate) exhaustive search
over (a discrete version of) the parameter space. Our empirical results suggest
that conformal predictors trained by minimizing their observed fuzziness
perform better than conformal predictors trained in the traditional way by
minimizing the \emph{prediction error} of the corresponding point classifier.
They also have a reasonable performance in terms of their prediction error on
the test set.",http://arxiv.org/pdf/2005.07037v1,cs.LG
2020-05-13 00:39:45+00:00,"Exchangeability, Conformal Prediction, and Rank Tests",['Arun Kumar Kuchibhotla'],"Conformal prediction has been a very popular method of distribution-free
predictive inference in recent years in machine learning and statistics. Its
popularity stems from the fact that it works as a wrapper around any prediction
algorithm such as neural networks or random forests. Exchangeability is at the
core of the validity of conformal prediction. The concept of exchangeability is
also at the core of rank tests widely known in nonparametric statistics. In
this paper, we review the concept of exchangeability and discuss the
implications for conformal prediction and rank tests. We provide a low-level
introduction to these topics, and discuss the similarities between conformal
prediction and rank tests.",http://arxiv.org/pdf/2005.06095v3,stat.ME
2020-05-11 19:07:26+00:00,Process Knowledge Driven Change Point Detection for Automated Calibration of Discrete Event Simulation Models Using Machine Learning,"['Suleyman Yildirim', 'Alper Ekrem Murat', 'Murat Yildirim', 'Suzan Arslanturk']","Initial development and subsequent calibration of discrete event simulation
models for complex systems require accurate identification of dynamically
changing process characteristics. Existing data driven change point methods
(DD-CPD) assume changes are extraneous to the system, thus cannot utilize
available process knowledge. This work proposes a unified framework for
process-driven multi-variate change point detection (PD-CPD) by combining
change point detection models with machine learning and process-driven
simulation modeling. The PD-CPD, after initializing with DD-CPD's change
point(s), uses simulation models to generate system level outputs as
time-series data streams which are then used to train neural network models to
predict system characteristics and change points. The accuracy of the
predictive models measures the likelihood that the actual process data conforms
to the simulated change points in system characteristics. PD-CPD iteratively
optimizes change points by repeating simulation and predictive model building
steps until the set of change point(s) with the maximum likelihood is
identified. Using an emergency department case study, we show that PD-CPD
significantly improves change point detection accuracy over DD-CPD estimates
and is able to detect actual change points.",http://arxiv.org/pdf/2005.05385v2,cs.LG
2020-05-02 08:51:07+00:00,"Smart, Adaptive Energy Optimization for Mobile Web Interactions","['Jie Ren', 'Lu Yuan', 'Petteri Nurmi', 'Xiaoming Wang', 'Miao Ma', 'Ling Gao', 'Zhanyong Tang', 'Jie Zheng', 'Zheng Wang']","Web technology underpins many interactive mobile applications. However,
energy-efficient mobile web interactions is an outstanding challenge. Given the
increasing diversity and complexity of mobile hardware, any practical
optimization scheme must work for a wide range of users, mobile platforms and
web workloads. This paper presents CAMEL , a novel energy optimization system
for mobile web interactions. CAMEL leverages machine learning techniques to
develop a smart, adaptive scheme to judiciously trade performance for reduced
power consumption. Unlike prior work, C AMEL directly models how a given web
content affects the user expectation and uses this to guide energy
optimization. It goes further by employing transfer learning and conformal
predictions to tune a previously learned model in the end-user environment and
improve it over time. We apply CAMEL to Chromium and evaluate it on four
distinct mobile systems involving 1,000 testing webpages and 30 users. Compared
to four state-of-the-art web-event optimizers, CAMEL delivers 22% more energy
savings, but with 49% fewer violations on the quality of user experience, and
exhibits orders of magnitudes less overhead when targeting a new computing
environment.",http://arxiv.org/pdf/2005.00749v1,cs.NI
2020-04-27 20:45:12+00:00,Energy-based models for atomic-resolution protein conformations,"['Yilun Du', 'Joshua Meier', 'Jerry Ma', 'Rob Fergus', 'Alexander Rives']","We propose an energy-based model (EBM) of protein conformations that operates
at atomic scale. The model is trained solely on crystallized protein data. By
contrast, existing approaches for scoring conformations use energy functions
that incorporate knowledge of physical principles and features that are the
complex product of several decades of research and tuning. To evaluate the
model, we benchmark on the rotamer recovery task, the problem of predicting the
conformation of a side chain from its context within a protein structure, which
has been used to evaluate energy functions for protein design. The model
achieves performance close to that of the Rosetta energy function, a
state-of-the-art method widely used in protein structure prediction and design.
An investigation of the model's outputs and hidden representations finds that
it captures physicochemical properties relevant to protein energy.",http://arxiv.org/pdf/2004.13167v1,cs.LG
2020-04-21 17:45:38+00:00,Knowing what you know: valid and validated confidence sets in multiclass and multilabel prediction,"['Maxime Cauchois', 'Suyash Gupta', 'John Duchi']","We develop conformal prediction methods for constructing valid predictive
confidence sets in multiclass and multilabel problems without assumptions on
the data generating distribution. A challenge here is that typical conformal
prediction methods---which give marginal validity (coverage)
guarantees---provide uneven coverage, in that they address easy examples at the
expense of essentially ignoring difficult examples. By leveraging ideas from
quantile regression, we build methods that always guarantee correct coverage
but additionally provide (asymptotically optimal) conditional coverage for both
multiclass and multilabel prediction problems. To address the potential
challenge of exponentially large confidence sets in multilabel prediction, we
build tree-structured classifiers that efficiently account for interactions
between labels. Our methods can be bolted on top of any classification
model---neural network, random forest, boosted tree---to guarantee its
validity. We also provide an empirical evaluation, simultaneously providing new
validation methods, that suggests the more robust coverage of our confidence
sets.",http://arxiv.org/pdf/2004.10181v3,stat.ML
2020-04-10 21:37:18+00:00,Using Conformity to Probe Interaction Challenges in XR Collaboration,"['Jeremy Hartmann', 'Hemant Bhaskar Surale', 'Aakar Gupta', 'Daniel Vogel']","The concept of a conformity spectrum is introduced to describe the degree to
which virtualization adheres to real world physical characteristics surrounding
the user. This is then used to examine interaction challenges when
collaborating across different levels of virtuality and conformity.",http://arxiv.org/pdf/2004.05235v1,cs.HC
2020-04-07 08:20:34+00:00,Predicting tail events in a RIA-EVT-Copula framework,"['Wei-Zhen Li', 'Jin-Rui Zhai', 'Zhi-Qiang Jiang', 'Gang-Jin Wang', 'Wei-Xing Zhou']","Predicting the occurrence of tail events is of great importance in financial
risk management. By employing the method of peak-over-threshold (POT) to
identify the financial extremes, we perform a recurrence interval analysis
(RIA) on these extremes. We find that the waiting time between consecutive
extremes (recurrence interval) follow a $q$-exponential distribution and the
sizes of extremes above the thresholds (exceeding size) conform to a
generalized Pareto distribution. We also find that there is a significant
correlation between recurrence intervals and exceeding sizes. We thus model the
joint distribution of recurrence intervals and exceeding sizes through
connecting the two corresponding marginal distributions with the Frank and AMH
copula functions, and apply this joint distribution to estimate the hazard
probability to observe another extreme in $\Delta t$ time since the last
extreme happened $t$ time ago. Furthermore, an extreme predicting model based
on RIA-EVT-Copula is proposed by applying a decision-making algorithm on the
hazard probability. Both in-sample and out-of-sample tests reveal that this new
extreme forecasting framework has better performance in prediction comparing
with the forecasting model based on the hazard probability only estimated from
the distribution of recurrence intervals. Our results not only shed a new light
on understanding the occurring pattern of extremes in financial markets, but
also improve the accuracy to predict financial extremes for risk management.",http://arxiv.org/pdf/2004.03190v2,q-fin.RM
2020-03-27 13:17:45+00:00,Learning Implicit Surface Light Fields,"['Michael Oechsle', 'Michael Niemeyer', 'Lars Mescheder', 'Thilo Strauss', 'Andreas Geiger']","Implicit representations of 3D objects have recently achieved impressive
results on learning-based 3D reconstruction tasks. While existing works use
simple texture models to represent object appearance, photo-realistic image
synthesis requires reasoning about the complex interplay of light, geometry and
surface properties. In this work, we propose a novel implicit representation
for capturing the visual appearance of an object in terms of its surface light
field. In contrast to existing representations, our implicit model represents
surface light fields in a continuous fashion and independent of the geometry.
Moreover, we condition the surface light field with respect to the location and
color of a small light source. Compared to traditional surface light field
models, this allows us to manipulate the light source and relight the object
using environment maps. We further demonstrate the capabilities of our model to
predict the visual appearance of an unseen object from a single real RGB image
and corresponding 3D shape information. As evidenced by our experiments, our
model is able to infer rich visual appearance including shadows and specular
reflections. Finally, we show that the proposed representation can be embedded
into a variational auto-encoder for generating novel appearances that conform
to the specified illumination conditions.",http://arxiv.org/pdf/2003.12406v1,cs.CV
2020-03-21 11:15:33+00:00,Detecting Adversarial Examples in Learning-Enabled Cyber-Physical Systems using Variational Autoencoder for Regression,"['Feiyang Cai', 'Jiani Li', 'Xenofon Koutsoukos']","Learning-enabled components (LECs) are widely used in cyber-physical systems
(CPS) since they can handle the uncertainty and variability of the environment
and increase the level of autonomy. However, it has been shown that LECs such
as deep neural networks (DNN) are not robust and adversarial examples can cause
the model to make a false prediction. The paper considers the problem of
efficiently detecting adversarial examples in LECs used for regression in CPS.
The proposed approach is based on inductive conformal prediction and uses a
regression model based on variational autoencoder. The architecture allows to
take into consideration both the input and the neural network prediction for
detecting adversarial, and more generally, out-of-distribution examples. We
demonstrate the method using an advanced emergency braking system implemented
in an open source simulator for self-driving cars where a DNN is used to
estimate the distance to an obstacle. The simulation results show that the
method can effectively detect adversarial examples with a short detection
delay.",http://arxiv.org/pdf/2003.10804v1,cs.LG
2020-03-11 04:31:10+00:00,Trusted Confidence Bounds for Learning Enabled Cyber-Physical Systems,"['Dimitrios Boursinos', 'Xenofon Koutsoukos']","Cyber-physical systems (CPS) can benefit by the use of learning enabled
components (LECs) such as deep neural networks (DNNs) for perception and
decision making tasks. However, DNNs are typically non-transparent making
reasoning about their predictions very difficult, and hence their application
to safety-critical systems is very challenging. LECs could be integrated easier
into CPS if their predictions could be complemented with a confidence measure
that quantifies how much we trust their output. The paper presents an approach
for computing confidence bounds based on Inductive Conformal Prediction (ICP).
We train a Triplet Network architecture to learn representations of the input
data that can be used to estimate the similarity between test examples and
examples in the training data set. Then, these representations are used to
estimate the confidence of set predictions from a classifier that is based on
the neural network architecture used in the triplet. The approach is evaluated
using a robotic navigation benchmark and the results show that we can computed
trusted confidence bounds efficiently in real-time.",http://arxiv.org/pdf/2003.05107v2,cs.LG
2020-03-03 01:40:14+00:00,Conformance Constraint Discovery: Measuring Trust in Data-Driven Systems,"['Anna Fariha', 'Ashish Tiwari', 'Arjun Radhakrishna', 'Sumit Gulwani', 'Alexandra Meliou']","The reliability and proper function of data-driven applications hinge on the
data's continued conformance to the applications' initial design. When data
deviates from this initial profile, system behavior becomes unpredictable. Data
profiling techniques such as functional dependencies and denial constraints
encode patterns in the data that can be used to detect deviations. But
traditional methods typically focus on exact constraints and categorical
attributes, and are ill-suited for tasks such as determining whether the
prediction of a machine learning system can be trusted or for quantifying data
drift. In this paper, we introduce data invariants, a new data-profiling
primitive that models arithmetic relationships involving multiple numerical
attributes within a (noisy) dataset and which complements the existing
data-profiling techniques. We propose a quantitative semantics to measure the
degree of violation of a data invariant, and establish that strong data
invariants can be constructed from observations with low variance on the given
dataset. A concrete instance of this principle gives the surprising result that
low-variance components of a principal component analysis (PCA), which are
usually discarded, generate better invariants than the high-variance
components. We demonstrate the value of data invariants on two applications:
trusted machine learning and data drift. We empirically show that data
invariants can (1) reliably detect tuples on which the prediction of a
machine-learned model should not be trusted, and (2) quantify data drift more
accurately than the state-of-the-art methods. Additionally, we show four case
studies where an intervention-centric explanation tool uses data invariants to
explain causes for tuple non-conformance.",http://arxiv.org/pdf/2003.01289v4,cs.DB
2020-02-26 22:28:00+00:00,Joint Unsupervised Learning of Optical Flow and Egomotion with Bi-Level Optimization,"['Shihao Jiang', 'Dylan Campbell', 'Miaomiao Liu', 'Stephen Gould', 'Richard Hartley']","We address the problem of joint optical flow and camera motion estimation in
rigid scenes by incorporating geometric constraints into an unsupervised deep
learning framework. Unlike existing approaches which rely on brightness
constancy and local smoothness for optical flow estimation, we exploit the
global relationship between optical flow and camera motion using epipolar
geometry. In particular, we formulate the prediction of optical flow and camera
motion as a bi-level optimization problem, consisting of an upper-level problem
to estimate the flow that conforms to the predicted camera motion, and a
lower-level problem to estimate the camera motion given the predicted optical
flow. We use implicit differentiation to enable back-propagation through the
lower-level geometric optimization layer independent of its implementation,
allowing end-to-end training of the network. With globally-enforced geometric
constraints, we are able to improve the quality of the estimated optical flow
in challenging scenarios and obtain better camera motion estimates compared to
other unsupervised learning methods.",http://arxiv.org/pdf/2002.11826v1,cs.CV
2020-02-23 03:35:45+00:00,Assembling Semantically-Disentangled Representations for Predictive-Generative Models via Adaptation from Synthetic Domain,"['Burkay Donderici', 'Caleb New', 'Chenliang Xu']","Deep neural networks can form high-level hierarchical representations of
input data. Various researchers have demonstrated that these representations
can be used to enable a variety of useful applications. However, such
representations are typically based on the statistics within the data, and may
not conform with the semantic representation that may be necessitated by the
application. Conditional models are typically used to overcome this challenge,
but they require large annotated datasets which are difficult to come by and
costly to create. In this paper, we show that semantically-aligned
representations can be generated instead with the help of a physics based
engine. This is accomplished by creating a synthetic dataset with decoupled
attributes, learning an encoder for the synthetic dataset, and augmenting
prescribed attributes from the synthetic domain with attributes from the real
domain. It is shown that the proposed (SYNTH-VAE-GAN) method can construct a
conditional predictive-generative model of human face attributes without
relying on real data labels.",http://arxiv.org/pdf/2002.09818v1,cs.CV
2020-02-14 10:10:43+00:00,Online Process Monitoring Using Incremental State-Space Expansion: An Exact Algorithm,"['Daniel Schuster', 'Sebastiaan J. van Zelst']","The execution of (business) processes generates valuable traces of event data
in the information systems employed within companies. Recently, approaches for
monitoring the correctness of the execution of running processes have been
developed in the area of process mining, i.e., online conformance checking. The
advantages of monitoring a process' conformity during its execution are clear,
i.e., deviations are detected as soon as they occur and countermeasures can
immediately be initiated to reduce the possible negative effects caused by
process deviations. Existing work in online conformance checking only allows
for obtaining approximations of non-conformity, e.g., overestimating the actual
severity of the deviation. In this paper, we present an exact, parameter-free,
online conformance checking algorithm that computes conformance checking
results on the fly. Our algorithm exploits the fact that the conformance
checking problem can be reduced to a shortest path problem, by incrementally
expanding the search space and reusing previously computed intermediate
results. Our experiments show that our algorithm outperforms comparable
state-of-the-art approximation algorithms.",http://arxiv.org/pdf/2002.05945v3,cs.DB
2020-02-12 16:29:24+00:00,Structure-Property Maps with Kernel Principal Covariates Regression,"['Benjamin A. Helfrecht', 'Rose K. Cersonsky', 'Guillaume Fraux', 'Michele Ceriotti']","Data analyses based on linear methods constitute the simplest, most robust,
and transparent approaches to the automatic processing of large amounts of data
for building supervised or unsupervised machine learning models. Principal
covariates regression (PCovR) is an underappreciated method that interpolates
between principal component analysis and linear regression, and can be used to
conveniently reveal structure-property relations in terms of
simple-to-interpret, low-dimensional maps. Here we provide a pedagogic overview
of these data analysis schemes, including the use of the kernel trick to
introduce an element of non-linearity, while maintaining most of the
convenience and the simplicity of linear approaches. We then introduce a
kernelized version of PCovR and a sparsified extension, and demonstrate the
performance of this approach in revealing and predicting structure-property
relations in chemistry and materials science, showing a variety of examples
including elemental carbon, porous silicate frameworks, organic molecules,
amino acid conformers, and molecular materials.",http://arxiv.org/pdf/2002.05076v2,stat.ML
2020-01-28 17:51:07+00:00,Real-time Out-of-distribution Detection in Learning-Enabled Cyber-Physical Systems,"['Feiyang Cai', 'Xenofon Koutsoukos']","Cyber-physical systems (CPS) greatly benefit by using machine learning
components that can handle the uncertainty and variability of the real-world.
Typical components such as deep neural networks, however, introduce new types
of hazards that may impact system safety. The system behavior depends on data
that are available only during runtime and may be different than the data used
for training. Out-of-distribution data may lead to a large error and compromise
safety. The paper considers the problem of efficiently detecting
out-of-distribution data in CPS control systems. Detection must be robust and
limit the number of false alarms while being computational efficient for
real-time monitoring. The proposed approach leverages inductive conformal
prediction and anomaly detection for developing a method that has a
well-calibrated false alarm rate. We use variational autoencoders and deep
support vector data description to learn models that can be used efficiently
compute the nonconformity of new inputs relative to the training set and enable
real-time detection of out-of-distribution high-dimensional inputs. We
demonstrate the method using an advanced emergency braking system and a
self-driving end-to-end controller implemented in an open source simulator for
self-driving cars. The simulation results show very small number of false
positives and detection delay while the execution time is comparable to the
execution time of the original machine learning components.",http://arxiv.org/pdf/2001.10494v1,cs.LG
2020-01-21 21:03:35+00:00,Missed opportunities in large scale comparison of QSAR and conformal prediction methods and their applications in drug discovery,['Damjan Krstajic'],"Recently Bosc et al. (J Cheminform 11(1): 4, 2019), published an article
describing a case study that directly compares conformal predictions with
traditional QSAR methods for large-scale predictions of target-ligand binding.
We consider this study to be very important. Unfortunately, we have found
several issues in the authors' approach as well as in the presentation of their
findings.",http://arxiv.org/pdf/2001.07773v1,stat.AP
2020-01-16 18:41:17+00:00,Cross-conformal e-prediction,['Vladimir Vovk'],"This note discusses a simple modification of cross-conformal prediction
inspired by recent work on e-values. The precursor of conformal prediction
developed in the 1990s by Gammerman, Vapnik, and Vovk was also based on
e-values and is called conformal e-prediction in this note. Replacing e-values
by p-values led to conformal prediction, which has important advantages over
conformal e-prediction without obvious disadvantages. The situation with
cross-conformal prediction is, however, different: whereas for cross-conformal
prediction validity is only an empirical fact (and can be broken with excessive
randomization), this note draws the reader's attention to the obvious fact that
cross-conformal e-prediction enjoys a guaranteed property of validity.",http://arxiv.org/pdf/2001.05989v1,cs.LG
2020-01-14 19:34:51+00:00,Assurance Monitoring of Cyber-Physical Systems with Machine Learning Components,"['Dimitrios Boursinos', 'Xenofon Koutsoukos']","Machine learning components such as deep neural networks are used extensively
in Cyber-Physical Systems (CPS). However, they may introduce new types of
hazards that can have disastrous consequences and need to be addressed for
engineering trustworthy systems. Although deep neural networks offer advanced
capabilities, they must be complemented by engineering methods and practices
that allow effective integration in CPS. In this paper, we investigate how to
use the conformal prediction framework for assurance monitoring of CPS with
machine learning components. In order to handle high-dimensional inputs in
real-time, we compute nonconformity scores using embedding representations of
the learned models. By leveraging conformal prediction, the approach provides
well-calibrated confidence and can allow monitoring that ensures a bounded
small error rate while limiting the number of inputs for which an accurate
prediction cannot be made. Empirical evaluation results using the German
Traffic Sign Recognition Benchmark and a robot navigation dataset demonstrate
that the error rates are well-calibrated while the number of alarms is small.
The method is computationally efficient, and therefore, the approach is
promising for assurance monitoring of CPS.",http://arxiv.org/pdf/2001.05014v2,cs.LG
2020-01-08 08:29:26+00:00,ILS-MPM: an implicit level-set-based material point method for frictional particulate contact mechanics of deformable particles,"['Chuanqi Liu', 'Waiching Sun']","Finite element simulations of frictional multi-body contact problems via
conformal meshes can be challenging and computationally demanding. To render
geometrical features, unstructured meshes must be used and this unavoidably
increases the degrees of freedom and therefore makes the construction of
slave/master pairs more demanding. In this work, we introduce an implicit
material point method designed to bypass the meshing of bodies by employing
level set functions to represent boundaries at structured grids. This implicit
function representation provides an elegant mean to link an unbiased
intermediate reference surface with the true boundaries by closest point
projection as shown in leichner et al. (2019). We then enforce the contact
constraints by a penalty method where the Coulomb friction law is implemented
as an elastoplastic constitutive model such that a return mapping algorithm can
be used to provide constitutive updates for both the stick and slip states. To
evolve the geometry of the contacts properly, the Hamilton-Jacobi equation is
solved incrementally such that the level set and material points are both
updated accord to the deformation field. To improve the accuracy and regularity
of the numerical integration of the material point method, a moving least
square method is used to project numerical values of the material points back
to the standard locations for Gaussian-Legendre quadrature. Several benchmarks
are used to verify the proposed model. Comparisons with discrete element
simulations are made to analyze the importance of stress fields on predicting
the macroscopic responses of granular assemblies.",http://arxiv.org/pdf/2001.02412v3,cs.CE
2020-01-03 06:12:26+00:00,Trajectory Forecasts in Unknown Environments Conditioned on Grid-Based Plans,"['Nachiket Deo', 'Mohan M. Trivedi']","We address the problem of forecasting pedestrian and vehicle trajectories in
unknown environments, conditioned on their past motion and scene structure.
Trajectory forecasting is a challenging problem due to the large variation in
scene structure and the multimodal distribution of future trajectories. Unlike
prior approaches that directly learn one-to-many mappings from observed context
to multiple future trajectories, we propose to condition trajectory forecasts
on plans sampled from a grid based policy learned using maximum entropy inverse
reinforcement learning (MaxEnt IRL). We reformulate MaxEnt IRL to allow the
policy to jointly infer plausible agent goals, and paths to those goals on a
coarse 2-D grid defined over the scene. We propose an attention based
trajectory generator that generates continuous valued future trajectories
conditioned on state sequences sampled from the MaxEnt policy. Quantitative and
qualitative evaluation on the publicly available Stanford drone and NuScenes
datasets shows that our model generates trajectories that are diverse,
representing the multimodal predictive distribution, and precise, conforming to
the underlying scene structure over long prediction horizons.",http://arxiv.org/pdf/2001.00735v2,cs.CV
2019-12-18 03:50:13+00:00,Real-Time Prediction of Delay Distribution in Service Systems using Mixture Density Networks,"['Majid Raeis', 'Ali Tizghadam', 'Alberto Leon-Garcia']","Motivated by interest in providing more efficient services in customer
service systems, we use statistical learning methods and delay history
information to predict the conditional distribution of the customers' waiting
times in queueing systems. From the predicted distributions, descriptive
statistics of the system such as the mean, variance and percentiles of the
waiting times can be obtained, which can be used for delay announcements, SLA
conformance and better system management. We model the conditional
distributions by mixtures of Gaussians, parameters of which can be estimated
using Mixture Density Networks. The evaluations show that exploiting more delay
history information can result in much more accurate predictions under
realistic time-varying arrival assumptions.",http://arxiv.org/pdf/1912.08368v1,cs.PF
2019-12-02 00:31:52+00:00,Conformance Checking Approximation using Subset Selection and Edit Distance,"['Mohammadreza Fani Sani', 'Sebastiaan J. van Zelst', 'Wil M. P. van der Aalst']","Conformance checking techniques let us find out to what degree a process
model and real execution data correspond to each other. In recent years,
alignments have proven extremely useful in calculating conformance statistics.
Most techniques to compute alignments provide an exact solution. However, in
many applications, it is enough to have an approximation of the conformance
value. Specifically, for large event data, the computing time for alignments is
considerably long using current techniques which makes them inapplicable in
reality. Also, it is no longer feasible to use standard hardware for complex
processes. Hence, we need techniques that enable us to obtain fast, and at the
same time, accurate approximation of the conformance values. This paper
proposes new approximation techniques to compute approximated conformance
checking values close to exact solution values in a faster time. Those methods
also provide upper and lower bounds for the approximated alignment value. Our
experiments on real event data show that it is possible to improve the
performance of conformance checking by using the proposed methods compared to
using the state-of-the-art alignment approximation technique. Results show that
in most of the cases, we provide tight bounds, accurate approximated alignment
values, and similar deviation statistics.",http://arxiv.org/pdf/1912.05022v1,cs.AI
2019-11-29 17:34:52+00:00,DeepAlign: Alignment-based Process Anomaly Correction using Recurrent Neural Networks,"['Timo Nolle', 'Alexander Seeliger', 'Nils Thoma', 'Max Mühlhäuser']","In this paper, we propose DeepAlign, a novel approach to multi-perspective
process anomaly correction, based on recurrent neural networks and
bidirectional beam search. At the core of the DeepAlign algorithm are two
recurrent neural networks trained to predict the next event. One is reading
sequences of process executions from left to right, while the other is reading
the sequences from right to left. By combining the predictive capabilities of
both neural networks, we show that it is possible to calculate sequence
alignments, which are used to detect and correct anomalies. DeepAlign utilizes
the case-level and event-level attributes to closely model the decisions within
a process. We evaluate the performance of our approach on an elaborate data
corpus of 252 realistic synthetic event logs and compare it to three
state-of-the-art conformance checking methods. DeepAlign produces better
corrections than the rest of the field reaching an overall $F_1$ score of
$0.9572$ across all datasets, whereas the best comparable state-of-the-art
method reaches $0.6411$.",http://arxiv.org/pdf/1911.13229v2,cs.AI
2019-11-03 18:18:09+00:00,Computationally efficient versions of conformal predictive distributions,"['Vladimir Vovk', 'Ivan Petej', 'Ilia Nouretdinov', 'Valery Manokhin', 'Alex Gammerman']","Conformal predictive systems are a recent modification of conformal
predictors that output, in regression problems, probability distributions for
labels of test observations rather than set predictions. The extra information
provided by conformal predictive systems may be useful, e.g., in decision
making problems. Conformal predictive systems inherit the relative
computational inefficiency of conformal predictors. In this paper we discuss
two computationally efficient versions of conformal predictive systems, which
we call split conformal predictive systems and cross-conformal predictive
systems. The main advantage of split conformal predictive systems is their
guaranteed validity, whereas for cross-conformal predictive systems validity
only holds empirically and in the absence of excessive randomization. The main
advantage of cross-conformal predictive systems is their greater predictive
efficiency.",http://arxiv.org/pdf/1911.00941v1,cs.LG
2019-10-23 13:44:38+00:00,Nested conformal prediction and quantile out-of-bag ensemble methods,"['Chirag Gupta', 'Arun K. Kuchibhotla', 'Aaditya K. Ramdas']","Conformal prediction is a popular tool for providing valid prediction sets
for classification and regression problems, without relying on any
distributional assumptions on the data. While the traditional description of
conformal prediction starts with a nonconformity score, we provide an alternate
(but equivalent) view that starts with a sequence of nested sets and calibrates
them to find a valid prediction set. The nested framework subsumes all
nonconformity scores, including recent proposals based on quantile regression
and density estimation. While these ideas were originally derived based on
sample splitting, our framework seamlessly extends them to other aggregation
schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the
framework to derive a new algorithm (QOOB, pronounced cube) that combines four
ideas: quantile regression, cross-conformalization, ensemble methods and
out-of-bag predictions. We develop a computationally efficient implementation
of cross-conformal, that is also used by QOOB. In a detailed numerical
investigation, QOOB performs either the best or close to the best on all
simulated and real datasets. Code for QOOB is available at
https://github.com/aigen/QOOB.",http://arxiv.org/pdf/1910.10562v4,stat.ME
2019-10-17 18:28:56+00:00,Multi-level conformal clustering: A distribution-free technique for clustering and anomaly detection,"['Ilia Nouretdinov', 'James Gammerman', 'Matteo Fontana', 'Daljit Rehal']","In this work we present a clustering technique called \textit{multi-level
conformal clustering (MLCC)}. The technique is hierarchical in nature because
it can be performed at multiple significance levels which yields greater
insight into the data than performing it at just one level. We describe the
theoretical underpinnings of MLCC, compare and contrast it with the
hierarchical clustering algorithm, and then apply it to real world datasets to
assess its performance. There are several advantages to using MLCC over more
classical clustering techniques: Once a significance level has been set, MLCC
is able to automatically select the number of clusters. Furthermore, thanks to
the conformal prediction framework the resulting clustering model has a clear
statistical meaning without any assumptions about the distribution of the data.
This statistical robustness also allows us to perform clustering and anomaly
detection simultaneously. Moreover, due to the flexibility of the conformal
prediction framework, our algorithm can be used on top of many other machine
learning algorithms.",http://arxiv.org/pdf/1910.08105v2,stat.ML
2019-10-12 15:23:13+00:00,Flexible distribution-free conditional predictive bands using density estimators,"['Rafael Izbicki', 'Gilson T. Shimizu', 'Rafael B. Stern']","Conformal methods create prediction bands that control average coverage under
no assumptions besides i.i.d. data. Besides average coverage, one might also
desire to control conditional coverage, that is, coverage for every new testing
point. However, without strong assumptions, conditional coverage is
unachievable. Given this limitation, the literature has focused on methods with
asymptotical conditional coverage. In order to obtain this property, these
methods require strong conditions on the dependence between the target variable
and the features. We introduce two conformal methods based on conditional
density estimators that do not depend on this type of assumption to obtain
asymptotic conditional coverage: Dist-split and CD-split. While Dist-split
asymptotically obtains optimal intervals, which are easier to interpret than
general regions, CD-split obtains optimal size regions, which are smaller than
intervals. CD-split also obtains local coverage by creating a data-driven
partition of the feature space that scales to high-dimensional settings and by
generating prediction bands locally on the partition elements. In a wide
variety of simulated scenarios, our methods have a better control of
conditional coverage and have smaller length than previously proposed methods.",http://arxiv.org/pdf/1910.05575v2,stat.ME
2019-09-28 03:03:09+00:00,D2D-LSTM based Prediction of the D2D Diffusion Path in Mobile Social Networks,['Hao Xu'],"Recently, how to expand data transmission to reduce cell data and repeated
cell transmission has received more and more research attention. In mobile
social networks, content popularity prediction has always been an important
part of traffic offloading and expanding data dissemination. However, current
mainstream content popularity prediction methods only use the number of
downloads and shares or the distribution of user interests, which do not
consider important time and geographic location information in mobile social
networks, and all of data is from OSN which is not same as MSN. In this work,
we propose D2D Long Short-Term Memory (D2D-LSTM), a deep neural network based
on LSTM, which is designed to predict a complete D2D diffusion path. Our work
is the first attempt in the world to use real data of MSN to predict diffusion
path with deep neural networks which conforms to the D2D structure. Compared to
linear sequence networks, only learn users' social features without time
distribution or GPS distribution and files' content features, our model can
predict the propagation path more accurately (up to 85.858\%) and can reach
convergence faster (less than 100 steps) because of the neural network that
conforms to the D2D structure and combines user social features and files
features. Moreover, we can simulate generating a D2D propagation tree. After
experiment and comparison, it is found to be very similar to the ground-truth
trees. Finally, we define a user prototype refinement that can more accurately
describe the propagation sharing habits of a prototype user (including content
preferences, time preferences, and geographic location preferences), and
experimentally validate the predictions when the user prototype is added to
1000 classes, it is almost identical to the 50 categories.",http://arxiv.org/pdf/1910.01453v1,cs.SI
2019-09-20 08:15:37+00:00,Computing Full Conformal Prediction Set with Approximate Homotopy,"['Eugene Ndiaye', 'Ichiro Takeuchi']","If you are predicting the label $y$ of a new object with $\hat y$, how
confident are you that $y = \hat y$? Conformal prediction methods provide an
elegant framework for answering such question by building a $100 (1 -
\alpha)\%$ confidence region without assumptions on the distribution of the
data. It is based on a refitting procedure that parses all the possibilities
for $y$ to select the most likely ones. Although providing strong coverage
guarantees, conformal set is impractical to compute exactly for many regression
problems. We propose efficient algorithms to compute conformal prediction set
using approximated solution of (convex) regularized empirical risk
minimization. Our approaches rely on a new homotopy continuation technique for
tracking the solution path with respect to sequential changes of the
observations. We also provide a detailed analysis quantifying its complexity.",http://arxiv.org/pdf/1909.09365v2,stat.ML
2019-09-17 15:30:21+00:00,Distributional conformal prediction,"['Victor Chernozhukov', 'Kaspar Wüthrich', 'Yinchu Zhu']","We propose a robust method for constructing conditionally valid prediction
intervals based on models for conditional distributions such as quantile and
distribution regression. Our approach can be applied to important prediction
problems including cross-sectional prediction, k-step-ahead forecasts,
synthetic controls and counterfactual prediction, and individual treatment
effects prediction. Our method exploits the probability integral transform and
relies on permuting estimated ranks. Unlike regression residuals, ranks are
independent of the predictors, allowing us to construct conditionally valid
prediction intervals under heteroskedasticity. We establish approximate
conditional validity under consistent estimation and provide approximate
unconditional validity under model misspecification, overfitting, and with time
series data. We also propose a simple ""shape"" adjustment of our baseline method
that yields optimal prediction intervals.",http://arxiv.org/pdf/1909.07889v3,econ.EM
2019-09-17 05:09:01+00:00,Conformal Prediction based Spectral Clustering,"['Lalith Srikanth Chintalapati', 'Raghunatha Sarma Rachakonda']","Spectral Clustering(SC) is a prominent data clustering technique of recent
times which has attracted much attention from researchers. It is a highly
data-driven method and makes no strict assumptions on the structure of the data
to be clustered. One of the central pieces of spectral clustering is the
construction of an affinity matrix based on a similarity measure between data
points. The way the similarity measure is defined between data points has a
direct impact on the performance of the SC technique. Several attempts have
been made in the direction of strengthening the pairwise similarity measure to
enhance the spectral clustering. In this work, we have defined a novel affinity
measure by employing the concept of non-conformity used in Conformal
Prediction(CP) framework. The non-conformity based affinity captures the
relationship between neighborhoods of data points and has the power to
generalize the notion of contextual similarity. We have shown that this
formulation of affinity measure gives good results and compares well with the
state of the art methods.",http://arxiv.org/pdf/1909.07594v1,cs.LG
2019-09-12 17:00:45+00:00,Self-Assembling Modular Networks for Interpretable Multi-Hop Reasoning,"['Yichen Jiang', 'Mohit Bansal']","Multi-hop QA requires a model to connect multiple pieces of evidence
scattered in a long context to answer the question. The recently proposed
HotpotQA (Yang et al., 2018) dataset is comprised of questions embodying four
different multi-hop reasoning paradigms (two bridge entity setups, checking
multiple properties, and comparing two entities), making it challenging for a
single neural network to handle all four. In this work, we present an
interpretable, controller-based Self-Assembling Neural Modular Network (Hu et
al., 2017, 2018) for multi-hop reasoning, where we design four novel modules
(Find, Relocate, Compare, NoOp) to perform unique types of language reasoning.
Based on a question, our layout controller RNN dynamically infers a series of
reasoning modules to construct the entire network. Empirically, we show that
our dynamic, multi-hop modular network achieves significant improvements over
the static, single-hop baseline (on both regular and adversarial evaluation).
We further demonstrate the interpretability of our model via three analyses.
First, the controller can softly decompose the multi-hop question into multiple
single-hop sub-questions to promote compositional reasoning behavior of the
main network. Second, the controller can predict layouts that conform to the
layouts designed by human experts. Finally, the intermediate module can infer
the entity that connects two distantly-located supporting facts by addressing
the sub-question from the controller.",http://arxiv.org/pdf/1909.05803v2,cs.CL
2019-09-12 01:48:11+00:00,A comparison of some conformal quantile regression methods,"['Matteo Sesia', 'Emmanuel J. Candès']","We compare two recently proposed methods that combine ideas from conformal
inference and quantile regression to produce locally adaptive and marginally
valid prediction intervals under sample exchangeability (Romano et al., 2019;
Kivaranovic et al., 2019). First, we prove that these two approaches are
asymptotically efficient in large samples, under some additional assumptions.
Then we compare them empirically on simulated and real data. Our results
demonstrate that the method in Romano et al. (2019) typically yields tighter
prediction intervals in finite samples. Finally, we discuss how to tune these
procedures by fixing the relative proportions of observations used for training
and conformalization.",http://arxiv.org/pdf/1909.05433v1,stat.ME
2019-09-11 07:14:58+00:00,Dynamic Fusion: Attentional Language Model for Neural Machine Translation,"['Michiki Kurosawa', 'Mamoru Komachi']","Neural Machine Translation (NMT) can be used to generate fluent output. As
such, language models have been investigated for incorporation with NMT. In
prior investigations, two models have been used: a translation model and a
language model. The translation model's predictions are weighted by the
language model with a hand-crafted ratio in advance. However, these approaches
fail to adopt the language model weighting with regard to the translation
history. In another line of approach, language model prediction is incorporated
into the translation model by jointly considering source and target
information. However, this line of approach is limited because it largely
ignores the adequacy of the translation output.
  Accordingly, this work employs two mechanisms, the translation model and the
language model, with an attentive architecture to the language model as an
auxiliary element of the translation model. Compared with previous work in
English--Japanese machine translation using a language model, the experimental
results obtained with the proposed Dynamic Fusion mechanism improve BLEU and
Rank-based Intuitive Bilingual Evaluation Scores (RIBES) scores. Additionally,
in the analyses of the attention and predictivity of the language model, the
Dynamic Fusion mechanism allows predictive language modeling that conforms to
the appropriate grammatical structure.",http://arxiv.org/pdf/1909.04879v1,cs.CL
2019-09-09 16:15:03+00:00,Tree-based Synthetic Control Methods: Consequences of moving the US Embassy,"['Nicolaj Søndergaard Mühlbach', 'Mikkel Slot Nielsen']","We recast the synthetic controls for evaluating policies as a counterfactual
prediction problem and replace its linear regression with a nonparametric model
inspired by machine learning. The proposed method enables us to achieve
accurate counterfactual predictions and we provide theoretical guarantees. We
apply our method to a highly debated policy: the relocation of the US embassy
to Jerusalem. In Israel and Palestine, we find that the average number of
weekly conflicts has increased by roughly 103\% over 48 weeks since the
relocation was announced on December 6, 2017. By using conformal inference and
placebo tests, we justify our model and find the increase to be statistically
significant.",http://arxiv.org/pdf/1909.03968v3,econ.EM
2019-09-05 00:34:33+00:00,Future Frame Prediction Using Convolutional VRNN for Anomaly Detection,"['Yiwei Lu', 'Mahesh Kumar Krishna Reddy', 'Seyed shahabeddin Nabavi', 'Yang Wang']","Anomaly detection in videos aims at reporting anything that does not conform
the normal behaviour or distribution. However, due to the sparsity of abnormal
video clips in real life, collecting annotated data for supervised learning is
exceptionally cumbersome. Inspired by the practicability of generative models
for semi-supervised learning, we propose a novel sequential generative model
based on variational autoencoder (VAE) for future frame prediction with
convolutional LSTM (ConvLSTM). To the best of our knowledge, this is the first
work that considers temporal information in future frame prediction based
anomaly detection framework from the model perspective. Our experiments
demonstrate that our approach is superior to the state-of-the-art methods on
three benchmark datasets.",http://arxiv.org/pdf/1909.02168v2,cs.CV
2019-08-30 19:04:18+00:00,Evaluating Conformance Measures in Process Mining using Conformance Propositions (Extended version),"['Anja F. Syring', 'Niek Tax', 'Wil M. P. van der Aalst']","Process mining sheds new light on the relationship between process models and
real-life processes. Process discovery can be used to learn process models from
event logs. Conformance checking is concerned with quantifying the quality of a
business process model in relation to event data that was logged during the
execution of the business process. There exist different categories of
conformance measures. Recall, also called fitness, is concerned with
quantifying how much of the behavior that was observed in the event log fits
the process model. Precision is concerned with quantifying how much behavior a
process model allows for that was never observed in the event log.
Generalization is concerned with quantifying how well a process model
generalizes to behavior that is possible in the business process but was never
observed in the event log. Many recall, precision, and generalization measures
have been developed throughout the years, but they are often defined in an
ad-hoc manner without formally defining the desired properties up front. To
address these problems, we formulate 21 conformance propositions and we use
these propositions to evaluate current and existing conformance measures. The
goal is to trigger a discussion by clearly formulating the challenges and
requirements (rather than proposing new measures). Additionally, this paper
serves as an overview of the conformance checking measures that are available
in the process mining area.",http://arxiv.org/pdf/1909.02393v1,cs.AI
2019-08-15 15:01:55+00:00,Combining Prediction Intervals on Multi-Source Non-Disclosed Regression Datasets,"['Ola Spjuth', 'Robin Carrión Brännström', 'Lars Carlsson', 'Niharika Gauraha']","Conformal Prediction is a framework that produces prediction intervals based
on the output from a machine learning algorithm. In this paper we explore the
case when training data is made up of multiple parts available in different
sources that cannot be pooled. We here consider the regression case and propose
a method where a conformal predictor is trained on each data source
independently, and where the prediction intervals are then combined into a
single interval. We call the approach Non-Disclosed Conformal Prediction
(NDCP), and we evaluate it on a regression dataset from the UCI machine
learning repository using support vector regression as the underlying machine
learning algorithm, with varying number of data sources and sizes. The results
show that the proposed method produces conservatively valid prediction
intervals, and while we cannot retain the same efficiency as when all data is
used, efficiency is improved through the proposed approach as compared to
predicting using a single arbitrarily chosen source.",http://arxiv.org/pdf/1908.05571v1,stat.ML
2019-08-13 12:57:26+00:00,Learning Credible Deep Neural Networks with Rationale Regularization,"['Mengnan Du', 'Ninghao Liu', 'Fan Yang', 'Xia Hu']","Recent explainability related studies have shown that state-of-the-art DNNs
do not always adopt correct evidences to make decisions. It not only hampers
their generalization but also makes them less likely to be trusted by
end-users. In pursuit of developing more credible DNNs, in this paper we
propose CREX, which encourages DNN models to focus more on evidences that
actually matter for the task at hand, and to avoid overfitting to
data-dependent bias and artifacts. Specifically, CREX regularizes the training
process of DNNs with rationales, i.e., a subset of features highlighted by
domain experts as justifications for predictions, to enforce DNNs to generate
local explanations that conform with expert rationales. Even when rationales
are not available, CREX still could be useful by requiring the generated
explanations to be sparse. Experimental results on two text classification
datasets demonstrate the increased credibility of DNNs trained with CREX.
Comprehensive analysis further shows that while CREX does not always improve
prediction accuracy on the held-out test set, it significantly increases DNN
accuracy on new and previously unseen data beyond test set, highlighting the
advantage of the increased credibility.",http://arxiv.org/pdf/1908.05601v1,cs.LG
2019-07-30 16:55:29+00:00,Predicting assisted ventilation in Amyotrophic Lateral Sclerosis using a mixture of experts and conformal predictors,"['Telma Pereira', 'Sofia Pires', 'Marta Gromicho', 'Susana Pinto', 'Mamede de Carvalho', 'Sara C. Madeira']","Amyotrophic Lateral Sclerosis (ALS) is a neurodegenerative disease
characterized by a rapid motor decline, leading to respiratory failure and
subsequently to death. In this context, researchers have sought for models to
automatically predict disease progression to assisted ventilation in ALS
patients. However, the clinical translation of such models is limited by the
lack of insight 1) on the risk of error for predictions at patient-level, and
2) on the most adequate time to administer the non-invasive ventilation. To
address these issues, we combine Conformal Prediction (a machine learning
framework that complements predictions with confidence measures) and a mixture
experts into a prognostic model which not only predicts whether an ALS patient
will suffer from respiratory insufficiency but also the most likely time window
of occurrence, at a given reliability level. Promising results were obtained,
with near 80% of predictions being correctly identified.",http://arxiv.org/pdf/1907.13070v1,cs.LG
2019-07-03 16:24:10+00:00,libconform v0.1.0: a Python library for conformal prediction,['Jonas Fassbender'],"This paper introduces libconform v0.1.0, a Python library for the conformal
prediction framework, licensed under the MIT-license. libconform is not yet
stable. This paper describes the main algorithms implemented and documents the
API of libconform. Also some details about the implementation and changes in
future versions are described.",http://arxiv.org/pdf/1907.02015v1,cs.LG
2019-05-25 17:07:33+00:00,"Adaptive, Distribution-Free Prediction Intervals for Deep Networks","['Danijel Kivaranovic', 'Kory D. Johnson', 'Hannes Leeb']","The machine learning literature contains several constructions for prediction
intervals that are intuitively reasonable but ultimately ad-hoc in that they do
not come with provable performance guarantees. We present methods from the
statistics literature that can be used efficiently with neural networks under
minimal assumptions with guaranteed performance. We propose a neural network
that outputs three values instead of a single point estimate and optimizes a
loss function motivated by the standard quantile regression loss. We provide
two prediction interval methods with finite sample coverage guarantees solely
under the assumption that the observations are independent and identically
distributed. The first method leverages the conformal inference framework and
provides average coverage. The second method provides a new, stronger guarantee
by conditioning on the observed data. Lastly, our loss function does not
compromise the predictive accuracy of the network like other prediction
interval methods. We demonstrate the ease of use of our procedures as well as
its improvements over other methods on both simulated and real data. As most
deep networks can easily be modified by our method to output predictions with
valid prediction intervals, its use should become standard practice, much like
reporting standard errors along with mean estimates.",http://arxiv.org/pdf/1905.10634v2,stat.ML
2019-05-22 01:13:55+00:00,Automatically Checking Conformance on Asynchronous Reactive Systems,"['Camila Sonada Gomes', 'Adilson Luiz Bonifacio']","Software testing is an important issue in software development process to
ensure higher quality on the products. Formal methods has been promising on
testing reactive systems, specially critical systems, where accuracy is
mandatory since any fault can cause severe damage. Systems of this nature are
characterized by receiving messages from the environment and producing outputs
in response. One of the most challenges in model-based testing is the
conformance checking of asynchronous reactive systems. The aim is to verify if
an implementation is in compliance with its respective specification. In this
work, we develop a practical tool to check conformance relation between
reactive models using a more general theory based on regular languages. The
approach, in fact, subsumes the classical \ioco conformance which is also
available in our tool. In addition, we present some studies with different
sceneries that are applied to practical tools with both notions of conformance.",http://arxiv.org/pdf/1905.08914v2,cs.SE
2019-05-20 05:46:03+00:00,Conformal Prediction Interval Estimations with an Application to Day-Ahead and Intraday Power Markets,"['Christopher Kath', 'Florian Ziel']","We discuss a concept denoted as Conformal Prediction (CP) in this paper.
While initially stemming from the world of machine learning, it was never
applied or analyzed in the context of short-term electricity price forecasting.
Therefore, we elaborate the aspects that render Conformal Prediction worthwhile
to know and explain why its simple yet very efficient idea has worked in other
fields of application and why its characteristics are promising for short-term
power applications as well. We compare its performance with different
state-of-the-art electricity price forecasting models such as quantile
regression averaging (QRA) in an empirical out-of-sample study for three
short-term electricity time series. We combine Conformal Prediction with
various underlying point forecast models to demonstrate its versatility and
behavior under changing conditions. Our findings suggest that Conformal
Prediction yields sharp and reliable prediction intervals in short-term power
markets. We further inspect the effect each of Conformal Prediction's model
components has and provide a path-based guideline on how to find the best CP
model for each market.",http://arxiv.org/pdf/1905.07886v2,econ.EM
2019-05-10 22:56:39+00:00,Prediction and outlier detection in classification problems,"['Leying Guan', 'Rob Tibshirani']","We consider the multi-class classification problem when the training data and
the out-of-sample test data may have different distributions and propose a
method called BCOPS (balanced and conformal optimized prediction sets). BCOPS
constructs a prediction set $C(x)$ as a subset of class labels, possibly empty.
It tries to optimize the out-of-sample performance, aiming to include the
correct class as often as possible, but also detecting outliers $x$, for which
the method returns no prediction (corresponding to $C(x)$ equal to the empty
set). The proposed method combines supervised-learning algorithms with the
method of conformal prediction to minimize a misclassification loss averaged
over the out-of-sample distribution. The constructed prediction sets have a
finite-sample coverage guarantee without distributional assumptions.
  We also propose a method to estimate the outlier detection rate of a given
method. We prove asymptotic consistency and optimality of our proposals under
suitable assumptions and illustrate our methods on real data examples.",http://arxiv.org/pdf/1905.04396v3,stat.ME
2019-05-09 14:31:29+00:00,Efficient and minimal length parametric conformal prediction regions,"['Daniel J. Eck', 'Forrest W. Crawford']","Conformal prediction methods construct prediction regions for iid data that
are valid in finite samples. We provide two parametric conformal prediction
regions that are applicable for a wide class of continuous statistical models.
This class of statistical models includes generalized linear models (GLMs) with
continuous outcomes. Our parametric conformal prediction regions possesses
finite sample validity, even when the model is misspecified, and are
asymptotically of minimal length when the model is correctly specified. The
first parametric conformal prediction region is constructed through binning of
the predictor space, guarantees finite-sample local validity and is
asymptotically minimal at the $\sqrt{\log(n)/n}$ rate when the dimension $d$ of
the predictor space is one or two, and converges at the
$O\{(\log(n)/n)^{1/d}\}$ rate when $d > 2$. The second parametric conformal
prediction region is constructed by transforming the outcome variable to a
common distribution via the probability integral transform, guarantees
finite-sample marginal validity, and is asymptotically minimal at the
$\sqrt{\log(n)/n}$ rate. We develop a novel concentration inequality for
maximum likelihood estimation that induces these convergence rates. We analyze
prediction region coverage properties, large-sample efficiency, and robustness
properties of four methods for constructing conformal prediction intervals for
GLMs: fully nonparametric kernel-based conformal, residual based conformal,
normalized residual based conformal, and parametric conformal which uses the
assumed GLM density as a conformity measure. Extensive simulations compare
these approaches to standard asymptotic prediction regions. The utility of the
parametric conformal prediction region is demonstrated in an application to
interval prediction of glycosylated hemoglobin levels, a blood measurement used
to diagnose diabetes.",http://arxiv.org/pdf/1905.03657v3,stat.ME
2019-05-08 17:21:11+00:00,Conformalized Quantile Regression,"['Yaniv Romano', 'Evan Patterson', 'Emmanuel J. Candès']","Conformal prediction is a technique for constructing prediction intervals
that attain valid coverage in finite samples, without making distributional
assumptions. Despite this appeal, existing conformal methods can be
unnecessarily conservative because they form intervals of constant or weakly
varying length across the input space. In this paper we propose a new method
that is fully adaptive to heteroscedasticity. It combines conformal prediction
with classical quantile regression, inheriting the advantages of both. We
establish a theoretical guarantee of valid coverage, supplemented by extensive
experiments on popular regression datasets. We compare the efficiency of
conformalized quantile regression to other conformal methods, showing that our
method tends to produce shorter intervals.",http://arxiv.org/pdf/1905.03222v1,stat.ME
2019-05-08 06:16:01+00:00,Predictive inference with the jackknife+,"['Rina Foygel Barber', 'Emmanuel J. Candes', 'Aaditya Ramdas', 'Ryan J. Tibshirani']","This paper introduces the jackknife+, which is a novel method for
constructing predictive confidence intervals. Whereas the jackknife outputs an
interval centered at the predicted response of a test point, with the width of
the interval determined by the quantiles of leave-one-out residuals, the
jackknife+ also uses the leave-one-out predictions at the test point to account
for the variability in the fitted regression function. Assuming exchangeable
training samples, we prove that this crucial modification permits rigorous
coverage guarantees regardless of the distribution of the data points, for any
algorithm that treats the training points symmetrically. Such guarantees are
not possible for the original jackknife and we demonstrate examples where the
coverage rate may actually vanish. Our theoretical and empirical analysis
reveals that the jackknife and the jackknife+ intervals achieve nearly exact
coverage and have similar lengths whenever the fitting algorithm obeys some
form of stability. Further, we extend the jackknife+ to K-fold cross validation
and similarly establish rigorous coverage properties. Our methods are related
to cross-conformal prediction proposed by Vovk [2015] and we discuss
connections.",http://arxiv.org/pdf/1905.02928v3,stat.ME
2019-04-12 17:18:08+00:00,Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout,"['Isidro Cortes-Ciriano', 'Andreas Bender']","While the use of deep learning in drug discovery is gaining increasing
attention, the lack of methods to compute reliable errors in prediction for
Neural Networks prevents their application to guide decision making in domains
where identifying unreliable predictions is essential, e.g. precision medicine.
Here, we present a framework to compute reliable errors in prediction for
Neural Networks using Test-Time Dropout and Conformal Prediction. Specifically,
the algorithm consists of training a single Neural Network using dropout, and
then applying it N times to both the validation and test sets, also employing
dropout in this step. Therefore, for each instance in the validation and test
sets an ensemble of predictions were generated. The residuals and absolute
errors in prediction for the validation set were then used to compute
prediction errors for test set instances using Conformal Prediction. We show
using 24 bioactivity data sets from ChEMBL 23 that dropout Conformal Predictors
are valid (i.e., the fraction of instances whose true value lies within the
predicted interval strongly correlates with the confidence level) and
efficient, as the predicted confidence intervals span a narrower set of values
than those computed with Conformal Predictors generated using Random Forest
(RF) models. Lastly, we show in retrospective virtual screening experiments
that dropout and RF-based Conformal Predictors lead to comparable retrieval
rates of active compounds. Overall, we propose a computationally efficient
framework (as only N extra forward passes are required in addition to training
a single network) to harness Test-Time Dropout and the Conformal Prediction
framework, and to thereby generate reliable prediction errors for deep Neural
Networks.",http://arxiv.org/pdf/1904.06330v1,cs.LG
2019-04-12 03:06:00+00:00,Conformal Prediction Under Covariate Shift,"['Ryan J. Tibshirani', 'Rina Foygel Barber', 'Emmanuel J. Candes', 'Aaditya Ramdas']","We extend conformal prediction methodology beyond the case of exchangeable
data. In particular, we show that a weighted version of conformal prediction
can be used to compute distribution-free prediction intervals for problems in
which the test and training covariate distributions differ, but the likelihood
ratio between these two distributions is known---or, in practice, can be
estimated accurately with access to a large set of unlabeled data (test
covariate points). Our weighted extension of conformal prediction also applies
more generally, to settings in which the data satisfies a certain weighted
notion of exchangeability. We discuss other potential applications of our new
conformal methodology, including latent variable and missing data problems.",http://arxiv.org/pdf/1904.06019v3,stat.ME
2019-04-11 20:09:39+00:00,Combining Conformance Checking and Classification of XES Log Data for the Manufacturing Domain,"['Matthias Ehrendorfer', 'Juergen-Albrecht Fassmann', 'Juergen Mangler', 'Stefanie Rinderle-Ma']","Currently, data collection on the shop floor is based on individual resources
such as machines, robots, and Autonomous Guided Vehicles (AGVs). There is a gap
between this approach and manufacturing orchestration software that supervises
the process of creating single products and controls the ressources'
interactions. This creates the need to save resource-based data streams in
databases, clean it, and then re-contextualize it, i.e., by connecting it to
orders, batches, and single products. Looking at this data from a
process-oriented analysis point of view enables new analysis prospects. This
paper utilises these prospects in an experimental way by creating BPMN models
for the manufacturing of two real-world products: (1) a low volume, high
complexity lower-housing for a gas-turbine and (2) a high volume, low
complexity, small tolerance valve lifter for a gas turbine. In contrast to the
resource-based data collection, 30+ values are modeled into the BPMN models and
enacted by a workflow engine, creating execution logs in the XES standard
format. Conformance checks are carried out and interpreted for both scenarios
and it is shown how existing classification and clustering techniques can be
applied on the collected data in order to predict good and bad parts, ex-post
and potentially at run-time.",http://arxiv.org/pdf/1904.05883v1,cs.OH
2019-04-02 22:22:03+00:00,Out of Site: Empowering a New Approach to Online Boycotts,"['H. Li', 'B. Alarcon', 'S. M. Espinosa', 'B', 'Hecht']","GrabYourWallet, #boycottNRA and other online boycott campaigns have attracted
substantial public interest in recent months. However, a number of significant
challenges are preventing online boycotts from reaching their potential. In
particular, complex webs of brands and subsidiaries can make it difficult for
participants to conform to the goals of a boycott. Similarly, participants and
organizers have limited visibility into a boycott's progress. This affects
their ability to use sociotechnical innovations from social computing to
incentivize participation. To address these challenges, this paper makes a
system contribution: a new boycott tool called Out of Site. Out of Site uses
lightweight automation to remove obstacles to successful online boycotts. We
describe the design challenges associated with Out of Site and report results
from two phases of deployment with the GrabYourWallet and Stop Animal Testing
boycott communities. Our findings highlight the potential of boycott-assisting
technologies and inform the design of this new class of technologies. Finally,
like is the case for many systems in social computing, while we designed Out of
Site for pro-social uses, there are a number of easily predictable ways in
which the system can be leveraged for anti-social purposes (e.g. exacerbating
filter bubble issues, empowering boycotts of businesses owned by racial,
ethnic, and religious minorities). As such, we developed for this project a
new, very straightforward design approach that treats preventing these
anti-social uses as a top-tier design concern. This approach stands in contrast
to the status quo of ignoring potential anti-social uses and/or considering
them to be a secondary design priority. We discuss how our simple approach may
help other research projects reduce their potential negative impacts with
minimal burden.",http://arxiv.org/pdf/1904.01688v1,cs.HC
2019-03-31 01:06:22+00:00,Molecular geometry prediction using a deep generative graph neural network,"['Elman Mansimov', 'Omar Mahmood', 'Seokho Kang', 'Kyunghyun Cho']","A molecule's geometry, also known as conformation, is one of a molecule's
most important properties, determining the reactions it participates in, the
bonds it forms, and the interactions it has with other molecules. Conventional
conformation generation methods minimize hand-designed molecular force field
energy functions that are often not well correlated with the true energy
function of a molecule observed in nature. They generate geometrically diverse
sets of conformations, some of which are very similar to the lowest-energy
conformations and others of which are very different. In this paper, we propose
a conditional deep generative graph neural network that learns an energy
function by directly learning to generate molecular conformations that are
energetically favorable and more likely to be observed experimentally in
data-driven manner. On three large-scale datasets containing small molecules,
we show that our method generates a set of conformations that on average is far
more likely to be close to the corresponding reference conformations than are
those obtained from conventional force field methods. Our method maintains
geometrical diversity by generating conformations that are not too similar to
each other, and is also computationally faster. We also show that our method
can be used to provide initial coordinates for conventional force field
methods. On one of the evaluated datasets we show that this combination allows
us to combine the best of both methods, yielding generated conformations that
are on average close to reference conformations with some very similar to
reference conformations.",http://arxiv.org/pdf/1904.00314v2,cs.LG
2019-03-29 05:43:33+00:00,Parallelizable global conformal parameterization of simply-connected surfaces via partial welding,"['Gary P. T. Choi', 'Yusan Leung-Liu', 'Xianfeng Gu', 'Lok Ming Lui']","Conformal surface parameterization is useful in graphics, imaging and
visualization, with applications to texture mapping, atlas construction,
registration, remeshing and so on. With the increasing capability in scanning
and storing data, dense 3D surface meshes are common nowadays. While meshes
with higher resolution better resemble smooth surfaces, they pose computational
difficulties for the existing parameterization algorithms. In this work, we
propose a novel parallelizable algorithm for computing the global conformal
parameterization of simply-connected surfaces via partial welding maps. A given
simply-connected surface is first partitioned into smaller subdomains. The
local conformal parameterizations of all subdomains are then computed in
parallel. The boundaries of the parameterized subdomains are subsequently
integrated consistently using a novel technique called partial welding, which
is developed based on conformal welding theory. Finally, by solving the Laplace
equation for each subdomain using the updated boundary conditions, we obtain a
global conformal parameterization of the given surface, with bijectivity
guaranteed by quasi-conformal theory. By including additional shape
constraints, our method can be easily extended to achieve disk conformal
parameterization for simply-connected open surfaces and spherical conformal
parameterization for genus-0 closed surfaces. Experimental results are
presented to demonstrate the effectiveness of our proposed algorithm. When
compared to the state-of-the-art conformal parameterization methods, our method
achieves a significant improvement in both computational time and accuracy.",http://arxiv.org/pdf/1903.12359v2,cs.CG
2019-03-27 17:47:13+00:00,Effect of Values and Technology Use on Exercise: Implications for Personalized Behavior Change Interventions,"['Yelena Mejova', 'Kyriaki Kalimeri']","Technology has recently been recruited in the war against the ongoing obesity
crisis; however, the adoption of Health & Fitness applications for regular
exercise is a struggle. In this study, we present a unique demographically
representative dataset of 15k US residents that combines technology use logs
with surveys on moral views, human values, and emotional contagion. Combining
these data, we provide a holistic view of individuals to model their physical
exercise behavior. First, we show which values determine the adoption of Health
& Fitness mobile applications, finding that users who prioritize the value of
purity and de-emphasize values of conformity, hedonism, and security are more
likely to use such apps. Further, we achieve a weighted AUROC of .673 in
predicting whether individual exercises, and we also show that the application
usage data allows for substantially better classification performance (.608)
compared to using basic demographics (.513) or internet browsing data (.546).
We also find a strong link of exercise to respondent socioeconomic status, as
well as the value of happiness. Using these insights, we propose actionable
design guidelines for persuasive technologies targeting health behavior
modification.",http://arxiv.org/pdf/1903.11579v1,cs.HC
2019-03-27 12:00:20+00:00,Does My Rebuttal Matter? Insights from a Major NLP Conference,"['Yang Gao', 'Steffen Eger', 'Ilia Kuznetsov', 'Iryna Gurevych', 'Yusuke Miyao']","Peer review is a core element of the scientific process, particularly in
conference-centered fields such as ML and NLP. However, only few studies have
evaluated its properties empirically. Aiming to fill this gap, we present a
corpus that contains over 4k reviews and 1.2k author responses from ACL-2018.
We quantitatively and qualitatively assess the corpus. This includes a pilot
study on paper weaknesses given by reviewers and on quality of author
responses. We then focus on the role of the rebuttal phase, and propose a novel
task to predict after-rebuttal (i.e., final) scores from initial reviews and
author responses. Although author responses do have a marginal (and
statistically significant) influence on the final scores, especially for
borderline papers, our results suggest that a reviewer's final score is largely
determined by her initial score and the distance to the other reviewers'
initial scores. In this context, we discuss the conformity bias inherent to
peer reviewing, a bias that has largely been overlooked in previous research.
We hope our analyses will help better assess the usefulness of the rebuttal
phase in NLP conferences.",http://arxiv.org/pdf/1903.11367v2,cs.CL
2019-03-19 20:42:22+00:00,Fluidic Fabric Muscle Sheets for Wearable and Soft Robotics,"['Mengjia Zhu', 'Thanh Nho Do', 'Elliot Hawkes', 'Yon Visell']","Conformable robotic systems are attractive for applications in which they can
be used to actuate structures with large surface areas, to provide forces
through wearable garments, or to realize autonomous robotic systems. We present
a new family of soft actuators that we refer to as Fluidic Fabric Muscle Sheets
(FFMS). They are composite fabric structures that integrate fluidic
transmissions based on arrays of elastic tubes. These sheet-like actuators can
strain, squeeze, bend, and conform to hard or soft objects of arbitrary shapes
or sizes, including the human body. We show how to design and fabricate FFMS
actuators via facile apparel engineering methods, including computerized sewing
techniques. Together, these determine the distributions of stresses and strains
that can be generated by the FFMS. We present a simple mathematical model that
proves effective for predicting their performance. FFMS can operate at
frequencies of 5 Hertz or more, achieve engineering strains exceeding 100%, and
exert forces greater than 115 times their own weight. They can be safely used
in intimate contact with the human body even when delivering stresses exceeding
10$^\text{6}$ Pascals. We demonstrate their versatility for actuating a variety
of bodies or structures, and in configurations that perform multi-axis
actuation, including bending and shape change. As we also show, FFMS can be
used to exert forces on body tissues for wearable and biomedical applications.
We demonstrate several potential use cases, including a miniature steerable
robot, a glove for grasp assistance, garments for applying compression to the
extremities, and devices for actuating small body regions or tissues via
localized skin stretch.",http://arxiv.org/pdf/1903.08253v1,cs.RO
2019-03-11 14:41:30+00:00,"Modeling, discretization, and hyperchaos detection of conformable derivative approach to a financial system with market confidence and ethics risk","['Baogui Xin', 'Wei Peng', 'Yekyung Kwon', 'Yanqin Liu']","A new chaotic financial system is proposed by considering ethics involvement
in a four-dimensional financial system with market confidence. A
five-dimensional conformable derivative financial system is presented by
introducing conformable fractional calculus to the integer-order system. A
discretization scheme is proposed to calculate numerical solutions of
conformable derivative systems. The scheme is illustrated by testing hyperchaos
for the system.",http://arxiv.org/pdf/1903.12267v2,q-fin.GN
2019-02-28 14:50:59+00:00,Citation Needed: A Taxonomy and Algorithmic Assessment of Wikipedia's Verifiability,"['Miriam Redi', 'Besnik Fetahu', 'Jonathan Morgan', 'Dario Taraborelli']","Wikipedia is playing an increasingly central role on the web,and the policies
its contributors follow when sourcing and fact-checking content affect million
of readers. Among these core guiding principles, verifiability policies have a
particularly important role. Verifiability requires that information included
in a Wikipedia article be corroborated against reliable secondary sources.
Because of the manual labor needed to curate and fact-check Wikipedia at scale,
however, its contents do not always evenly comply with these policies.
Citations (i.e. reference to external sources) may not conform to verifiability
requirements or may be missing altogether, potentially weakening the
reliability of specific topic areas of the free encyclopedia. In this paper, we
aim to provide an empirical characterization of the reasons why and how
Wikipedia cites external sources to comply with its own verifiability
guidelines. First, we construct a taxonomy of reasons why inline citations are
required by collecting labeled data from editors of multiple Wikipedia language
editions. We then collect a large-scale crowdsourced dataset of Wikipedia
sentences annotated with categories derived from this taxonomy. Finally, we
design and evaluate algorithmic models to determine if a statement requires a
citation, and to predict the citation reason based on our taxonomy. We evaluate
the robustness of such models across different classes of Wikipedia articles of
varying quality, as well as on an additional dataset of claims annotated for
fact-checking purposes.",http://arxiv.org/pdf/1902.11116v1,cs.CY
2019-02-26 05:17:18+00:00,Functional Transparency for Structured Data: a Game-Theoretic Approach,"['Guang-He Lee', 'Wengong Jin', 'David Alvarez-Melis', 'Tommi S. Jaakkola']","We provide a new approach to training neural models to exhibit transparency
in a well-defined, functional manner. Our approach naturally operates over
structured data and tailors the predictor, functionally, towards a chosen
family of (local) witnesses. The estimation problem is setup as a co-operative
game between an unrestricted predictor such as a neural network, and a set of
witnesses chosen from the desired transparent family. The goal of the witnesses
is to highlight, locally, how well the predictor conforms to the chosen family
of functions, while the predictor is trained to minimize the highlighted
discrepancy. We emphasize that the predictor remains globally powerful as it is
only encouraged to agree locally with locally adapted witnesses. We analyze the
effect of the proposed approach, provide example formulations in the context of
deep graph and sequence models, and empirically illustrate the idea in chemical
property prediction, temporal modeling, and molecule representation learning.",http://arxiv.org/pdf/1902.09737v1,cs.LG
2019-02-21 09:17:33+00:00,Robust Power Scheduling for Microgrids with Uncertainty in Renewable Energy Generation,"['Amir Valibeygi', 'Abdulelah H. Habib', 'Raymond A. de Callafon']","A robust power scheduling algorithm is proposed to schedule power flow
between the main electricity grid and a microgird with solar energy generation
and battery energy storage subject to uncertainty in solar energy production.
To avoid over-conservatism in power scheduling while guaranteeing robustness
against uncertainties, time-varying ""soft"" constraints on the State of Charge
(SoC) of the battery are proposed. These soft constraints allow SoC limit
violation at steps far from the current step but aim to minimize such
violations in a controlled manner. The model predictive formulation of the
problem over a receding time horizon ensures that the resulting solution
eventually conforms to the hard SoC limits of the system at every step. The
optimization problem for each step is formulated as a quadratic programming
problem that is solved iteratively to find the soft constraints that are
closest to the hard ones and still yield a feasible solution. Optimization
results demonstrate the effectiveness of the approach.",http://arxiv.org/pdf/1902.07927v1,cs.SY
2019-02-18 14:13:04+00:00,Conformal calibrators,"['Vladimir Vovk', 'Ivan Petej', 'Paolo Toccaceli', 'Alex Gammerman']","Most existing examples of full conformal predictive systems, split-conformal
predictive systems, and cross-conformal predictive systems impose severe
restrictions on the adaptation of predictive distributions to the test object
at hand. In this paper we develop split-conformal and cross-conformal
predictive systems that are fully adaptive. Our method consists in calibrating
existing predictive systems; the input predictive system is not supposed to
satisfy any properties of validity, whereas the output predictive system is
guaranteed to be calibrated in probability. It is interesting that the method
may also work without the IID assumption, standard in conformal prediction.",http://arxiv.org/pdf/1902.06579v1,cs.LG
2019-02-07 17:12:53+00:00,A conformance relation and complete test suites for I/O systems,"['Adilson Luiz Bonifacio', 'Arnaldo Vieira Moura']","Model based testing is a well-established approach to verify implementations
modeled by I/O labeled transition systems (IOLTSs). One of the challenges
stemming from model based testing is the conformance checking and the
generation of test suites, specially when completeness is a required property.
In order to check whether an implementation under test is in compliance with
its respective specification one resorts to some form of conformance relation
that guarantees the expected behavior of the implementations, given the
behavior of the specification. The ioco conformance relation is an example of
such a relation, specially suited for asynchronous models. In this work we
study a more general conformance relation, show how to generate finite and
complete test suites, and discuss the complexity of the test generation
mechanism under this more general conformance relation. We also show that ioco
conformance is a special case of this new conformance relation, and we
investigate the complexity of classical ioco-complete test suites. Further, we
relate our contributions to more recent works, accommodating the restrictions
of their classes of fault models within our more general approach as special
cases, and expose the complexity of generating any complete test suite that
must satisfy their restrictions.",http://arxiv.org/pdf/1902.10278v3,cs.SE
2018-12-07 10:59:09+00:00,More or Less? Predict the Social Influence of Malicious URLs on Social Media,"['Chun-Ming Lai', 'Xiaoyun Wang', 'Jon W. Chapman', 'Yu-Cheng Lin', 'Yu-Chang Ho', 'S. Felix Wu', 'Patrick McDaniel', 'Hasan Cam']","Users of Online Social Networks (OSNs) interact with each other more than
ever. In the context of a public discussion group, people receive, read, and
write comments in response to articles and postings. In the absence of access
control mechanisms, OSNs are a great environment for attackers to influence
others, from spreading phishing URLs, to posting fake news. Moreover, OSN user
behavior can be predicted by social science concepts which include conformity
and the bandwagon effect. In this paper, we show how social recommendation
systems affect the occurrence of malicious URLs on Facebook. We exploit
temporal features to build a prediction framework, having greater than 75%
accuracy, to predict whether the following group users' behavior will increase
or not. Included in this work, we demarcate classes of URLs, including those
malicious URLs classified as creating critical damage, as well as those of a
lesser nature which only inflict light damage such as aggressive commercial
advertisements and spam content. It is our hope that the data and analyses in
this paper provide a better understanding of OSN user reactions to different
categories of malicious URLs, thereby providing a way to mitigate the influence
of these malicious URL attacks.",http://arxiv.org/pdf/1812.02978v1,cs.SI
2018-12-03 18:43:27+00:00,FoldingZero: Protein Folding from Scratch in Hydrophobic-Polar Model,"['Yanjun Li', 'Hengtong Kang', 'Ketian Ye', 'Shuyu Yin', 'Xiaolin Li']","De novo protein structure prediction from amino acid sequence is one of the
most challenging problems in computational biology. As one of the extensively
explored mathematical models for protein folding, Hydrophobic-Polar (HP) model
enables thorough investigation of protein structure formation and evolution.
Although HP model discretizes the conformational space and simplifies the
folding energy function, it has been proven to be an NP-complete problem. In
this paper, we propose a novel protein folding framework FoldingZero,
self-folding a de novo protein 2D HP structure from scratch based on deep
reinforcement learning. FoldingZero features the coupled approach of a two-head
(policy and value heads) deep convolutional neural network (HPNet) and a
regularized Upper Confidence Bounds for Trees (R-UCT). It is trained solely by
a reinforcement learning algorithm, which improves HPNet and R-UCT iteratively
through iterative policy optimization. Without any supervision and domain
knowledge, FoldingZero not only achieves comparable results, but also learns
the latent folding knowledge to stabilize the structure. Without exponential
computation, FoldingZero shows promising potential to be adopted for real-world
protein properties prediction.",http://arxiv.org/pdf/1812.00967v1,cs.LG
2018-12-02 21:24:10+00:00,Automatic hyperparameter selection in Autodock,"['Hojjat Rakhshani', 'Lhassane Idoumghar', 'Julien Lepagnot', 'Mathieu Brevilliers', 'Edward Keedwell']","Autodock is a widely used molecular modeling tool which predicts how small
molecules bind to a receptor of known 3D structure. The current version of
AutoDock uses meta-heuristic algorithms in combination with local search
methods for doing the conformation search. Appropriate settings of
hyperparameters in these algorithms are important, particularly for novice
users who often find it hard to identify the best configuration. In this work,
we design a surrogate based multi-objective algorithm to help such users by
automatically tuning hyperparameter settings. The proposed method iteratively
uses a radial basis function model and non-dominated sorting to evaluate the
sampled configurations during the search phase. Our experimental results using
Autodock show that the introduced component is practical and effective.",http://arxiv.org/pdf/1812.02618v1,stat.ML
2018-12-02 08:09:13+00:00,That's Mine! Learning Ownership Relations and Norms for Robots,"['Zhi-Xuan Tan', 'Jake Brawer', 'Brian Scassellati']","The ability for autonomous agents to learn and conform to human norms is
crucial for their safety and effectiveness in social environments. While recent
work has led to frameworks for the representation and inference of simple
social rules, research into norm learning remains at an exploratory stage.
Here, we present a robotic system capable of representing, learning, and
inferring ownership relations and norms. Ownership is represented as a graph of
probabilistic relations between objects and their owners, along with a database
of predicate-based norms that constrain the actions permissible on owned
objects. To learn these norms and relations, our system integrates (i) a novel
incremental norm learning algorithm capable of both one-shot learning and
induction from specific examples, (ii) Bayesian inference of ownership
relations in response to apparent rule violations, and (iii) percept-based
prediction of an object's likely owners. Through a series of simulated and
real-world experiments, we demonstrate the competence and flexibility of the
system in performing object manipulation tasks that require a variety of norms
to be followed, laying the groundwork for future research into the acquisition
and application of social norms.",http://arxiv.org/pdf/1812.02576v2,cs.AI
2018-11-24 08:57:26+00:00,Three-Dimensionally Embedded Graph Convolutional Network (3DGCN) for Molecule Interpretation,"['Hyeoncheol Cho', 'Insung S. Choi']","We present a three-dimensional graph convolutional network (3DGCN), which
predicts molecular properties and biochemical activities, based on 3D molecular
graph. In the 3DGCN, graph convolution is unified with learning operations on
the vector to handle the spatial information from molecular topology. The 3DGCN
model exhibits significantly higher performance on various tasks compared with
other deep-learning models, and has the ability of generalizing a given
conformer to targeted features regardless of its rotations in the 3D space.
More significantly, our model also can distinguish the 3D rotations of a
molecule and predict the target value, depending upon the rotation degree, in
the protein-ligand docking problem, when trained with orientation-dependent
datasets. The rotation distinguishability of 3DGCN, along with rotation
equivariance, provides a key milestone in the implementation of
three-dimensionality to the field of deep-learning chemistry that solves
challenging biochemical problems.",http://arxiv.org/pdf/1811.09794v4,cs.LG
2018-11-06 19:00:06+00:00,Deep Weighted Averaging Classifiers,"['Dallas Card', 'Michael Zhang', 'Noah A. Smith']","Recent advances in deep learning have achieved impressive gains in
classification accuracy on a variety of types of data, including images and
text. Despite these gains, however, concerns have been raised about the
calibration, robustness, and interpretability of these models. In this paper we
propose a simple way to modify any conventional deep architecture to
automatically provide more transparent explanations for classification
decisions, as well as an intuitive notion of the credibility of each
prediction. Specifically, we draw on ideas from nonparametric kernel
regression, and propose to predict labels based on a weighted sum of training
instances, where the weights are determined by distance in a learned
instance-embedding space. Working within the framework of conformal methods, we
propose a new measure of nonconformity suggested by our model, and
experimentally validate the accompanying theoretical expectations,
demonstrating improved transparency, controlled error rates, and robustness to
out-of-domain data, without compromising on accuracy or calibration.",http://arxiv.org/pdf/1811.02579v2,cs.LG
2018-11-04 08:49:38+00:00,Deep Robust Framework for Protein Function Prediction using Variable-Length Protein Sequences,"['Ashish Ranjan', 'Md Shah Fahad', 'David Fernandez-Baca', 'Akshay Deepak', 'Sudhakar Tripathi']","Amino acid sequence portrays most intrinsic form of a protein and expresses
primary structure of protein. The order of amino acids in a sequence enables a
protein to acquire a particular stable conformation that is responsible for the
functions of the protein. This relationship between a sequence and its function
motivates the need to analyse the sequences for predicting protein functions.
Early generation computational methods using BLAST, FASTA, etc. perform
function transfer based on sequence similarity with existing databases and are
computationally slow. Although machine learning based approaches are fast, they
fail to perform well for long protein sequences (i.e., protein sequences with
more than 300 amino acid residues). In this paper, we introduce a novel method
for construction of two separate feature sets for protein sequences based on
analysis of 1) single fixed-sized segments and 2) multi-sized segments, using
bi-directional long short-term memory network. Further, model based on proposed
feature set is combined with the state of the art Multi-lable Linear
Discriminant Analysis (MLDA) features based model to improve the accuracy.
Extensive evaluations using separate datasets for biological processes and
molecular functions demonstrate promising results for both single-sized and
multi-sized segments based feature sets. While former showed an improvement of
+3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00%
respectively for two datasets over the state of the art MLDA based classifier.
After combining two models, there is a significant improvement of +7.41% and
+9.21% respectively for two datasets compared to MLDA based classifier.
Specifically, the proposed approach performed well for the long protein
sequences and superior overall performance.",http://arxiv.org/pdf/1811.01338v2,cs.LG
2018-10-12 12:58:11+00:00,Social capital predicts corruption risk in towns,"['Johannes Wachs', 'Taha Yasseri', 'Balázs Lengyel', 'János Kertész']","Corruption is a social plague: gains accrue to small groups, while its costs
are borne by everyone. Significant variation in its level between and within
countries suggests a relationship between social structure and the prevalence
of corruption, yet, large scale empirical studies thereof have been missing due
to lack of data. In this paper we relate the structural characteristics of
social capital of towns with corruption in their local governments. Using
datasets from Hungary, we quantify corruption risk by suppressed competition
and lack of transparency in the town's awarded public contracts. We
characterize social capital using social network data from a popular online
platform. Controlling for social, economic, and political factors, we find that
settlements with fragmented social networks, indicating an excess of
\textit{bonding social capital} have higher corruption risk and towns with more
diverse external connectivity, suggesting a surplus of \textit{bridging social
capital} are less exposed to corruption. We interpret fragmentation as
fostering in-group favoritism and conformity, which increase corruption, while
diversity facilitates impartiality in public life and stifles corruption.",http://arxiv.org/pdf/1810.05485v1,cs.SI
2018-09-24 17:08:08+00:00,Deep Confidence: A Computationally Efficient Framework for Calculating Reliable Errors for Deep Neural Networks,"['Isidro Cortes-Ciriano', 'Andreas Bender']","Deep learning architectures have proved versatile in a number of drug
discovery applications, including the modelling of in vitro compound activity.
While controlling for prediction confidence is essential to increase the trust,
interpretability and usefulness of virtual screening models in drug discovery,
techniques to estimate the reliability of the predictions generated with deep
learning networks remain largely underexplored. Here, we present Deep
Confidence, a framework to compute valid and efficient confidence intervals for
individual predictions using the deep learning technique Snapshot Ensembling
and conformal prediction. Specifically, Deep Confidence generates an ensemble
of deep neural networks by recording the network parameters throughout the
local minima visited during the optimization phase of a single neural network.
This approach serves to derive a set of base learners (i.e., snapshots) with
comparable predictive power on average, that will however generate slightly
different predictions for a given instance. The variability across base
learners and the validation residuals are in turn harnessed to compute
confidence intervals using the conformal prediction framework. Using a set of
24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles
perform on par with Random Forest (RF) and ensembles of independently trained
deep neural networks. In addition, we find that the confidence regions
predicted using the Deep Confidence framework span a narrower set of values.
Overall, Deep Confidence represents a highly versatile error prediction
framework that can be applied to any deep learning-based application at no
extra computational cost.",http://arxiv.org/pdf/1809.09060v1,cs.LG
2018-09-20 01:00:00+00:00,Distribution-Free Prediction Sets for Two-Layer Hierarchical Models,"['Robin Dunn', 'Larry Wasserman', 'Aaditya Ramdas']","We consider the problem of constructing distribution-free prediction sets for
data from two-layer hierarchical distributions. For iid data, prediction sets
can be constructed using the method of conformal prediction. The validity of
conformal prediction hinges on the exchangeability of the data, which does not
hold when groups of observations come from distinct distributions, such as
multiple observations on each patient in a medical database. We extend
conformal methods to this hierarchical setting. We develop CDF pooling, single
subsampling, and repeated subsampling approaches to construct prediction sets
in unsupervised and supervised settings. We compare these approaches in terms
of coverage and average set size. If asymptotic coverage is acceptable, we
recommend CDF pooling for its balance between empirical coverage and average
set size. If we desire coverage guarantees, then we recommend the repeated
subsampling approach.",http://arxiv.org/pdf/1809.07441v4,stat.ME
2018-08-24 20:59:56+00:00,Reproducible and Interpretable Spiculation Quantification for Lung Cancer Screening,"['Wookjin Choi', 'Saad Nadeem', 'Sadegh Riyahi', 'Joseph O. Deasy', 'Allen Tannenbaum', 'Wei Lu']","Spiculations are important predictors of lung cancer malignancy, which are
spikes on the surface of the pulmonary nodules. In this study, we proposed an
interpretable and parameter-free technique to quantify the spiculation using
area distortion metric obtained by the conformal (angle-preserving) spherical
parameterization. We exploit the insight that for an angle-preserved spherical
mapping of a given nodule, the corresponding negative area distortion precisely
characterizes the spiculations on that nodule. We introduced novel spiculation
scores based on the area distortion metric and spiculation measures. We also
semi-automatically segment lung nodule (for reproducibility) as well as vessel
and wall attachment to differentiate the real spiculations from lobulation and
attachment. A simple pathological malignancy prediction model is also
introduced. We used the publicly-available LIDC-IDRI dataset pathologists
(strong-label) and radiologists (weak-label) ratings to train and test
radiomics models containing this feature, and then externally validate the
models. We achieved AUC$=$0.80 and 0.76, respectively, with the models trained
on the 811 weakly-labeled LIDC datasets and tested on the 72 strongly-labeled
LIDC and 73 LUNGx datasets; the previous best model for LUNGx had AUC$=$0.68.
The number-of-spiculations feature was found to be highly correlated
(Spearman's rank correlation coefficient $\rho = 0.44$) with the radiologists'
spiculation score. We developed a reproducible and interpretable,
parameter-free technique for quantifying spiculations on nodules. The
spiculation quantification measures was then applied to the radiomics framework
for pathological malignancy prediction with reproducible semi-automatic
segmentation of nodule. Using our interpretable features (size, attachment,
spiculation, lobulation), we were able to achieve higher performance than
previous models.",http://arxiv.org/pdf/1808.08307v3,cs.CV
2018-08-15 04:05:02+00:00,Putting the Horse Before the Cart:A Generator-Evaluator Framework for Question Generation from Text,"['Vishwajeet Kumar', 'Ganesh Ramakrishnan', 'Yuan-Fang Li']","Automatic question generation (QG) is a useful yet challenging task in NLP.
Recent neural network-based approaches represent the state-of-the-art in this
task. In this work, we attempt to strengthen them significantly by adopting a
holistic and novel generator-evaluator framework that directly optimizes
objectives that reward semantics and structure. The {\it generator} is a
sequence-to-sequence model that incorporates the {\it structure} and {\it
semantics} of the question being generated. The generator predicts an answer in
the passage that the question can pivot on. Employing the copy and coverage
mechanisms, it also acknowledges other contextually important (and possibly
rare) keywords in the passage that the question needs to conform to, while not
redundantly repeating words. The {\it evaluator} model evaluates and assigns a
reward to each predicted question based on its conformity to the {\it
structure} of ground-truth questions. We propose two novel QG-specific reward
functions for text conformity and answer conformity of the generated question.
The evaluator also employs structure-sensitive rewards based on evaluation
measures such as BLEU, GLEU, and ROUGE-L, which are suitable for QG. In
contrast, most of the previous works only optimize the cross-entropy loss,
which can induce inconsistencies between training (objective) and testing
(evaluation) measures. Our evaluation shows that our approach significantly
outperforms state-of-the-art systems on the widely-used SQuAD benchmark as per
both automatic and human evaluation.",http://arxiv.org/pdf/1808.04961v5,cs.CL
2018-07-23 08:59:04+00:00,Conformal Mesh Parameterization Using Discrete Calabi Flow,"['Hui Zhao', 'Xuan Li', 'Huabin Ge', 'Xianfeng Gu', 'Na Lei']","In this paper, we introduce discrete Calabi flow to the graphics research
community and present a novel conformal mesh parameterization algorithm. Calabi
energy has a succinct and explicit format. Its corresponding flow is conformal
and convergent under certain conditions. Our method is based on the Calabi
energy and Calabi flow with solid theoretical and mathematical base. We
demonstrate our approach on dozens of models and compare it with other related
flow based methods, such as the well-known Ricci flow and CETM. Our experiments
show that the performance of our algorithm is comparably the same with other
methods. The discrete Calabi flow in our method provides another perspective on
conformal flow and conformal parameterization.",http://arxiv.org/pdf/1807.08486v1,cs.GR
2018-07-12 21:11:46+00:00,Decentralized Multi-UAV Routing in the Presence of Disturbances,"['Wei Zhao', 'Borzoo Bonakdarpour']","We introduce a decentralized and online path planning technique for a network
of unmanned aerial vehicles (UAVs) in the presence of weather disturbances. In
our problem setting, the group of UAVs are required to collaboratively visit a
set of goals scattered in a 2-dimensional area. Each UAV will have to spend
energy to reach these goals, but due to unforeseen disturbances, the required
energy may vary over time and does not necessarily conform with the initial
forecast and/or pre-computed optimal paths. Thus, we are dealing with two
fundamental interrelated problems to find a global optimum at each point of
time: (1) energy consumption prediction based on disturbances and, hence,
online path replanning, and (2) distributed agreement among all UAVs to divide
the remaining unvisited goals based on their positions and energy requirements.
Our approach consists of four main components: (i) a distributed algorithm that
periodically divides the unvisited goals among all the UAVs based on the
current energy requirements of the UAVs, (ii) a local (i.e., UAV-level)
$\AStar$-based algorithm that computes the {\em desirable} path for each UAV to
reach the nodes assigned to it, (iii) a local PID controller that {\em
predicts} the inputs to the UAV (i.e., thrust and moments), and (iv) a planner
that computes the required energy and the replanning time period. We validate
our proposed solution through a rich set of simulations and show that our
approach is significantly more efficient than a best-effort algorithm that
directs each idle UAV to visit the closest unvisited goal.",http://arxiv.org/pdf/1807.04823v1,cs.DC
2018-07-04 14:43:57+00:00,Ensemble learning with Conformal Predictors: Targeting credible predictions of conversion from Mild Cognitive Impairment to Alzheimer's Disease,"['Telma Pereira', 'Sandra Cardoso', 'Dina Silva', 'Manuela Guerreiro', 'Alexandre de Mendonça', 'Sara C. Madeira']","Most machine learning classifiers give predictions for new examples
accurately, yet without indicating how trustworthy predictions are. In the
medical domain, this hampers their integration in decision support systems,
which could be useful in the clinical practice. We use a supervised learning
approach that combines Ensemble learning with Conformal Predictors to predict
conversion from Mild Cognitive Impairment to Alzheimer's Disease. Our goal is
to enhance the classification performance (Ensemble learning) and complement
each prediction with a measure of credibility (Conformal Predictors). Our
results showed the superiority of the proposed approach over a similar ensemble
framework with standard classifiers.",http://arxiv.org/pdf/1807.01619v2,cs.LG
2018-06-29 00:38:01+00:00,An Influence Network Model to Study Discrepancies in Expressed and Private Opinions,"['Mengbin Ye', 'Yuzhen Qin', 'Alain Govaert', 'Brian D. O. Anderson', 'Ming Cao']","In many social situations, a discrepancy arises between an individual's
private and expressed opinions on a given topic. Motivated by Solomon Asch's
seminal experiments on social conformity and other related socio-psychological
works, we propose a novel opinion dynamics model to study how such a
discrepancy can arise in general social networks of interpersonal influence.
Each individual in the network has both a private and an expressed opinion: an
individual's private opinion evolves under social influence from the expressed
opinions of the individual's neighbours, while the individual determines his or
her expressed opinion under a pressure to conform to the average expressed
opinion of his or her neighbours, termed the local public opinion. General
conditions on the network that guarantee exponentially fast convergence of the
opinions to a limit are obtained. Further analysis of the limit yields several
semi-quantitative conclusions, which have insightful social interpretations,
including the establishing of conditions that ensure every individual in the
network has such a discrepancy. Last, we show the generality and validity of
the model by using it to explain and predict the results of Solomon Asch's
seminal experiments.",http://arxiv.org/pdf/1806.11236v3,cs.SI
2018-06-11 14:07:24+00:00,Aggregating Predictions on Multiple Non-disclosed Datasets using Conformal Prediction,"['Ola Spjuth', 'Lars Carlsson', 'Niharika Gauraha']","Conformal Prediction is a machine learning methodology that produces valid
prediction regions under mild conditions. In this paper, we explore the
application of making predictions over multiple data sources of different sizes
without disclosing data between the sources. We propose that each data source
applies a transductive conformal predictor independently using the local data,
and that the individual predictions are then aggregated to form a combined
prediction region. We demonstrate the method on several data sets, and show
that the proposed method produces conservatively valid predictions and reduces
the variance in the aggregated predictions. We also study the effect that the
number of data sources and size of each source has on aggregated predictions,
as compared with equally sized sources and pooled data.",http://arxiv.org/pdf/1806.04000v2,stat.ML
2018-05-24 00:17:24+00:00,Cautious Deep Learning,"['Yotam Hechtlinger', 'Barnabás Póczos', 'Larry Wasserman']","Most classifiers operate by selecting the maximum of an estimate of the
conditional distribution $p(y|x)$ where $x$ stands for the features of the
instance to be classified and $y$ denotes its label. This often results in a
{\em hubristic bias}: overconfidence in the assignment of a definite label.
Usually, the observations are concentrated on a small volume but the classifier
provides definite predictions for the entire space. We propose constructing
conformal prediction sets which contain a set of labels rather than a single
label. These conformal prediction sets contain the true label with probability
$1-\alpha$. Our construction is based on $p(x|y)$ rather than $p(y|x)$ which
results in a classifier that is very cautious: it outputs the null set ---
meaning ""I don't know"" --- when the object does not resemble the training
examples. An important property of our approach is that adversarial attacks are
likely to be predicted as the null set or would also include the true label. We
demonstrate the performance on the ImageNet ILSVRC dataset and the CelebA and
IMDB-Wiki facial datasets using high dimensional features obtained from state
of the art convolutional neural networks.",http://arxiv.org/pdf/1805.09460v2,stat.ML
2018-05-04 13:59:59+00:00,The conformable fractional grey system model,"['Xin Ma', 'Wenqing Wu', 'Bo Zeng', 'Yong Wang', 'Xinxing Wu']","The fractional order grey models (FGM) have appealed considerable interest of
research in recent years due to its higher effectiveness and flexibility than
the conventional grey models and other prediction models. However, the
definitions of the fractional order accumulation (FOA) and difference (FOD) is
computationally complex, which leads to difficulties for the theoretical
analysis and applications. In this paper, the new definition of the FOA are
proposed based on the definitions of Conformable Fractional Derivative, which
is called the Conformable Fractional Accumulation (CFA), along with its inverse
operation, the Conformable Fractional Difference (CFD). Then the new
Conformable Fractional Grey Model (CFGM) based on CFA and CFD is introduced
with detailed modelling procedures. The feasibility and simplicity and the CFGM
are shown in the numerical example. And the at last the comprehensive
real-world case studies of natural gas production forecasting in 11 countries
are presented, and results show that the CFGM is much more effective than the
existing FGM model in the 165 subcases.",http://arxiv.org/pdf/1805.01789v3,stat.ME
2018-04-17 15:40:00+00:00,Classifying Antimicrobial and Multifunctional Peptides with Bayesian Network Models,"['Rainier Barrett', 'Shaoyi Jiang', 'Andrew D White']","Bayesian network models are finding success in characterizing
enzyme-catalyzed reactions, slow conformational changes, predicting enzyme
inhibition, and genomics. In this work, we apply them to statistical modeling
of peptides by simultaneously identifying amino acid sequence motifs and using
a motif-based model to clarify the role motifs may play in antimicrobial
activity. We construct models of increasing sophistication, demonstrating how
chemical knowledge of a peptide system may be embedded without requiring new
derivation of model fitting equations after changing model structure. These
models are used to construct classifiers with good performance (94% accuracy,
Matthews correlation coefficient of 0.87) at predicting antimicrobial activity
in peptides, while at the same time being built of interpretable parameters. We
demonstrate use of these models to identify peptides that are potentially both
antimicrobial and antifouling, and show that the background distribution of
amino acids could play a greater role in activity than sequence motifs do. This
provides an advancement in the type of peptide activity modeling that can be
done and the ease in which models can be constructed.",http://arxiv.org/pdf/1804.06327v1,stat.AP
2018-04-16 03:45:53+00:00,conformalClassification: A Conformal Prediction R Package for Classification,"['Niharika Gauraha', 'Ola Spjuth']","The conformalClassification package implements Transductive Conformal
Prediction (TCP) and Inductive Conformal Prediction (ICP) for classification
problems. Conformal Prediction (CP) is a framework that complements the
predictions of machine learning algorithms with reliable measures of
confidence. TCP gives results with higher validity than ICP, however ICP is
computationally faster than TCP. The package conformalClassification is built
upon the random forest method, where votes of the random forest for each class
are considered as the conformity scores for each data point. Although the main
aim of the conformalClassification package is to generate CP errors (p-values)
for classification problems, the package also implements various diagnostic
measures such as deviation from validity, error rate, efficiency, observed
fuzziness and calibration plots. In future releases, we plan to extend the
package to use other machine learning algorithms, (e.g. support vector
machines) for model fitting.",http://arxiv.org/pdf/1804.05494v1,stat.ML
2018-04-03 22:56:25+00:00,Hospital Readmission Prediction - Applying Hierarchical Sparsity Norms for Interpretable Models,"['Jialiang Jiang', 'Sharon Hewner', 'Varun Chandola']","Hospital readmissions have become one of the key measures of healthcare
quality. Preventable readmissions have been identified as one of the primary
targets for reducing costs and improving healthcare delivery. However, most
data driven studies for understanding readmissions have produced black box
classification and predictive models with moderate performance, which precludes
them from being used effectively within the decision support systems in the
hospitals. In this paper we present an application of structured
sparsity-inducing norms for predicting readmission risk for patients based on
their disease history and demographics. Most existing studies have focused on
hospital utilization, test results, etc., to assign a readmission label to each
episode of hospitalization. However, we focus on assigning a readmission risk
label to a patient based on their disease history. Our emphasis is on
interpreting the models to improve the understanding of the readmission
problem. To achieve this, we exploit the domain induced hierarchical structure
available for the disease codes which are the features for the classification
algorithm. We use a tree based sparsity-inducing regularization strategy that
explicitly uses the domain hierarchy. The resulting model not only outperforms
standard regularization procedures but is also highly sparse and interpretable.
We analyze the model and identify several significant factors that have an
effect on readmission risk. Some of these factors conform to existing beliefs,
e.g., impact of surgical complications and infections during hospital stay.
Other factors, such as the impact of mental disorder and substance abuse on
readmission, provide empirical evidence for several pre-existing but unverified
hypotheses. The analysis also reveals previously undiscovered connections such
as the influence of socioeconomic factors like lack of housing and
malnutrition.",http://arxiv.org/pdf/1804.01188v1,cs.LG
2018-03-29 16:21:10+00:00,Conformal Prediction in Learning Under Privileged Information Paradigm with Applications in Drug Discovery,"['Niharika Gauraha', 'Lars Carlsson', 'Ola Spjuth']","This paper explores conformal prediction in the learning under privileged
information (LUPI) paradigm. We use the SVM+ realization of LUPI in an
inductive conformal predictor, and apply it to the MNIST benchmark dataset and
three datasets in drug discovery. The results show that using privileged
information produces valid models and improves efficiency compared to standard
SVM, however the improvement varies between the tested datasets and is not
substantial in the drug discovery applications. More importantly, using SVM+ in
a conformal prediction framework enables valid prediction intervals at
specified significance levels.",http://arxiv.org/pdf/1803.11136v2,stat.ML
2018-02-17 21:43:28+00:00,Exact and Robust Conformal Inference Methods for Predictive Machine Learning With Dependent Data,"['Victor Chernozhukov', 'Kaspar Wuthrich', 'Yinchu Zhu']","We extend conformal inference to general settings that allow for time series
data. Our proposal is developed as a randomization method and accounts for
potential serial dependence by including block structures in the permutation
scheme. As a result, the proposed method retains the exact, model-free validity
when the data are i.i.d. or more generally exchangeable, similar to usual
conformal inference methods. When exchangeability fails, as is the case for
common time series data, the proposed approach is approximately valid under
weak assumptions on the conformity score.",http://arxiv.org/pdf/1802.06300v3,stat.ML
2017-12-28 14:04:52+00:00,Future Frame Prediction for Anomaly Detection -- A New Baseline,"['Wen Liu', 'Weixin Luo', 'Dongze Lian', 'Shenghua Gao']","Anomaly detection in videos refers to the identification of events that do
not conform to expected behavior. However, almost all existing methods tackle
the problem by minimizing the reconstruction errors of training data, which
cannot guarantee a larger reconstruction error for an abnormal event. In this
paper, we propose to tackle the anomaly detection problem within a video
prediction framework. To the best of our knowledge, this is the first work that
leverages the difference between a predicted future frame and its ground truth
to detect an abnormal event. To predict a future frame with higher quality for
normal events, other than the commonly used appearance (spatial) constraints on
intensity and gradient, we also introduce a motion (temporal) constraint in
video prediction by enforcing the optical flow between predicted frames and
ground truth frames to be consistent, and this is the first work that
introduces a temporal constraint into the video prediction task. Such spatial
and motion constraints facilitate the future frame prediction for normal
events, and consequently facilitate to identify those abnormal events that do
not conform the expectation. Extensive experiments on both a toy dataset and
some publicly available datasets validate the effectiveness of our method in
terms of robustness to the uncertainty in normal events and the sensitivity to
abnormal events.",http://arxiv.org/pdf/1712.09867v3,cs.CV
2017-12-25 15:29:10+00:00,An Exact and Robust Conformal Inference Method for Counterfactual and Synthetic Controls,"['Victor Chernozhukov', 'Kaspar Wüthrich', 'Yinchu Zhu']","We introduce new inference procedures for counterfactual and synthetic
control methods for policy evaluation. We recast the causal inference problem
as a counterfactual prediction and a structural breaks testing problem. This
allows us to exploit insights from conformal prediction and structural breaks
testing to develop permutation inference procedures that accommodate modern
high-dimensional estimators, are valid under weak and easy-to-verify
conditions, and are provably robust against misspecification. Our methods work
in conjunction with many different approaches for predicting counterfactual
mean outcomes in the absence of the policy intervention. Examples include
synthetic controls, difference-in-differences, factor and matrix completion
models, and (fused) time series panel data models. Our approach demonstrates an
excellent small-sample performance in simulations and is taken to a data
application where we re-evaluate the consequences of decriminalizing indoor
prostitution. Open-source software for implementing our conformal inference
methods is available.",http://arxiv.org/pdf/1712.09089v10,econ.EM
2017-11-08 22:28:09+00:00,Learning Credible Models,"['Jiaxuan Wang', 'Jeeheh Oh', 'Haozhu Wang', 'Jenna Wiens']","In many settings, it is important that a model be capable of providing
reasons for its predictions (i.e., the model must be interpretable). However,
the model's reasoning may not conform with well-established knowledge. In such
cases, while interpretable, the model lacks \textit{credibility}. In this work,
we formally define credibility in the linear setting and focus on techniques
for learning models that are both accurate and credible. In particular, we
propose a regularization penalty, expert yielded estimates (EYE), that
incorporates expert knowledge about well-known relationships among covariates
and the outcome of interest. We give both theoretical and empirical results
comparing our proposed method to several other regularization techniques.
Across a range of settings, experiments on both synthetic and real data show
that models learned using the EYE penalty are significantly more credible than
those learned using other penalties. Applied to a large-scale patient risk
stratification task, our proposed technique results in a model whose top
features overlap significantly with known clinical risk factors, while still
achieving good predictive performance.",http://arxiv.org/pdf/1711.03190v3,cs.LG
2017-10-24 17:10:49+00:00,Conformal predictive distributions with kernels,"['Vladimir Vovk', 'Ilia Nouretdinov', 'Valery Manokhin', 'Alex Gammerman']","This paper reviews the checkered history of predictive distributions in
statistics and discusses two developments, one from recent literature and the
other new. The first development is bringing predictive distributions into
machine learning, whose early development was so deeply influenced by two
remarkable groups at the Institute of Automation and Remote Control. The second
development is combining predictive distributions with kernel methods, which
were originated by one of those groups, including Emmanuel Braverman.",http://arxiv.org/pdf/1710.08894v1,cs.LG
2017-09-19 03:08:21+00:00,Discretized conformal prediction for efficient distribution-free inference,"['Wenyu Chen', 'Kelli-Jean Chun', 'Rina Foygel Barber']","In regression problems where there is no known true underlying model,
conformal prediction methods enable prediction intervals to be constructed
without any assumptions on the distribution of the underlying data, except that
the training and test data are assumed to be exchangeable. However, these
methods bear a heavy computational cost-and, to be carried out exactly, the
regression algorithm would need to be fitted infinitely many times. In
practice, the conformal prediction method is run by simply considering only a
finite grid of finely spaced values for the response variable. This paper
develops discretized conformal prediction algorithms that are guaranteed to
cover the target value with the desired probability, and that offer a tradeoff
between computational cost and prediction accuracy.",http://arxiv.org/pdf/1709.06233v2,stat.ME
2017-08-17 21:18:26+00:00,Learning Actionable Analytics from Multiple Software Projects,"['Rahul Krishna', 'Tim Menzies']","The current generation of software analytics tools are mostly prediction
algorithms (e.g. support vector machines, naive bayes, logistic regression,
etc). While prediction is useful, after prediction comes planning about what
actions to take in order to improve quality. This research seeks methods that
generate demonstrably useful guidance on ""what to do"" within the context of a
specific software project. Specifically, we propose XTREE (for within-project
planning) and BELLTREE (for cross-project planning) to generating plans that
can improve software quality. Each such plan has the property that, if
followed, it reduces the expected number of future defect reports. To find this
expected number, planning was first applied to data from release x. Next, we
looked for changes in release x+1 that conformed to our plans. This procedure
was applied using a range of planners from the literature, as well as XTREE. In
10 open-source JAVA systems, several hundreds of defects were reduced in
sections of the code that conformed to XTREE's plans. Further, when compared to
other planners, XTREE's plans were found to be easier to implement (since they
were shorter) and more effective at reducing the expected number of defects.",http://arxiv.org/pdf/1708.05442v4,cs.SE
2017-08-01 17:29:56+00:00,Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy,['Jing Lei'],"Conformal prediction is a general method that converts almost any point
predictor to a prediction set. The resulting set keeps good statistical
properties of the original estimator under standard assumptions, and guarantees
valid average coverage even when the model is misspecified. A main challenge in
applying conformal prediction in modern applications is efficient computation,
as it generally requires an exhaustive search over the entire output space. In
this paper we develop an exact and computationally efficient conformalization
of the Lasso and elastic net. The method makes use of a novel piecewise linear
homotopy of the Lasso solution under perturbation of a single input sample
point. As a by-product, we provide a simpler and better justified online Lasso
algorithm, which may be of independent interest. Our derivation also reveals an
interesting accuracy-stability trade-off in conformal inference, which is
analogous to the bias-variance trade-off in traditional parameter estimation.
The practical performance of the new algorithm is demonstrated using both
synthetic and real data examples.",http://arxiv.org/pdf/1708.00427v1,stat.ME
2017-07-04 13:31:34+00:00,Automatic estimation of harmonic tension by distributed representation of chords,"['Ali Nikrang', 'David R. W. Sears', 'Gerhard Widmer']","The buildup and release of a sense of tension is one of the most essential
aspects of the process of listening to music. A veridical computational model
of perceived musical tension would be an important ingredient for many music
informatics applications. The present paper presents a new approach to
modelling harmonic tension based on a distributed representation of chords. The
starting hypothesis is that harmonic tension as perceived by human listeners is
related, among other things, to the expectedness of harmonic units (chords) in
their local harmonic context. We train a word2vec-type neural network to learn
a vector space that captures contextual similarity and expectedness, and define
a quantitative measure of harmonic tension on top of this. To assess the
veridicality of the model, we compare its outputs on a number of well-defined
chord classes and cadential contexts to results from pertinent empirical
studies in music psychology. Statistical analysis shows that the model's
predictions conform very well with empirical evidence obtained from human
listeners.",http://arxiv.org/pdf/1707.00972v1,cs.SD
2017-06-25 12:38:55+00:00,Finding optimal finite biological sequences over finite alphabets: the OptiFin toolbox,"['Régis Garnier', 'Christophe Guyeux', 'Stéphane Chrétien']","In this paper, we present a toolbox for a specific optimization problem that
frequently arises in bioinformatics or genomics. In this specific optimisation
problem, the state space is a set of words of specified length over a finite
alphabet. To each word is associated a score. The overall objective is to find
the words which have the lowest possible score. This type of general
optimization problem is encountered in e.g 3D conformation optimisation for
protein structure prediction, or largest core genes subset discovery based on
best supported phylogenetic tree for a set of species. In order to solve this
problem, we propose a toolbox that can be easily launched using MPI and embeds
3 well-known metaheuristics. The toolbox is fully parametrized and well
documented. It has been specifically designed to be easy modified and possibly
improved by the user depending on the application, and does not require to be a
computer scientist. We show that the toolbox performs very well on two
difficult practical problems.",http://arxiv.org/pdf/1706.08089v1,cs.AI
2017-06-11 21:45:24+00:00,Conformal k-NN Anomaly Detector for Univariate Data Streams,"['Vladislav Ishimtsev', 'Ivan Nazarov', 'Alexander Bernstein', 'Evgeny Burnaev']","Anomalies in time-series data give essential and often actionable information
in many applications. In this paper we consider a model-free anomaly detection
method for univariate time-series which adapts to non-stationarity in the data
stream and provides probabilistic abnormality scores based on the conformal
prediction paradigm. Despite its simplicity the method performs on par with
complex prediction-based models on the Numenta Anomaly Detection benchmark and
the Yahoo! S5 dataset.",http://arxiv.org/pdf/1706.03412v1,stat.ML
2017-03-25 14:30:31+00:00,Hidden space reconstruction inspires link prediction in complex networks,"['Hao Liao', 'Mingyang Zhou', 'Zong-Wen Wei', 'Rui Mao', 'Alexandre Vidmer', 'Yi-Cheng Zhang']","As a fundamental challenge in vast disciplines, link prediction aims to
identify potential links in a network based on the incomplete observed
information, which has broad applications ranging from uncovering missing
protein-protein interaction to predicting the evolution of networks. One of the
most influential methods rely on similarity indices characterized by the common
neighbors or its variations. We construct a hidden space mapping a network into
Euclidean space based solely on the connection structures of a network.
Compared with real geographical locations of nodes, our reconstructed locations
are in conformity with those real ones. The distances between nodes in our
hidden space could serve as a novel similarity metric in link prediction. In
addition, we hybrid our hidden space method with other state-of-the-art
similarity methods which substantially outperforms the existing methods on the
prediction accuracy. Hence, our hidden space reconstruction model provides a
fresh perspective to understand the network structure, which in particular
casts a new light on link prediction.",http://arxiv.org/pdf/1705.02199v1,cs.SI
2017-03-15 10:20:32+00:00,Online Learning for Distribution-Free Prediction,"['Dave Zachariah', 'Petre Stoica', 'Thomas B. Schön']","We develop an online learning method for prediction, which is important in
problems with large and/or streaming data sets. We formulate the learning
approach using a covariance-fitting methodology, and show that the resulting
predictor has desirable computational and distribution-free properties: It is
implemented online with a runtime that scales linearly in the number of
samples; has a constant memory requirement; avoids local minima problems; and
prunes away redundant feature dimensions without relying on restrictive
assumptions on the data distribution. In conjunction with the split conformal
approach, it also produces distribution-free prediction confidence intervals in
a computationally efficient manner. The method is demonstrated on both real and
synthetic datasets.",http://arxiv.org/pdf/1703.05060v1,cs.LG
2016-12-30 15:39:45+00:00,Feedback Networks,"['Amir R. Zamir', 'Te-Lin Wu', 'Lin Sun', 'William Shen', 'Jitendra Malik', 'Silvio Savarese']","Currently, the most successful learning models in computer vision are based
on learning successive representations followed by a decision layer. This is
usually actualized through feedforward multilayer neural networks, e.g.
ConvNets, where each layer forms one of such successive representations.
However, an alternative that can achieve the same goal is a feedback based
approach in which the representation is formed in an iterative manner based on
a feedback received from previous iteration's output.
  We establish that a feedback based approach has several fundamental
advantages over feedforward: it enables making early predictions at the query
time, its output naturally conforms to a hierarchical structure in the label
space (e.g. a taxonomy), and it provides a new basis for Curriculum Learning.
We observe that feedback networks develop a considerably different
representation compared to feedforward counterparts, in line with the
aforementioned advantages. We put forth a general feedback based learning
architecture with the endpoint results on par or better than existing
feedforward networks with the addition of the above advantages. We also
investigate several mechanisms in feedback architectures (e.g. skip connections
in time) and design choices (e.g. feedback length). We hope this study offers
new perspectives in quest for more natural and practical learning models.",http://arxiv.org/pdf/1612.09508v3,cs.CV
2016-12-15 08:48:32+00:00,Towards an Approximate Conformance Relation for Hybrid I/O Automata,"['Morteza Mohaqeqi', 'Mohammad Reza Mousavi']","Several notions of conformance have been proposed for checking the behavior
of cyber-physical systems against their hybrid systems models. In this paper,
we explore the initial idea of a notion of approximate conformance that allows
for comparison of both observable discrete actions and (sampled) continuous
trajectories. As such, this notion will consolidate two earlier notions, namely
the notion of Hybrid Input-Output Conformance (HIOCO) by M. van Osch and the
notion of Hybrid Conformance by H. Abbas and G.E. Fainekos. We prove that our
proposed notion of conformance satisfies a semi-transitivity property, which
makes it suitable for a step-wise proof of conformance or refinement.",http://arxiv.org/pdf/1612.04975v1,cs.LO
2016-12-07 10:50:37+00:00,Fast Adaptation in Generative Models with Generative Matching Networks,"['Sergey Bartunov', 'Dmitry P. Vetrov']","Despite recent advances, the remaining bottlenecks in deep generative models
are necessity of extensive training and difficulties with generalization from
small number of training examples. We develop a new generative model called
Generative Matching Network which is inspired by the recently proposed matching
networks for one-shot learning in discriminative tasks. By conditioning on the
additional input dataset, our model can instantly learn new concepts that were
not available in the training data but conform to a similar generative process.
The proposed framework does not explicitly restrict diversity of the
conditioning data and also does not require an extensive inference procedure
for training or adaptation. Our experiments on the Omniglot dataset demonstrate
that Generative Matching Networks significantly improve predictive performance
on the fly as more additional data is available and outperform existing state
of the art conditional generative models.",http://arxiv.org/pdf/1612.02192v2,stat.ML
2016-11-29 03:04:41+00:00,Social Behavior Prediction from First Person Videos,"['Shan Su', 'Jung Pyo Hong', 'Jianbo Shi', 'Hyun Soo Park']","This paper presents a method to predict the future movements (location and
gaze direction) of basketball players as a whole from their first person
videos. The predicted behaviors reflect an individual physical space that
affords to take the next actions while conforming to social behaviors by
engaging to joint attention. Our key innovation is to use the 3D reconstruction
of multiple first person cameras to automatically annotate each other's the
visual semantics of social configurations.
  We leverage two learning signals uniquely embedded in first person videos.
Individually, a first person video records the visual semantics of a spatial
and social layout around a person that allows associating with past similar
situations. Collectively, first person videos follow joint attention that can
link the individuals to a group. We learn the egocentric visual semantics of
group movements using a Siamese neural network to retrieve future trajectories.
We consolidate the retrieved trajectories from all players by maximizing a
measure of social compatibility---the gaze alignment towards joint attention
predicted by their social formation, where the dynamics of joint attention is
learned by a long-term recurrent convolutional network. This allows us to
characterize which social configuration is more plausible and predict future
group trajectories.",http://arxiv.org/pdf/1611.09464v1,cs.CV
2016-11-17 07:39:46+00:00,SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning,"['Long Chen', 'Hanwang Zhang', 'Jun Xiao', 'Liqiang Nie', 'Jian Shao', 'Wei Liu', 'Tat-Seng Chua']","Visual attention has been successfully applied in structural prediction tasks
such as visual captioning and question answering. Existing visual attention
models are generally spatial, i.e., the attention is modeled as spatial
probabilities that re-weight the last conv-layer feature map of a CNN encoding
an input image. However, we argue that such spatial attention does not
necessarily conform to the attention mechanism --- a dynamic feature extractor
that combines contextual fixations over time, as CNN features are naturally
spatial, channel-wise and multi-layer. In this paper, we introduce a novel
convolutional neural network dubbed SCA-CNN that incorporates Spatial and
Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN
dynamically modulates the sentence generation context in multi-layer feature
maps, encoding where (i.e., attentive spatial locations at multiple layers) and
what (i.e., attentive channels) the visual attention is. We evaluate the
proposed SCA-CNN architecture on three benchmark image captioning datasets:
Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN
significantly outperforms state-of-the-art visual attention-based image
captioning methods.",http://arxiv.org/pdf/1611.05594v2,cs.CV
2016-09-19 22:30:36+00:00,Conformalized Kernel Ridge Regression,"['Evgeny Burnaev', 'Ivan Nazarov']","General predictive models do not provide a measure of confidence in
predictions without Bayesian assumptions. A way to circumvent potential
restrictions is to use conformal methods for constructing non-parametric
confidence regions, that offer guarantees regarding validity. In this paper we
provide a detailed description of a computationally efficient conformal
procedure for Kernel Ridge Regression (KRR), and conduct a comparative
numerical study to see how well conformal regions perform against the Bayesian
confidence sets. The results suggest that conformalized KRR can yield
predictive confidence regions with specified coverage rate, which is essential
in constructing anomaly detection systems based on predictive models.",http://arxiv.org/pdf/1609.05959v1,stat.ML
2016-08-18 11:11:48+00:00,Poverty Index With Time Varying Consumption and Income Distributions,"['Amit K Chattopadhyay', 'T Krishna Kumar', 'Sushanta K Mallick']","In a recent work (Chattopadhyay, A. K. et al, Europhys. Lett. {\bf 91},
58003, 2010) based on food consumption statistics, we showed how a stochastic
agent based model could represent the time variation of the income distribution
statistics in a developing economy, thereby defining an alternative
\enquote{poverty index} (PI) that largely agreed with poverty gap index data.
This PI used two variables, the probability density function of the income
statistics and a consumption deprivation (CD) function, representing the
shortfall in the minimum consumption needed for survival. Since the time
dependence of the CD function was introduced there through data extrapolation
only and not through an endogenous time dependent series, this model left
unexplained how the minimum consumption needed for survival varies with time.
The present article overcomes these limitations and arrives at a new unified
theoretical structure through time varying consumption and income distributions
where trade is only allowed when the income exceeds consumption deprivation
(CD). Our results reveal that such CD-dynamics reduces the threshold level of
consumption of basic necessities, suggesting a possible dietary transition in
terms of lower saturation level of food-grain consumption. The new poverty
index conforms to recently observed trends more closely than conventional
measures of poverty and allows probabilistic prediction of PI for future times.",http://arxiv.org/pdf/1608.05650v1,q-fin.GN
2016-04-14 14:46:16+00:00,Distribution-Free Predictive Inference For Regression,"['Jing Lei', ""Max G'Sell"", 'Alessandro Rinaldo', 'Ryan J. Tibshirani', 'Larry Wasserman']","We develop a general framework for distribution-free predictive inference in
regression, using conformal inference. The proposed methodology allows for the
construction of a prediction band for the response variable using any estimator
of the regression function. The resulting prediction band preserves the
consistency properties of the original estimator under standard assumptions,
while guaranteeing finite-sample marginal coverage even when these assumptions
do not hold. We analyze and compare, both empirically and theoretically, the
two major variants of our conformal framework: full conformal inference and
split conformal inference, along with a related jackknife method. These methods
offer different tradeoffs between statistical accuracy (length of resulting
prediction intervals) and computational efficiency. As extensions, we develop a
method for constructing valid in-sample prediction intervals called {\it
rank-one-out} conformal inference, which has essentially the same computational
efficiency as split conformal inference. We also describe an extension of our
procedures for producing prediction bands with locally varying length, in order
to adapt to heteroskedascity in the data. Finally, we propose a model-free
notion of variable importance, called {\it leave-one-covariate-out} or LOCO
inference. Accompanying this paper is an R package {\tt conformalInference}
that implements all of the proposals we have introduced. In the spirit of
reproducibility, all of our empirical results can also be easily (re)generated
using this package.",http://arxiv.org/pdf/1604.04173v2,stat.ME
2016-03-14 23:37:37+00:00,Conformal Predictors for Compound Activity Prediction,"['Paolo Toccacheli', 'Ilia Nouretdinov', 'Alexander Gammerman']","The paper presents an application of Conformal Predictors to a
chemoinformatics problem of identifying activities of chemical compounds. The
paper addresses some specific challenges of this domain: a large number of
compounds (training examples), high-dimensionality of feature space, sparseness
and a strong class imbalance. A variant of conformal predictors called
Inductive Mondrian Conformal Predictor is applied to deal with these
challenges. Results are presented for several non-conformity measures (NCM)
extracted from underlying algorithms and different kernels. A number of
performance measures are used in order to demonstrate the flexibility of
Inductive Mondrian Conformal Predictors in dealing with such a complex set of
data.
  Keywords: Conformal Prediction, Confidence Estimation, Chemoinformatics,
Non-Conformity Measure.",http://arxiv.org/pdf/1603.04506v1,cs.LG
2016-03-14 19:49:07+00:00,Criteria of efficiency for conformal prediction,"['Vladimir Vovk', 'Ilia Nouretdinov', 'Valentina Fedorova', 'Ivan Petej', 'Alex Gammerman']","We study optimal conformity measures for various criteria of efficiency of
classification in an idealised setting. This leads to an important class of
criteria of efficiency that we call probabilistic; it turns out that the most
standard criteria of efficiency used in literature on conformal prediction are
not probabilistic unless the problem of classification is binary. We consider
both unconditional and label-conditional conformal prediction.",http://arxiv.org/pdf/1603.04416v2,cs.LG
2016-03-14 14:43:48+00:00,Universal probability-free prediction,"['Vladimir Vovk', 'Dusko Pavlovic']","We construct universal prediction systems in the spirit of Popper's
falsifiability and Kolmogorov complexity and randomness. These prediction
systems do not depend on any statistical assumptions (but under the IID
assumption they dominate, to within the usual accuracy, conformal prediction).
Our constructions give rise to a theory of algorithmic complexity and
randomness of time containing analogues of several notions and results of the
classical theory of Kolmogorov complexity and randomness.",http://arxiv.org/pdf/1603.04283v2,cs.LG
2016-03-07 20:47:07+00:00,Guided macro-mutation in a graded energy based genetic algorithm for protein structure prediction,"['Mahmood A. Rashid', 'Sumaiya Iqbal', 'Firas Khatib', 'Md Tamjidul Hoque', 'Abdul Sattar']","Protein structure prediction is considered as one of the most challenging and
computationally intractable combinatorial problem. Thus, the efficient modeling
of convoluted search space, the clever use of energy functions, and more
importantly, the use of effective sampling algorithms become crucial to address
this problem. For protein structure modeling, an off-lattice model provides
limited scopes to exercise and evaluate the algorithmic developments due to its
astronomically large set of data-points. In contrast, an on-lattice model
widens the scopes and permits studying the relatively larger proteins because
of its finite set of data-points. In this work, we took the full advantage of
an on-lattice model by using a face-centered-cube lattice that has the highest
packing density with the maximum degree of freedom. We proposed a graded
energy-strategically mixes the Miyazawa-Jernigan (MJ) energy with the
hydrophobic-polar (HP) energy-based genetic algorithm (GA) for conformational
search. In our application, we introduced a 2x2 HP energy guided macro-mutation
operator within the GA to explore the best possible local changes exhaustively.
Conversely, the 20x20 MJ energy model-the ultimate objective function of our GA
that needs to be minimized-considers the impacts amongst the 20 different amino
acids and allow searching the globally acceptable conformations. On a set of
benchmark proteins, our proposed approach outperformed state-of-the-art
approaches in terms of the free energy levels and the root-mean-square
deviations.",http://arxiv.org/pdf/1607.06113v1,cs.NE
2015-11-17 05:16:07+00:00,Towards composition of conformant systems,"['Houssam Abbas', 'Georgios Fainekos']","Motivated by the Model-Based Design process for Cyber-Physical Systems, we
consider issues in conformance testing of systems. Conformance is a
quantitative notion of similarity between the output trajectories of systems,
which considers both temporal and spatial aspects of the outputs. Previous work
developed algorithms for computing the conformance degree between two systems,
and demonstrated how formal verification results for one system can be re-used
for a system that is conformant to it. In this paper, we study the relation
between conformance and a generalized approximate simulation relation for the
class of Open Metric Transition Systems (OMTS). This allows us to prove a
small-gain theorem for OMTS, which gives sufficient conditions under which the
feedback interconnection of systems respects the conformance relation, thus
allowing the building of more complex systems from conformant components.",http://arxiv.org/pdf/1511.05273v2,cs.SY
2015-10-27 16:20:28+00:00,Blitzkriging: Kronecker-structured Stochastic Gaussian Processes,"['Thomas Nickson', 'Tom Gunter', 'Chris Lloyd', 'Michael A Osborne', 'Stephen Roberts']","We present Blitzkriging, a new approach to fast inference for Gaussian
processes, applicable to regression, optimisation and classification.
State-of-the-art (stochastic) inference for Gaussian processes on very large
datasets scales cubically in the number of 'inducing inputs', variables
introduced to factorise the model. Blitzkriging shares state-of-the-art scaling
with data, but reduces the scaling in the number of inducing points to
approximately linear. Further, in contrast to other methods, Blitzkriging: does
not force the data to conform to any particular structure (including
grid-like); reduces reliance on error-prone optimisation of inducing point
locations; and is able to learn rich (covariance) structure from the data. We
demonstrate the benefits of our approach on real data in regression,
time-series prediction and signal-interpolation experiments.",http://arxiv.org/pdf/1510.07965v2,stat.ML
2015-08-11 02:06:01+00:00,Support for Non-conformal Meshes in PETSc's DMPlex Interface,"['Tobin Isaac', 'Matthew G. Knepley']","PETSc's DMPlex interface for unstructured meshes has been extended to support
non-conformal meshes. The topological construct that DMPlex implements---the
CW-complex---is by definition conformal, so representing non- conformal meshes
in a way that hides complexity requires careful attention to the interface
between DMPlex and numerical methods such as the finite element method. Our
approach---which combines a tree structure for subset- superset relationships
and a ""reference tree"" describing the types of non-conformal
interfaces---allows finite element code written for conformal meshes to extend
automatically: in particular, all ""hanging-node"" constraint calculations are
handled behind the scenes. We give example code demonstrating the use of this
extension, and use it to convert forests of quadtrees and forests of octrees
from the p4est library to DMPlex meshes.",http://arxiv.org/pdf/1508.02470v1,cs.MS
2015-07-30 09:34:30+00:00,Multilinear Map Layer: Prediction Regularization by Structural Constraint,"['Shuchang Zhou', 'Yuxin Wu']","In this paper we propose and study a technique to impose structural
constraints on the output of a neural network, which can reduce amount of
computation and number of parameters besides improving prediction accuracy when
the output is known to approximately conform to the low-rankness prior. The
technique proceeds by replacing the output layer of neural network with the
so-called MLM layers, which forces the output to be the result of some
Multilinear Map, like a hybrid-Kronecker-dot product or Kronecker Tensor
Product. In particular, given an ""autoencoder"" model trained on SVHN dataset,
we can construct a new model with MLM layer achieving 62\% reduction in total
number of parameters and reduction of $\ell_2$ reconstruction error from 0.088
to 0.004. Further experiments on other autoencoder model variants trained on
SVHN datasets also demonstrate the efficacy of MLM layers.",http://arxiv.org/pdf/1507.08429v1,cs.CV
2015-06-10 21:49:31+00:00,Bayesian Poisson Tensor Factorization for Inferring Multilateral Relations from Sparse Dyadic Event Counts,"['Aaron Schein', 'John Paisley', 'David M. Blei', 'Hanna Wallach']","We present a Bayesian tensor factorization model for inferring latent group
structures from dynamic pairwise interaction patterns. For decades, political
scientists have collected and analyzed records of the form ""country $i$ took
action $a$ toward country $j$ at time $t$""---known as dyadic events---in order
to form and test theories of international relations. We represent these event
data as a tensor of counts and develop Bayesian Poisson tensor factorization to
infer a low-dimensional, interpretable representation of their salient
patterns. We demonstrate that our model's predictive performance is better than
that of standard non-negative tensor factorization methods. We also provide a
comparison of our variational updates to their maximum likelihood counterparts.
In doing so, we identify a better way to form point estimates of the latent
factors than that typically used in Bayesian Poisson matrix factorization.
Finally, we showcase our model as an exploratory analysis tool for political
scientists. We show that the inferred latent factor matrices capture
interpretable multilateral relations that both conform to and inform our
knowledge of international affairs.",http://arxiv.org/pdf/1506.03493v1,stat.ML
2015-05-21 18:42:33+00:00,Quantifying Conformance using the Skorokhod Metric (full version),"['Jyotirmoy V. Deshmukh', 'Rupak Majumdar', 'Vinayak S. Prabhu']","The conformance testing problem for dynamical systems asks, given two
dynamical models (e.g., as Simulink diagrams), whether their behaviors are
""close"" to each other. In the semi-formal approach to conformance testing, the
two systems are simulated on a large set of tests, and a metric, defined on
pairs of real-valued, real-timed trajectories, is used to determine a lower
bound on the distance. We show how the Skorkhod metric on continuous dynamical
systems can be used as the foundation for conformance testing of complex
dynamical models. The Skorokhod metric allows for both state value mismatches
and timing distortions, and is thus well suited for checking conformance
between idealized models of dynamical systems and their implementations. We
demonstrate the robustness of the system conformance quantification by proving
a \emph{transference theorem}: trajectories close under the Skorokhod metric
satisfy ""close"" logical properties. Specifically, we show the result for the
timed linear time logic \TLTL augmented with a rich class of temporal and
spatial constraint predicates. We provide a window-based streaming algorithm to
compute the Skorokhod metric, and use it as a basis for a conformance testing
tool for Simulink. We experimentally demonstrate the effectiveness of our tool
in finding discrepant behaviors on a set of control system benchmarks,
including an industrial challenge problem.",http://arxiv.org/pdf/1505.05832v1,cs.SY
2015-04-28 19:53:59+00:00,"Or's of And's for Interpretable Classification, with Application to Context-Aware Recommender Systems","['Tong Wang', 'Cynthia Rudin', 'Finale Doshi-Velez', 'Yimin Liu', 'Erica Klampfl', 'Perry MacNeille']","We present a machine learning algorithm for building classifiers that are
comprised of a small number of disjunctions of conjunctions (or's of and's). An
example of a classifier of this form is as follows: If X satisfies (x1 = 'blue'
AND x3 = 'middle') OR (x1 = 'blue' AND x2 = '<15') OR (x1 = 'yellow'), then we
predict that Y=1, ELSE predict Y=0. An attribute-value pair is called a literal
and a conjunction of literals is called a pattern. Models of this form have the
advantage of being interpretable to human experts, since they produce a set of
conditions that concisely describe a specific class. We present two
probabilistic models for forming a pattern set, one with a Beta-Binomial prior,
and the other with Poisson priors. In both cases, there are prior parameters
that the user can set to encourage the model to have a desired size and shape,
to conform with a domain-specific definition of interpretability. We provide
two scalable MAP inference approaches: a pattern level search, which involves
association rule mining, and a literal level search. We show stronger priors
reduce computation. We apply the Bayesian Or's of And's (BOA) model to predict
user behavior with respect to in-vehicle context-aware personalized recommender
systems.",http://arxiv.org/pdf/1504.07614v1,cs.LG
2015-04-09 19:25:33+00:00,Predicting Complete 3D Models of Indoor Scenes,"['Ruiqi Guo', 'Chuhang Zou', 'Derek Hoiem']","One major goal of vision is to infer physical models of objects, surfaces,
and their layout from sensors. In this paper, we aim to interpret indoor scenes
from one RGBD image. Our representation encodes the layout of walls, which must
conform to a Manhattan structure but is otherwise flexible, and the layout and
extent of objects, modeled with CAD-like 3D shapes. We represent both the
visible and occluded portions of the scene, producing a complete 3D parse. Such
a scene interpretation is useful for robotics and visual reasoning, but
difficult to produce due to the well-known challenge of segmentation, the high
degree of occlusion, and the diversity of objects in indoor scene. We take a
data-driven approach, generating sets of potential object regions, matching to
regions in training images, and transferring and aligning associated 3D models
while encouraging fit to observations and overall consistency. We demonstrate
encouraging results on the NYU v2 dataset and highlight a variety of
interesting directions for future work.",http://arxiv.org/pdf/1504.02437v3,cs.CV
2015-04-01 03:27:02+00:00,Conformal Surface Morphing with Applications on Facial Expressions,"['Mei-Heng Yueh', 'Xianfeng David Gu', 'Wen-Wei Lin', 'Chin-Tien Wu', 'Shing-Tung Yau']","Morphing is the process of changing one figure into another. Some numerical
methods of 3D surface morphing by deformable modeling and conformal mapping are
shown in this study. It is well known that there exists a unique Riemann
conformal mapping from a simply connected surface into a unit disk by the
Riemann mapping theorem. The dilation and relative orientations of the 3D
surfaces can be linked through the M\""obius transformation due to the conformal
characteristic of the Riemann mapping. On the other hand, a 3D surface
deformable model can be built via various approaches such as mutual
parameterization from direct interpolation or surface matching using landmarks.
In this paper, we take the advantage of the unique representation of 3D
surfaces by the mean curvatures and the conformal factors associated with the
Riemann mapping. By registering the landmarks on the conformal parametric
domains, the correspondence of the mean curvatures and the conformal factors
for each surfaces can be obtained. As a result, we can construct the 3D
deformation field from the surface reconstruction algorithm proposed by Gu and
Yau. Furthermore, by composition of the M\""obius transformation and the 3D
deformation field, the morphing sequence can be generated from the mean
curvatures and the conformal factors on a unified mesh structure by using the
cubic spline homotopy. Several numerical experiments of the face morphing are
presented to demonstrate the robustness of our approach.",http://arxiv.org/pdf/1504.00097v1,cs.GR
2015-03-17 09:09:30+00:00,Conformance Checking Based on Multi-Perspective Declarative Process Models,"['Andrea Burattin', 'Fabrizio Maria Maggi', 'Alessandro Sperduti']","Process mining is a family of techniques that aim at analyzing business
process execution data recorded in event logs. Conformance checking is a branch
of this discipline embracing approaches for verifying whether the behavior of a
process, as recorded in a log, is in line with some expected behaviors provided
in the form of a process model. The majority of these approaches require the
input process model to be procedural (e.g., a Petri net). However, in turbulent
environments, characterized by high variability, the process behavior is less
stable and predictable. In these environments, procedural process models are
less suitable to describe a business process. Declarative specifications,
working in an open world assumption, allow the modeler to express several
possible execution paths as a compact set of constraints. Any process execution
that does not contradict these constraints is allowed. One of the open
challenges in the context of conformance checking with declarative models is
the capability of supporting multi-perspective specifications. In this paper,
we close this gap by providing a framework for conformance checking based on
MP-Declare, a multi-perspective version of the declarative process modeling
language Declare. The approach has been implemented in the process mining tool
ProM and has been experimented in three real life case studies.",http://arxiv.org/pdf/1503.04957v1,cs.SE
2014-12-22 07:38:26+00:00,Discrete Conformal Deformation: Algorithm and Experiments,"['Jian Sun', 'Tianqi Wu', 'Xianfeng Gu', 'Feng Luo']","In this paper, we introduce a definition of discrete conformality for
triangulated surfaces with flat cone metrics and describe an algorithm for
solving the problem of prescribing curvature, that is to deform the metric
discrete conformally so that the curvature of the resulting metric coincides
with the prescribed curvature. We explicitly construct a discrete conformal map
between the input triangulated surface and the deformed triangulated surface.
Our algorithm can handle the surface with any topology with or without
boundary, and can find a deformed metric for any prescribed curvature
satisfying the Gauss-Bonnet formula. In addition, we present the numerical
examples to show the convergence of our discrete conformality and to
demonstrate the efficiency and the robustness of our algorithm.",http://arxiv.org/pdf/1412.6892v1,cs.CG
2014-09-13 18:32:56+00:00,"Rawls' Fairness, Income Distribution and Alarming Level of Gini Coefficient","['Yong Tao', 'Xiangjun Wu', 'Changshuai Li']","The argument that the alarming level of Gini coefficient is 0.4 is very
popular, especially in the media industry, all around the world for a long
time. Although the 0.4 standard is widely accepted, the derivation of the value
lacks rigid theoretical foundations. In fact, to the best of our knowledge, it
is not based on any prevalent and convincing economic theories. In this paper,
we incorporate Rawls' principle of fair equality of opportunity into
Arrow-Debreu's framework of general equilibrium theory with heterogeneous
agents, and derive the alarming level of Gini coefficient formally. Our theory
reveals that the exponential distribution of income not only satisfies Pareto
optimality, but also obeys social fairness in Rawls' sense. Therefore, we
specify the maximal value of the Gini coefficient when income follows
exponential distribution as a possible alarming level. Our computations show
that the alarming level should be specified at least equal or larger than 0.5
rather than 0.4. We empirically investigate if our model receives support from
a large data set of all kinds of countries all over the world from Word Bank in
1990, 1995, 2000 and 2005 using the distribution fitting and statistical
decision methodology. The results suggest that the value of 0.4 is around the
mean of the Gini coefficients, corresponding to the most probable event in a
peaceful world, rather than the alarming level, while the two-sigma rule shows
that in our sample the alarming levels are all larger than 0.5, conforming to
the predictions of our theory.",http://arxiv.org/pdf/1409.3979v1,q-fin.GN
2014-08-29 10:31:56+00:00,Fast Disk Conformal Parameterization of Simply-connected Open Surfaces,"['Pui Tung Choi', 'Lok Ming Lui']","Surface parameterizations have been widely used in computer graphics and
geometry processing. In particular, as simply-connected open surfaces are
conformally equivalent to the unit disk, it is desirable to compute the disk
conformal parameterizations of the surfaces. In this paper, we propose a novel
algorithm for the conformal parameterization of a simply-connected open surface
onto the unit disk, which significantly speeds up the computation, enhances the
conformality and stability, and guarantees the bijectivity. The conformality
distortions at the inner region and on the boundary are corrected by two steps,
with the aid of an iterative scheme using quasi-conformal theories.
Experimental results demonstrate the effectiveness of our proposed method.",http://arxiv.org/pdf/1408.6974v1,cs.CG
2014-07-23 01:19:47+00:00,Stabilized Sparse Ordinal Regression for Medical Risk Stratification,"['Truyen Tran', 'Dinh Phung', 'Wei Luo', 'Svetha Venkatesh']","The recent wide adoption of Electronic Medical Records (EMR) presents great
opportunities and challenges for data mining. The EMR data is largely temporal,
often noisy, irregular and high dimensional. This paper constructs a novel
ordinal regression framework for predicting medical risk stratification from
EMR. First, a conceptual view of EMR as a temporal image is constructed to
extract a diverse set of features. Second, ordinal modeling is applied for
predicting cumulative or progressive risk. The challenges are building a
transparent predictive model that works with a large number of weakly
predictive features, and at the same time, is stable against resampling
variations. Our solution employs sparsity methods that are stabilized through
domain-specific feature interaction networks. We introduces two indices that
measure the model stability against data resampling. Feature networks are used
to generate two multivariate Gaussian priors with sparse precision matrices
(the Laplacian and Random Walk). We apply the framework on a large short-term
suicide risk prediction problem and demonstrate that our methods outperform
clinicians to a large-margin, discover suicide risk factors that conform with
mental health knowledge, and produce models with enhanced stability.",http://arxiv.org/pdf/1407.6084v1,stat.AP
2014-06-21 11:47:21+00:00,From conformal to probabilistic prediction,"['Vladimir Vovk', 'Ivan Petej', 'Valentina Fedorova']","This paper proposes a new method of probabilistic prediction, which is based
on conformal prediction. The method is applied to the standard USPS data set
and gives encouraging results.",http://arxiv.org/pdf/1406.5600v1,cs.LG
2014-04-08 10:49:08+00:00,Efficiency of conformalized ridge regression,"['Evgeny Burnaev', 'Vladimir Vovk']","Conformal prediction is a method of producing prediction sets that can be
applied on top of a wide range of prediction algorithms. The method has a
guaranteed coverage probability under the standard IID assumption regardless of
whether the assumptions (often considerably more restrictive) of the underlying
algorithm are satisfied. However, for the method to be really useful it is
desirable that in the case where the assumptions of the underlying algorithm
are satisfied, the conformal predictor loses little in efficiency as compared
with the underlying algorithm (whereas being a conformal predictor, it has the
stronger guarantee of validity). In this paper we explore the degree to which
this additional requirement of efficiency is satisfied in the case of Bayesian
ridge regression; we find that asymptotically conformal prediction sets differ
little from ridge regression prediction intervals when the standard Bayesian
assumptions are satisfied.",http://arxiv.org/pdf/1404.2083v1,cs.LG
2014-03-26 10:21:03+00:00,QCMC: Quasi-conformal Parameterizations for Multiply-connected domains,"['Kin Tat Ho', 'Lok Ming Lui']","This paper presents a method to compute the {\it quasi-conformal
parameterization} (QCMC) for a multiply-connected 2D domain or surface. QCMC
computes a quasi-conformal map from a multiply-connected domain $S$ onto a
punctured disk $D_S$ associated with a given Beltrami differential. The
Beltrami differential, which measures the conformality distortion, is a
complex-valued function $\mu:S\to\mathbb{C}$ with supremum norm strictly less
than 1. Every Beltrami differential gives a conformal structure of $S$. Hence,
the conformal module of $D_S$, which are the radii and centers of the inner
circles, can be fully determined by $\mu$, up to a M\""obius transformation. In
this paper, we propose an iterative algorithm to simultaneously search for the
conformal module and the optimal quasi-conformal parameterization. The key idea
is to minimize the Beltrami energy subject to the boundary constraints. The
optimal solution is our desired quasi-conformal parameterization onto a
punctured disk. The parameterization of the multiply-connected domain
simplifies numerical computations and has important applications in various
fields, such as in computer graphics and vision. Experiments have been carried
out on synthetic data together with real multiply-connected Riemann surfaces.
Results show that our proposed method can efficiently compute quasi-conformal
parameterizations of multiply-connected domains and outperforms other
state-of-the-art algorithms. Applications of the proposed parameterization
technique have also been explored.",http://arxiv.org/pdf/1403.6614v1,cs.CG
2014-01-21 07:17:47+00:00,Conformance Testing as Falsification for Cyber-Physical Systems,"['Houssam Abbas', 'Bardh Hoxha', 'Georgios Fainekos', 'Jyotirmoy V. Deshmukh', 'James Kapinski', 'Koichi Ueda']","In Model-Based Design of Cyber-Physical Systems (CPS), it is often desirable
to develop several models of varying fidelity. Models of different fidelity
levels can enable mathematical analysis of the model, control synthesis, faster
simulation etc. Furthermore, when (automatically or manually) transitioning
from a model to its implementation on an actual computational platform, then
again two different versions of the same system are being developed. In all
previous cases, it is necessary to define a rigorous notion of conformance
between different models and between models and their implementations. This
paper argues that conformance should be a measure of distance between systems.
Albeit a range of theoretical distance notions exists, a way to compute such
distances for industrial size systems and models has not been proposed yet.
This paper addresses exactly this problem. A universal notion of conformance as
closeness between systems is rigorously defined, and evidence is presented that
this implies a number of other application-dependent conformance notions. An
algorithm for detecting that two systems are not conformant is then proposed,
which uses existing proven tools. A method is also proposed to measure the
degree of conformance between two systems. The results are demonstrated on a
range of models.",http://arxiv.org/pdf/1401.5200v2,cs.SY
2014-01-16 05:12:21+00:00,Regression Conformal Prediction with Nearest Neighbours,"['Harris Papadopoulos', 'Vladimir Vovk', 'Alex Gammerman']","In this paper we apply Conformal Prediction (CP) to the k-Nearest Neighbours
Regression (k-NNR) algorithm and propose ways of extending the typical
nonconformity measure used for regression so far. Unlike traditional regression
methods which produce point predictions, Conformal Predictors output predictive
regions that satisfy a given confidence level. The regions produced by any
Conformal Predictor are automatically valid, however their tightness and
therefore usefulness depends on the nonconformity measure used by each CP. In
effect a nonconformity measure evaluates how strange a given example is
compared to a set of other examples based on some traditional machine learning
algorithm. We define six novel nonconformity measures based on the k-Nearest
Neighbours Regression algorithm and develop the corresponding CPs following
both the original (transductive) and the inductive CP approaches. A comparison
of the predictive regions produced by our measures with those of the typical
regression measure suggests that a major improvement in terms of predictive
region tightness is achieved by the new measures.",http://arxiv.org/pdf/1401.3880v1,cs.LG
2014-01-15 05:27:00+00:00,Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width,"['Hector Palacios', 'Hector Geffner']","Conformant planning is the problem of finding a sequence of actions for
achieving a goal in the presence of uncertainty in the initial state or action
effects. The problem has been approached as a path-finding problem in belief
space where good belief representations and heuristics are critical for scaling
up. In this work, a different formulation is introduced for conformant problems
with deterministic actions where they are automatically converted into
classical ones and solved by an off-the-shelf classical planner. The
translation maps literals L and sets of assumptions t about the initial
situation, into new literals KL/t that represent that L must be true if t is
initially true. We lay out a general translation scheme that is sound and
establish the conditions under which the translation is also complete. We show
that the complexity of the complete translation is exponential in a parameter
of the problem called the conformant width, which for most benchmarks is
bounded. The planner based on this translation exhibits good performance in
comparison with existing planners, and is the basis for T0, the best performing
planner in the Conformant Track of the 2006 International Planning Competition.",http://arxiv.org/pdf/1401.3468v1,cs.AI
2013-11-14 00:37:22+00:00,SUNNY: a Lazy Portfolio Approach for Constraint Solving,"['Roberto Amadini', 'Maurizio Gabbrielli', 'Jacopo Mauro']","*** To appear in Theory and Practice of Logic Programming (TPLP) ***
  Within the context of constraint solving, a portfolio approach allows one to
exploit the synergy between different solvers in order to create a globally
better solver. In this paper we present SUNNY: a simple and flexible algorithm
that takes advantage of a portfolio of constraint solvers in order to compute
--- without learning an explicit model --- a schedule of them for solving a
given Constraint Satisfaction Problem (CSP). Motivated by the performance
reached by SUNNY vs. different simulations of other state of the art
approaches, we developed sunny-csp, an effective portfolio solver that exploits
the underlying SUNNY algorithm in order to solve a given CSP. Empirical tests
conducted on exhaustive benchmarks of MiniZinc models show that the actual
performance of SUNNY conforms to the predictions. This is encouraging both for
improving the power of CSP portfolio solvers and for trying to export them to
fields such as Answer Set Programming and Constraint Logic Programming.",http://arxiv.org/pdf/1311.3353v2,cs.AI
2013-10-31 16:41:44+00:00,A Hybrid Local Search for Simplified Protein Structure Prediction,"['Swakkhar Shatabda', 'M. A. Hakim Newton', 'Duc Nghia Pham', 'Abdul Sattar']","Protein structure prediction based on Hydrophobic-Polar energy model
essentially becomes searching for a conformation having a compact hydrophobic
core at the center. The hydrophobic core minimizes the interaction energy
between the amino acids of the given protein. Local search algorithms can
quickly find very good conformations by moving repeatedly from the current
solution to its ""best"" neighbor. However, once such a compact hydrophobic core
is found, the search stagnates and spends enormous effort in quest of an
alternative core. In this paper, we attempt to restructure segments of a
conformation with such compact core. We select one large segment or a number of
small segments and apply exhaustive local search. We also apply a mix of
heuristics so that one heuristic can help escape local minima of another. We
evaluated our algorithm by using Face Centered Cubic (FCC) Lattice on a set of
standard benchmark proteins and obtain significantly better results than that
of the state-of-the-art methods.",http://arxiv.org/pdf/1310.8583v1,cs.CE
2013-09-30 01:06:30+00:00,Self Organizing Maps to efficiently cluster and functionally interpret protein conformational ensembles,"['Domenico Fraccalvieri', 'Laura Bonati', 'Fabio Stella']","An approach that combines Self-Organizing maps, hierarchical clustering and
network components is presented, aimed at comparing protein conformational
ensembles obtained from multiple Molecular Dynamic simulations. As a first
result the original ensembles can be summarized by using only the
representative conformations of the clusters obtained. In addition the network
components analysis allows to discover and interpret the dynamic behavior of
the conformations won by each neuron. The results showed the ability of this
approach to efficiently derive a functional interpretation of the protein
dynamics described by the original conformational ensemble, highlighting its
potential as a support for protein engineering.",http://arxiv.org/pdf/1309.7694v1,cs.CE
2013-07-06 10:17:44+00:00,Ensemble Methods for Multi-label Classification,"['Lior Rokach', 'Alon Schclar', 'Ehud Itach']","Ensemble methods have been shown to be an effective tool for solving
multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm,
each member of the ensemble is associated with a small randomly-selected subset
of k labels. Then, a single label classifier is trained according to each
combination of elements in the subset. In this paper we adopt a similar
approach, however, instead of randomly choosing subsets, we select the minimum
required subsets of k labels that cover all labels and meet additional
constraints such as coverage of inter-label correlations. Construction of the
cover is achieved by formulating the subset selection as a minimum set covering
problem (SCP) and solving it by using approximation algorithms. Every cover
needs only to be prepared once by offline algorithms. Once prepared, a cover
may be applied to the classification of any given multi-label dataset whose
properties conform with those of the cover. The contribution of this paper is
two-fold. First, we introduce SCP as a general framework for constructing label
covers while allowing the user to incorporate cover construction constraints.
We demonstrate the effectiveness of this framework by proposing two
construction constraints whose enforcement produces covers that improve the
prediction performance of random selection. Second, we provide theoretical
bounds that quantify the probabilities of random selection to produce covers
that meet the proposed construction criteria. The experimental results indicate
that the proposed methods improve multi-label classification accuracy and
stability compared with the RAKEL algorithm and to other state-of-the-art
algorithms.",http://arxiv.org/pdf/1307.1769v1,stat.ML
2013-06-11 02:01:17+00:00,An Aggregation-Based Overall Quality Measurement for Visualization,['Weidong Huang'],"Aesthetics are often used to evaluate the quality of graph drawings. However,
the existing aesthetic criteria are useful in judging the extents to which a
drawing conforms to particular drawing rules. They have limitations in
evaluating overall quality. Currently the overall quality of graph drawings is
mainly evaluated based on personal judgments and user studies. Personal
judgments are not reliable, while user studies can be costly to run. Therefore,
there is a need for a direct measure of overall quality. This measure can be
used by visualization designers to quickly compare the quality of drawings at
hand at the design stage and make decisions accordingly. In an attempt to meet
this need, we propose a measure that measures overall quality based on
aggregation of individual aesthetic criteria. We present a user study that
validates this measure and demonstrates its capacity in predicting the
performance of human graph comprehension. The implications of the proposed
measure for future research are discussed.",http://arxiv.org/pdf/1306.2404v1,cs.HC
2013-03-05 12:18:31+00:00,Decomposability in Input Output Conformance Testing,"['Neda Noroozi', 'Mohammad Reza Mousavi', 'Tim A. C. Willemse']","We study the problem of deriving a specification for a third-party component,
based on the specification of the system and the environment in which the
component is supposed to reside. Particularly, we are interested in using
component specifications for conformance testing of black-box components, using
the theory of input-output conformance (ioco) testing. We propose and prove
sufficient criteria for decompositionality, i.e., that components conforming to
the derived specification will always compose to produce a correct system with
respect to the system specification. We also study the criteria for strong
decomposability, by which we can ensure that only those components conforming
to the derived specification can lead to a correct system.",http://arxiv.org/pdf/1303.1009v1,cs.SE
2013-02-26 15:16:32+00:00,A Conformal Prediction Approach to Explore Functional Data,"['Jing Lei', 'Alessandro Rinaldo', 'Larry Wasserman']","This paper applies conformal prediction techniques to compute simultaneous
prediction bands and clustering trees for functional data. These tools can be
used to detect outliers and clusters. Both our prediction bands and clustering
trees provide prediction sets for the underlying stochastic process with a
guaranteed finite sample behavior, under no distributional assumptions. The
prediction sets are also informative in that they correspond to the high
density region of the underlying process. While ordinary conformal prediction
has high computational cost for functional data, we use the inductive conformal
predictor, together with several novel choices of conformity scores, to
simplify the computation. Our methods are illustrated on some real data
examples.",http://arxiv.org/pdf/1302.6452v1,stat.ML
2013-02-20 23:15:57+00:00,Prediction and Clustering in Signed Networks: A Local to Global Perspective,"['Kai-Yang Chiang', 'Cho-Jui Hsieh', 'Nagarajan Natarajan', 'Ambuj Tewari', 'Inderjit S. Dhillon']","The study of social networks is a burgeoning research area. However, most
existing work deals with networks that simply encode whether relationships
exist or not. In contrast, relationships in signed networks can be positive
(""like"", ""trust"") or negative (""dislike"", ""distrust""). The theory of social
balance shows that signed networks tend to conform to some local patterns that,
in turn, induce certain global characteristics. In this paper, we exploit both
local as well as global aspects of social balance theory for two fundamental
problems in the analysis of signed networks: sign prediction and clustering.
Motivated by local patterns of social balance, we first propose two families of
sign prediction methods: measures of social imbalance (MOIs), and supervised
learning using high order cycles (HOCs). These methods predict signs of edges
based on triangles and \ell-cycles for relatively small values of \ell.
Interestingly, by examining measures of social imbalance, we show that the
classic Katz measure, which is used widely in unsigned link prediction,
actually has a balance theoretic interpretation when applied to signed
networks. Furthermore, motivated by the global structure of balanced networks,
we propose an effective low rank modeling approach for both sign prediction and
clustering. For the low rank modeling approach, we provide theoretical
performance guarantees via convex relaxations, scale it up to large problem
sizes using a matrix factorization based algorithm, and provide extensive
experimental validation including comparisons with local approaches. Our
experimental results indicate that, by adopting a more global viewpoint of
balance structure, we get significant performance and computational gains in
prediction and clustering tasks on signed networks. Our work therefore
highlights the usefulness of the global aspect of balance theory for the
analysis of signed networks.",http://arxiv.org/pdf/1302.5145v2,cs.SI
2012-09-12 17:39:37+00:00,Conditional validity of inductive conformal predictors,['Vladimir Vovk'],"Conformal predictors are set predictors that are automatically valid in the
sense of having coverage probability equal to or exceeding a given confidence
level. Inductive conformal predictors are a computationally efficient version
of conformal predictors satisfying the same property of validity. However,
inductive conformal predictors have been only known to control unconditional
coverage probability. This paper explores various versions of conditional
validity and various ways to achieve them using inductive conformal predictors
and their modifications.",http://arxiv.org/pdf/1209.2673v2,cs.LG
2012-08-03 18:01:52+00:00,Cross-conformal predictors,['Vladimir Vovk'],"This note introduces the method of cross-conformal prediction, which is a
hybrid of the methods of inductive conformal prediction and cross-validation,
and studies its validity and predictive efficiency empirically.",http://arxiv.org/pdf/1208.0806v1,stat.ML
2012-05-09 06:35:04+00:00,Non-Parametric Methods Applied to the N-Sample Series Comparison,"[""Paolo D'Alberto"", 'Chris Drome', 'Ali Dasdan']","Anomaly and similarity detection in multidimensional series have a long
history and have found practical usage in many different fields such as
medicine, networks, and finance. Anomaly detection is of great appeal for many
different disciplines; for example, mathematicians searching for a unified
mathematical formulation based on probability, statisticians searching for
error bound estimates, and computer scientists who are trying to design fast
algorithms, to name just a few. In summary, we have two contributions: First,
we present a self-contained survey of the most promising methods being used in
the fields of machine learning, statistics, and bio-informatics today. Included
we present discussions about conformal prediction, kernels in the Hilbert
space, Kolmogorov's information measure, and non-parametric cumulative
distribution function comparison methods (NCDF). Second, building upon this
foundation, we provide a powerful NCDF method for series with small
dimensionality. Through a combination of data organization and statistical
tests, we describe extensions that scale well with increased dimensionality.",http://arxiv.org/pdf/1205.1880v1,stat.CO
2012-03-24 15:04:02+00:00,Distribution Free Prediction Bands,"['Jing Lei', 'Larry Wasserman']","We study distribution free, nonparametric prediction bands with a special
focus on their finite sample behavior. First we investigate and develop
different notions of finite sample coverage guarantees. Then we give a new
prediction band estimator by combining the idea of ""conformal prediction"" (Vovk
et al. 2009) with nonparametric conditional density estimation. The proposed
estimator, called COPS (Conformal Optimized Prediction Set), always has finite
sample guarantee in a stronger sense than the original conformal prediction
estimator. Under regularity conditions the estimator converges to an oracle
band at a minimax optimal rate. A fast approximation algorithm and a data
driven method for selecting the bandwidth are developed. The method is
illustrated first in simulated data. Then, an application shows that the
proposed method gives desirable prediction intervals in an automatic way, as
compared to the classical linear regression modeling.",http://arxiv.org/pdf/1203.5422v1,stat.ME
2012-02-28 05:33:46+00:00,Towards Symbolic Model-Based Mutation Testing: Combining Reachability and Refinement Checking,"['Bernhard K. Aichernig', 'Elisabeth Jöbstl']","Model-based mutation testing uses altered test models to derive test cases
that are able to reveal whether a modelled fault has been implemented. This
requires conformance checking between the original and the mutated model. This
paper presents an approach for symbolic conformance checking of action systems,
which are well-suited to specify reactive systems. We also consider
nondeterminism in our models. Hence, we do not check for equivalence, but for
refinement. We encode the transition relation as well as the conformance
relation as a constraint satisfaction problem and use a constraint solver in
our reachability and refinement checking algorithms. Explicit conformance
checking techniques often face state space explosion. First experimental
evaluations show that our approach has potential to outperform explicit
conformance checkers.",http://arxiv.org/pdf/1202.6123v1,cs.SE
2011-08-25 18:22:45+00:00,To Switch or Not To Switch: Understanding Social Influence in Recommender Systems,"['Haiyi Zhu', 'Bernardo A. Huberman', 'Yarun Luon']","We designed and ran an experiment to test how often people's choices are
reversed by others' recommendations when facing different levels of
confirmation and conformity pressures. In our experiment participants were
first asked to provide their preferences between pairs of items. They were then
asked to make second choices about the same pairs with knowledge of others'
preferences. Our results show that others people's opinions significantly sway
people's own choices. The influence is stronger when people are required to
make their second decision sometime later (22.4%) than immediately (14.1%).
Moreover, people are most likely to reverse their choices when facing a
moderate number of opposing opinions. Finally, the time people spend making the
first decision significantly predicts whether they will reverse their decisions
later on, while demographics such as age and gender do not. These results have
implications for consumer behavior research as well as online marketing
strategies.",http://arxiv.org/pdf/1108.5147v2,cs.CY
2010-12-13 12:42:15+00:00,Phase Transitions of Plan Modification in Conformant Planning,"['Junping Zhou', 'Minghao Yin']","We explore phase transitions of plan modification, which mainly focus on the
conformant planning problems. By analyzing features of plan modification in
conformant planning problems, quantitative results are obtained. If the number
of operators is less than, almost all conformant planning problems can't be
solved with plan modification. If the number of operators is more than, almost
all conformant planning problems can be solved with plan modification. The
results of the experiments also show that there exists an experimental
threshold of density (ratio of number of operators to number of propositions),
which separates the region where almost all conformant planning problems can't
be solved with plan modification from the region where almost all conformant
planning problems can be solved with plan modification.",http://arxiv.org/pdf/1012.2713v1,cs.AI
2010-07-29 10:44:49+00:00,CLP-based protein fragment assembly,"[""Alessandro Dal Palu'"", 'Agostino Dovier', 'Federico Fogolari', 'Enrico Pontelli']","The paper investigates a novel approach, based on Constraint Logic
Programming (CLP), to predict the 3D conformation of a protein via fragments
assembly. The fragments are extracted by a preprocessor-also developed for this
work- from a database of known protein structures that clusters and classifies
the fragments according to similarity and frequency. The problem of assembling
fragments into a complete conformation is mapped to a constraint solving
problem and solved using CLP. The constraint-based model uses a medium
discretization degree Ca-side chain centroid protein model that offers
efficiency and a good approximation for space filling. The approach adapts
existing energy models to the protein representation used and applies a large
neighboring search strategy. The results shows the feasibility and efficiency
of the method. The declarative nature of the solution allows to include future
extensions, e.g., different size fragments for better accuracy.",http://arxiv.org/pdf/1007.5180v1,cs.AI
2010-03-04 23:19:01+00:00,Required Behavior of Sequence Diagrams: Semantics and Conformance,"['Lunjin Lu', 'Dae-kyoo Kim']","Sequence diagrams are a widely used design notation for describing software
behaviors. Many reusable software artifacts such as design patterns and design
aspects make use of sequence diagrams to describe interaction behaviors. When a
pattern or an aspect is reused in an application, it is important to ensure
that the sequence diagrams for the application conform to the corresponding
sequence diagrams for the pattern or aspect. Reasoning about conformance
relationship between sequence diagrams has not been addressed adequately in
literature. In this paper, we focus on required behavior specified by a UML
sequence diagram. A novel trace semantics is given that captures precisely
required behavior specified by a sequence diagram and a conformance relation
between sequence diagrams is formalized based on the semantics. Properties of
the trace semantics and the conformance relation are studied.",http://arxiv.org/pdf/1003.1160v5,cs.SE
2007-06-21 16:40:06+00:00,A tutorial on conformal prediction,"['Glenn Shafer', 'Vladimir Vovk']","Conformal prediction uses past experience to determine precise levels of
confidence in new predictions. Given an error probability $\epsilon$, together
with a method that makes a prediction $\hat{y}$ of a label $y$, it produces a
set of labels, typically containing $\hat{y}$, that also contains $y$ with
probability $1-\epsilon$. Conformal prediction can be applied to any method for
producing $\hat{y}$: a nearest-neighbor method, a support-vector machine, ridge
regression, etc.
  Conformal prediction is designed for an on-line setting in which labels are
predicted successively, each one being revealed before the next is predicted.
The most novel and valuable feature of conformal prediction is that if the
successive examples are sampled independently from the same distribution, then
the successive predictions will be right $1-\epsilon$ of the time, even though
they are based on an accumulating dataset rather than on independent datasets.
  In addition to the model under which successive examples are sampled
independently, other on-line compression models can also use conformal
prediction. The widely used Gaussian linear model is one of these.
  This tutorial presents a self-contained account of the theory of conformal
prediction and works through several numerical examples. A more comprehensive
treatment of the topic is provided in ""Algorithmic Learning in a Random World"",
by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).",http://arxiv.org/pdf/0706.3188v1,cs.LG
2006-05-24 02:13:13+00:00,Modeling the Dynamics of Social Networks,"['Victor V. Kryssanov', 'Frank J. Rinaldo', 'Evgeny L. Kuleshov', 'Hitoshi Ogawa']","Modeling human dynamics responsible for the formation and evolution of the
so-called social networks - structures comprised of individuals or
organizations and indicating connectivities existing in a community - is a
topic recently attracting a significant research interest. It has been claimed
that these dynamics are scale-free in many practically important cases, such as
impersonal and personal communication, auctioning in a market, accessing sites
on the WWW, etc., and that human response times thus conform to the power law.
While a certain amount of progress has recently been achieved in predicting the
general response rate of a human population, existing formal theories of human
behavior can hardly be found satisfactory to accommodate and comprehensively
explain the scaling observed in social networks. In the presented study, a
novel system-theoretic modeling approach is proposed and successfully applied
to determine important characteristics of a communication network and to
analyze consumer behavior on the WWW.",http://arxiv.org/pdf/cs/0605101v1,cs.CY
2002-12-13 05:33:10+00:00,Computing Conformal Structure of Surfaces,"['Xianfeng Gu', 'Shing-Tung Yau']","This paper solves the problem of computing conformal structures of general
2-manifolds represented as triangle meshes. We compute conformal structures in
the following way: first compute homology bases from simplicial complex
structures, then construct dual cohomology bases and diffuse them to harmonic
1-forms. Next, we construct bases of holomorphic differentials. We then obtain
period matrices by integrating holomorphic differentials along homology bases.
We also study the global conformal mapping between genus zero surfaces and
spheres, and between general meshes and planes. Our method of computing
conformal structures can be applied to tackle fundamental problems in computer
aid design and computer graphics, such as geometry classification and
identification, and surface global parametrization.",http://arxiv.org/pdf/cs/0212043v1,cs.GR
1997-10-01 00:00:00+00:00,Analysis of Three-Dimensional Protein Images,"['L. Leherte', 'J. Glasgow', 'K. Baxter', 'E. Steeg', 'S. Fortier']","A fundamental goal of research in molecular biology is to understand protein
structure. Protein crystallography is currently the most successful method for
determining the three-dimensional (3D) conformation of a protein, yet it
remains labor intensive and relies on an expert's ability to derive and
evaluate a protein scene model. In this paper, the problem of protein structure
determination is formulated as an exercise in scene analysis. A computational
methodology is presented in which a 3D image of a protein is segmented into a
graph of critical points. Bayesian and certainty factor approaches are
described and used to analyze critical point graphs and identify meaningful
substructures, such as alpha-helices and beta-sheets. Results of applying the
methodologies to protein images at low and medium resolution are reported. The
research is related to approaches to representation, segmentation and
classification in vision, as well as to top-down approaches to protein
structure prediction.",http://arxiv.org/pdf/cs/9710101v1,cs.AI
